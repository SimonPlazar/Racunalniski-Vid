{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T19:28:41.989841Z",
     "start_time": "2025-12-07T19:28:39.567600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from itertools import cycle\n",
    "\n",
    "from Models import KeypointNet, KeypointDataset\n",
    "from Helper import save_checkpoint_generic, load_checkpoint_generic\n"
   ],
   "id": "cfa9f4796db9bbc2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T19:28:42.021277Z",
     "start_time": "2025-12-07T19:28:41.998087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# HYPERPARAMETERS\n",
    "# ============================================================\n",
    "\n",
    "# Model parameters (SuperPoint: Adam, lr=0.001, beta=(0.9, 0.999))\n",
    "learning_rate = 0.001\n",
    "adam_betas = (0.9, 0.999)\n",
    "weight_decay = 0.0\n",
    "\n",
    "# Training parameters (iteration-based)\n",
    "num_iterations = 200_000  # SuperPoint uses 200k iterations\n",
    "batch_size = 24  # SuperPoint uses 32\n",
    "\n",
    "# Image parameters\n",
    "image_size = (240, 320)  # (Height, Width)\n",
    "\n",
    "# Dataset parameters\n",
    "num_train_samples = 5000  # Number of pregenerated training samples\n",
    "num_test_samples = 500   # Number of pregenerated test samples\n",
    "\n",
    "# Augmentation settings (applied during training, not during generation)\n",
    "use_homography_augment = True    # Apply random homography to training data\n",
    "use_photometric_augment = True   # Apply brightness/contrast to training data\n",
    "use_geometric_augment = True     # Apply flips to training data\n",
    "\n",
    "# Dataset file paths (.npz format - contains pregenerated images)\n",
    "dataset_cache_dir = './datasets'\n",
    "load_datasets_if_exist = True    # Load from .npz files if available\n",
    "\n",
    "# Checkpoint parameters\n",
    "checkpoint_dir = './checkpoints'\n",
    "save_checkpoint_every = 5000  # Save every N iterations\n",
    "max_checkpoints = 4\n",
    "\n",
    "# Logging parameters\n",
    "eval_every = 100   # Evaluate on test set every N iterations\n",
    "print_every = 10\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(dataset_cache_dir, exist_ok=True)\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"  Training samples: {num_train_samples}\")\n",
    "print(f\"  Test samples: {num_test_samples}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Target iterations: {num_iterations:,}\")\n",
    "print(f\"  Training augmentation: {'ENABLED' if use_homography_augment else 'DISABLED'}\")\n",
    "print()\n"
   ],
   "id": "ed395d636eee7e2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration loaded\n",
      "  Device: CUDA\n",
      "  Training samples: 5000\n",
      "  Test samples: 500\n",
      "  Batch size: 24\n",
      "  Target iterations: 200,000\n",
      "  Training augmentation: ENABLED\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T19:28:42.029253Z",
     "start_time": "2025-12-07T19:28:42.025783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # ============================================================\n",
    "# # DATASET GENERATION AND SAVING (Run once to create datasets)\n",
    "# # ============================================================\n",
    "#\n",
    "# train_samples_path = os.path.join(dataset_cache_dir, f'train_samples_{num_train_samples}.npz')\n",
    "# test_samples_path = os.path.join(dataset_cache_dir, f'test_samples_{num_test_samples}.npz')\n",
    "#\n",
    "# print(\"=\" * 60)\n",
    "# print(\"DATASET GENERATION\")\n",
    "# print(\"=\" * 60)\n",
    "# print()\n",
    "#\n",
    "# # Generate and save training samples (raw, no augmentation)\n",
    "# print(f\"Generating {num_train_samples} training samples...\")\n",
    "# train_generator = KeypointDataset(\n",
    "#     num_samples=num_train_samples,\n",
    "#     image_shape=image_size,\n",
    "#     generate_fn=generate_synthetic_image,\n",
    "#     generate_kwargs={\n",
    "#         'width': image_size[1],\n",
    "#         'height': image_size[0],\n",
    "#         'shape_type': 'random',\n",
    "#     },\n",
    "#     use_homography_augment=False,  # No augmentation during generation\n",
    "#     use_photometric_augment=False,\n",
    "#     use_geometric_augment=False,\n",
    "#     pregenerate=True\n",
    "# )\n",
    "# print(f\"‚úì Training samples generated: {len(train_generator)} samples\")\n",
    "#\n",
    "# # Save training samples\n",
    "# print(f\"Saving to {train_samples_path}...\")\n",
    "# train_generator.save_to_file(train_samples_path)\n",
    "# print(f\"‚úì Training samples saved!\")\n",
    "# print()\n",
    "#\n",
    "# # Generate and save test samples (raw, no augmentation)\n",
    "# print(f\"Generating {num_test_samples} test samples...\")\n",
    "# test_generator = KeypointDataset(\n",
    "#     num_samples=num_test_samples,\n",
    "#     image_shape=image_size,\n",
    "#     generate_fn=generate_synthetic_image,\n",
    "#     generate_kwargs={\n",
    "#         'width': image_size[1],\n",
    "#         'height': image_size[0],\n",
    "#         'shape_type': 'random',\n",
    "#     },\n",
    "#     use_homography_augment=False,  # No augmentation during generation\n",
    "#     use_photometric_augment=False,\n",
    "#     use_geometric_augment=False,\n",
    "#     pregenerate=True\n",
    "# )\n",
    "# print(f\"‚úì Test samples generated: {len(test_generator)} samples\")\n",
    "#\n",
    "# # Save test samples\n",
    "# print(f\"Saving to {test_samples_path}...\")\n",
    "# test_generator.save_to_file(test_samples_path)\n",
    "# print(f\"‚úì Test samples saved!\")\n",
    "# print()\n",
    "#\n",
    "# print(\"=\" * 60)\n",
    "# print(\"‚úì Dataset generation complete!\")\n",
    "# print(\"=\" * 60)\n",
    "# print(f\"Training samples: {train_samples_path}\")\n",
    "# print(f\"Test samples: {test_samples_path}\")\n",
    "# print()\n"
   ],
   "id": "db4c52bcd31f3516",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T19:30:58.219425Z",
     "start_time": "2025-12-07T19:30:56.437217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# LOAD DATASETS AND INIT MODEL\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING SETUP\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATASETS FROM .NPZ FILES\n",
    "# ============================================================\n",
    "\n",
    "train_samples_path = os.path.join(dataset_cache_dir, f'train_samples_{num_train_samples}.npz')\n",
    "test_samples_path = os.path.join(dataset_cache_dir, f'test_samples_{num_test_samples}.npz')\n",
    "\n",
    "train_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "# Load training dataset WITH augmentation\n",
    "if load_datasets_if_exist and os.path.exists(train_samples_path):\n",
    "    print(f\"Loading training samples from {train_samples_path}...\")\n",
    "    print(f\"  Augmentation: {'ENABLED' if use_homography_augment else 'DISABLED'}\")\n",
    "    try:\n",
    "        train_dataset = KeypointDataset(\n",
    "            num_samples=num_train_samples,\n",
    "            image_shape=image_size,\n",
    "            use_homography_augment=use_homography_augment,\n",
    "            use_photometric_augment=use_photometric_augment,\n",
    "            use_geometric_augment=use_geometric_augment,\n",
    "            pregenerate=False,  # Don't regenerate, just load\n",
    "            load_from_file=train_samples_path\n",
    "        )\n",
    "        print(f\"‚úì Training dataset loaded: {len(train_dataset)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Failed to load training dataset: {e}\")\n",
    "        print(\"Please run the dataset generation cell first!\")\n",
    "        train_dataset = None\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Training samples not found at {train_samples_path}\")\n",
    "    print(\"Please run the dataset generation cell first!\")\n",
    "\n",
    "# Load test dataset WITHOUT augmentation\n",
    "if load_datasets_if_exist and os.path.exists(test_samples_path):\n",
    "    print(f\"Loading test samples from {test_samples_path}...\")\n",
    "    # print(f\"  Augmentation: {'ENABLED' if use_homography_augment else 'DISABLED'} (test set)\")\n",
    "    try:\n",
    "        test_dataset = KeypointDataset(\n",
    "            num_samples=num_test_samples,\n",
    "            image_shape=image_size,\n",
    "            # use_homography_augment=False,\n",
    "            # use_photometric_augment=False,\n",
    "            # use_geometric_augment=False,\n",
    "            pregenerate=False,\n",
    "            # load_from_file=test_samples_path\n",
    "        )\n",
    "        print(f\"‚úì Test dataset loaded: {len(test_dataset)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Failed to load test dataset: {e}\")\n",
    "        print(\"Please run the dataset generation cell first!\")\n",
    "        test_dataset = None\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Test samples not found at {test_samples_path}\")\n",
    "    print(\"Please run the dataset generation cell first!\")\n",
    "\n",
    "# Check if datasets were loaded successfully\n",
    "if train_dataset is None or test_dataset is None:\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚ö†Ô∏è  ERROR: Datasets not loaded!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Please run Cell 3 (Dataset Generation) first to create the .npz files.\")\n",
    "    print()\n",
    "    raise RuntimeError(\"Datasets not found. Run dataset generation cell first.\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# CREATE DATALOADERS\n",
    "# ============================================================\n",
    "\n",
    "print(\"Creating DataLoaders...\")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"‚úì DataLoaders created\")\n",
    "print(f\"  Training batches per epoch: {len(train_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# MODEL INITIALIZATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"Initializing model...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = KeypointNet().to(device)\n",
    "\n",
    "# Optimizer (SuperPoint paper: Adam with lr=0.001, betas=(0.9, 0.999))\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=adam_betas,\n",
    "    weight_decay=weight_decay\n",
    ")\n"
   ],
   "id": "5ff1a7b425389884",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING SETUP\n",
      "============================================================\n",
      "\n",
      "Loading training samples from ./datasets\\train_samples_5000.npz...\n",
      "  Augmentation: ENABLED\n",
      "Loading 5000 samples from ./datasets\\train_samples_5000.npz...\n",
      "‚úì Loaded 5000 samples!\n",
      "‚úì Training dataset loaded: 5000 samples\n",
      "Loading test samples from ./datasets\\test_samples_500.npz...\n",
      "‚úì Test dataset loaded: 500 samples\n",
      "\n",
      "Creating DataLoaders...\n",
      "‚úì DataLoaders created\n",
      "  Training batches per epoch: 209\n",
      "  Test batches: 21\n",
      "\n",
      "Initializing model...\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T19:13:04.587235Z",
     "start_time": "2025-12-07T17:08:46.021909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "# Loss tracking\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "start_iteration = 0\n",
    "\n",
    "# Load checkpoint if exists\n",
    "checkpoint = load_checkpoint_generic(checkpoint_dir, device)\n",
    "if checkpoint:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_iteration = checkpoint.get('iteration', 0)\n",
    "    train_losses = checkpoint.get('train_losses', [])\n",
    "    test_losses = checkpoint.get('test_losses', [])\n",
    "    print(f\"‚úì Resuming from iteration {start_iteration:,}\")\n",
    "else:\n",
    "    print(\"‚úì Starting from scratch\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# ITERATION-BASED TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "model.train()\n",
    "running_loss = 0.0\n",
    "iteration = start_iteration\n",
    "\n",
    "# Create iterator for infinite cycling through dataset\n",
    "train_iterator = cycle(train_loader)\n",
    "\n",
    "while iteration < num_iterations:\n",
    "    # Get next batch (infinite cycling)\n",
    "    images, targets = next(train_iterator)\n",
    "\n",
    "    images = images.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(images, return_logits=True)  # (B, 65, H/8, W/8)\n",
    "    targets_idx = targets.argmax(dim=1)  # (B, H/8, W/8)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = F.cross_entropy(logits, targets_idx)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    iteration += 1\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    # Print training loss\n",
    "    if iteration % print_every == 0:\n",
    "        avg_loss = running_loss / print_every\n",
    "        running_loss = 0.0\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Iter [{iteration:>6}/{num_iterations}] Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    if iteration % eval_every == 0:\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        num_test_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for test_images, test_targets in test_loader:\n",
    "                test_images = test_images.to(device)\n",
    "                test_targets = test_targets.to(device)\n",
    "\n",
    "                logits = model(test_images, return_logits=True)\n",
    "                targets_idx = test_targets.argmax(dim=1)\n",
    "                loss = F.cross_entropy(logits, targets_idx)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                num_test_batches += 1\n",
    "\n",
    "        avg_test_loss = test_loss / num_test_batches\n",
    "        test_losses.append(avg_test_loss)\n",
    "        print(f\"  ‚îî‚îÄ Test Loss: {avg_test_loss:.4f}\")\n",
    "        model.train()\n",
    "\n",
    "    # Save checkpoint\n",
    "    if iteration % save_checkpoint_every == 0 or iteration == num_iterations:\n",
    "        save_checkpoint_generic(\n",
    "            checkpoint_dir,\n",
    "            iteration,\n",
    "            {\n",
    "                'iteration': iteration,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'test_losses': test_losses,\n",
    "                'config': {\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'batch_size': batch_size,\n",
    "                    'num_iterations': num_iterations,\n",
    "                    'eval_every': eval_every,\n",
    "                    'print_every': print_every,\n",
    "                    'image_size': image_size,\n",
    "                }\n",
    "            },\n",
    "            max_checkpoints=max_checkpoints\n",
    "        )\n",
    "        print(f\"  ‚îî‚îÄ Checkpoint saved\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úì TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "if len(train_losses) > 0:\n",
    "    print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "if len(test_losses) > 0:\n",
    "    print(f\"Final Test Loss: {test_losses[-1]:.4f}\")\n",
    "print()\n"
   ],
   "id": "5c963dabbb8a6d57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded checkpoint: ./checkpoints\\checkpoint_epoch_5000.pth (epoch 5000)\n",
      "‚úì Resuming from iteration 5,000\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "Iter [  5010/200000] Loss: 0.0440\n",
      "Iter [  5020/200000] Loss: 0.0442\n",
      "Iter [  5030/200000] Loss: 0.0482\n",
      "Iter [  5040/200000] Loss: 0.0454\n",
      "Iter [  5050/200000] Loss: 0.0450\n",
      "Iter [  5060/200000] Loss: 0.0455\n",
      "Iter [  5070/200000] Loss: 0.0447\n",
      "Iter [  5080/200000] Loss: 0.0436\n",
      "Iter [  5090/200000] Loss: 0.0435\n",
      "Iter [  5100/200000] Loss: 0.0444\n",
      "  ‚îî‚îÄ Test Loss: 0.0472\n",
      "Iter [  5110/200000] Loss: 0.0476\n",
      "Iter [  5120/200000] Loss: 0.0452\n",
      "Iter [  5130/200000] Loss: 0.0456\n",
      "Iter [  5140/200000] Loss: 0.0458\n",
      "Iter [  5150/200000] Loss: 0.0419\n",
      "Iter [  5160/200000] Loss: 0.0430\n",
      "Iter [  5170/200000] Loss: 0.0449\n",
      "Iter [  5180/200000] Loss: 0.0450\n",
      "Iter [  5190/200000] Loss: 0.0483\n",
      "Iter [  5200/200000] Loss: 0.0428\n",
      "  ‚îî‚îÄ Test Loss: 0.0457\n",
      "Iter [  5210/200000] Loss: 0.0486\n",
      "Iter [  5220/200000] Loss: 0.0419\n",
      "Iter [  5230/200000] Loss: 0.0414\n",
      "Iter [  5240/200000] Loss: 0.0454\n",
      "Iter [  5250/200000] Loss: 0.0437\n",
      "Iter [  5260/200000] Loss: 0.0435\n",
      "Iter [  5270/200000] Loss: 0.0443\n",
      "Iter [  5280/200000] Loss: 0.0427\n",
      "Iter [  5290/200000] Loss: 0.0419\n",
      "Iter [  5300/200000] Loss: 0.0440\n",
      "  ‚îî‚îÄ Test Loss: 0.0457\n",
      "Iter [  5310/200000] Loss: 0.0427\n",
      "Iter [  5320/200000] Loss: 0.0464\n",
      "Iter [  5330/200000] Loss: 0.0447\n",
      "Iter [  5340/200000] Loss: 0.0451\n",
      "Iter [  5350/200000] Loss: 0.0437\n",
      "Iter [  5360/200000] Loss: 0.0420\n",
      "Iter [  5370/200000] Loss: 0.0432\n",
      "Iter [  5380/200000] Loss: 0.0441\n",
      "Iter [  5390/200000] Loss: 0.0428\n",
      "Iter [  5400/200000] Loss: 0.0468\n",
      "  ‚îî‚îÄ Test Loss: 0.0453\n",
      "Iter [  5410/200000] Loss: 0.0423\n",
      "Iter [  5420/200000] Loss: 0.0492\n",
      "Iter [  5430/200000] Loss: 0.0404\n",
      "Iter [  5440/200000] Loss: 0.0404\n",
      "Iter [  5450/200000] Loss: 0.0450\n",
      "Iter [  5460/200000] Loss: 0.0428\n",
      "Iter [  5470/200000] Loss: 0.0429\n",
      "Iter [  5480/200000] Loss: 0.0439\n",
      "Iter [  5490/200000] Loss: 0.0414\n",
      "Iter [  5500/200000] Loss: 0.0417\n",
      "  ‚îî‚îÄ Test Loss: 0.0480\n",
      "Iter [  5510/200000] Loss: 0.0428\n",
      "Iter [  5520/200000] Loss: 0.0439\n",
      "Iter [  5530/200000] Loss: 0.0442\n",
      "Iter [  5540/200000] Loss: 0.0435\n",
      "Iter [  5550/200000] Loss: 0.0452\n",
      "Iter [  5560/200000] Loss: 0.0433\n",
      "Iter [  5570/200000] Loss: 0.0412\n",
      "Iter [  5580/200000] Loss: 0.0419\n",
      "Iter [  5590/200000] Loss: 0.0439\n",
      "Iter [  5600/200000] Loss: 0.0428\n",
      "  ‚îî‚îÄ Test Loss: 0.0495\n",
      "Iter [  5610/200000] Loss: 0.0458\n",
      "Iter [  5620/200000] Loss: 0.0414\n",
      "Iter [  5630/200000] Loss: 0.0481\n",
      "Iter [  5640/200000] Loss: 0.0393\n",
      "Iter [  5650/200000] Loss: 0.0410\n",
      "Iter [  5660/200000] Loss: 0.0435\n",
      "Iter [  5670/200000] Loss: 0.0418\n",
      "Iter [  5680/200000] Loss: 0.0427\n",
      "Iter [  5690/200000] Loss: 0.0432\n",
      "Iter [  5700/200000] Loss: 0.0407\n",
      "  ‚îî‚îÄ Test Loss: 0.0436\n",
      "Iter [  5710/200000] Loss: 0.0417\n",
      "Iter [  5720/200000] Loss: 0.0409\n",
      "Iter [  5730/200000] Loss: 0.0436\n",
      "Iter [  5740/200000] Loss: 0.0441\n",
      "Iter [  5750/200000] Loss: 0.0424\n",
      "Iter [  5760/200000] Loss: 0.0449\n",
      "Iter [  5770/200000] Loss: 0.0420\n",
      "Iter [  5780/200000] Loss: 0.0406\n",
      "Iter [  5790/200000] Loss: 0.0429\n",
      "Iter [  5800/200000] Loss: 0.0427\n",
      "  ‚îî‚îÄ Test Loss: 0.0474\n",
      "Iter [  5810/200000] Loss: 0.0420\n",
      "Iter [  5820/200000] Loss: 0.0445\n",
      "Iter [  5830/200000] Loss: 0.0412\n",
      "Iter [  5840/200000] Loss: 0.0466\n",
      "Iter [  5850/200000] Loss: 0.0389\n",
      "Iter [  5860/200000] Loss: 0.0421\n",
      "Iter [  5870/200000] Loss: 0.0406\n",
      "Iter [  5880/200000] Loss: 0.0427\n",
      "Iter [  5890/200000] Loss: 0.0410\n",
      "Iter [  5900/200000] Loss: 0.0431\n",
      "  ‚îî‚îÄ Test Loss: 0.0457\n",
      "Iter [  5910/200000] Loss: 0.0391\n",
      "Iter [  5920/200000] Loss: 0.0410\n",
      "Iter [  5930/200000] Loss: 0.0403\n",
      "Iter [  5940/200000] Loss: 0.0427\n",
      "Iter [  5950/200000] Loss: 0.0446\n",
      "Iter [  5960/200000] Loss: 0.0400\n",
      "Iter [  5970/200000] Loss: 0.0451\n",
      "Iter [  5980/200000] Loss: 0.0397\n",
      "Iter [  5990/200000] Loss: 0.0415\n",
      "Iter [  6000/200000] Loss: 0.0415\n",
      "  ‚îî‚îÄ Test Loss: 0.0467\n",
      "Iter [  6010/200000] Loss: 0.0428\n",
      "Iter [  6020/200000] Loss: 0.0407\n",
      "Iter [  6030/200000] Loss: 0.0444\n",
      "Iter [  6040/200000] Loss: 0.0408\n",
      "Iter [  6050/200000] Loss: 0.0454\n",
      "Iter [  6060/200000] Loss: 0.0378\n",
      "Iter [  6070/200000] Loss: 0.0421\n",
      "Iter [  6080/200000] Loss: 0.0391\n",
      "Iter [  6090/200000] Loss: 0.0415\n",
      "Iter [  6100/200000] Loss: 0.0407\n",
      "  ‚îî‚îÄ Test Loss: 0.0489\n",
      "Iter [  6110/200000] Loss: 0.0412\n",
      "Iter [  6120/200000] Loss: 0.0381\n",
      "Iter [  6130/200000] Loss: 0.0415\n",
      "Iter [  6140/200000] Loss: 0.0387\n",
      "Iter [  6150/200000] Loss: 0.0434\n",
      "Iter [  6160/200000] Loss: 0.0419\n",
      "Iter [  6170/200000] Loss: 0.0406\n",
      "Iter [  6180/200000] Loss: 0.0436\n",
      "Iter [  6190/200000] Loss: 0.0382\n",
      "Iter [  6200/200000] Loss: 0.0406\n",
      "  ‚îî‚îÄ Test Loss: 0.0503\n",
      "Iter [  6210/200000] Loss: 0.0416\n",
      "Iter [  6220/200000] Loss: 0.0407\n",
      "Iter [  6230/200000] Loss: 0.0408\n",
      "Iter [  6240/200000] Loss: 0.0434\n",
      "Iter [  6250/200000] Loss: 0.0409\n",
      "Iter [  6260/200000] Loss: 0.0431\n",
      "Iter [  6270/200000] Loss: 0.0375\n",
      "Iter [  6280/200000] Loss: 0.0420\n",
      "Iter [  6290/200000] Loss: 0.0371\n",
      "Iter [  6300/200000] Loss: 0.0400\n",
      "  ‚îî‚îÄ Test Loss: 0.0500\n",
      "Iter [  6310/200000] Loss: 0.0391\n",
      "Iter [  6320/200000] Loss: 0.0402\n",
      "Iter [  6330/200000] Loss: 0.0376\n",
      "Iter [  6340/200000] Loss: 0.0401\n",
      "Iter [  6350/200000] Loss: 0.0384\n",
      "Iter [  6360/200000] Loss: 0.0430\n",
      "Iter [  6370/200000] Loss: 0.0414\n",
      "Iter [  6380/200000] Loss: 0.0388\n",
      "Iter [  6390/200000] Loss: 0.0432\n",
      "Iter [  6400/200000] Loss: 0.0380\n",
      "  ‚îî‚îÄ Test Loss: 0.0491\n",
      "Iter [  6410/200000] Loss: 0.0392\n",
      "Iter [  6420/200000] Loss: 0.0411\n",
      "Iter [  6430/200000] Loss: 0.0411\n",
      "Iter [  6440/200000] Loss: 0.0417\n",
      "Iter [  6450/200000] Loss: 0.0399\n",
      "Iter [  6460/200000] Loss: 0.0407\n",
      "Iter [  6470/200000] Loss: 0.0407\n",
      "Iter [  6480/200000] Loss: 0.0371\n",
      "Iter [  6490/200000] Loss: 0.0416\n",
      "Iter [  6500/200000] Loss: 0.0360\n",
      "  ‚îî‚îÄ Test Loss: 0.0570\n",
      "Iter [  6510/200000] Loss: 0.0388\n",
      "Iter [  6520/200000] Loss: 0.0380\n",
      "Iter [  6530/200000] Loss: 0.0390\n",
      "Iter [  6540/200000] Loss: 0.0361\n",
      "Iter [  6550/200000] Loss: 0.0403\n",
      "Iter [  6560/200000] Loss: 0.0375\n",
      "Iter [  6570/200000] Loss: 0.0430\n",
      "Iter [  6580/200000] Loss: 0.0396\n",
      "Iter [  6590/200000] Loss: 0.0386\n",
      "Iter [  6600/200000] Loss: 0.0421\n",
      "  ‚îî‚îÄ Test Loss: 0.0586\n",
      "Iter [  6610/200000] Loss: 0.0380\n",
      "Iter [  6620/200000] Loss: 0.0373\n",
      "Iter [  6630/200000] Loss: 0.0410\n",
      "Iter [  6640/200000] Loss: 0.0400\n",
      "Iter [  6650/200000] Loss: 0.0414\n",
      "Iter [  6660/200000] Loss: 0.0398\n",
      "Iter [  6670/200000] Loss: 0.0401\n",
      "Iter [  6680/200000] Loss: 0.0399\n",
      "Iter [  6690/200000] Loss: 0.0353\n",
      "Iter [  6700/200000] Loss: 0.0409\n",
      "  ‚îî‚îÄ Test Loss: 0.0487\n",
      "Iter [  6710/200000] Loss: 0.0349\n",
      "Iter [  6720/200000] Loss: 0.0377\n",
      "Iter [  6730/200000] Loss: 0.0381\n",
      "Iter [  6740/200000] Loss: 0.0376\n",
      "Iter [  6750/200000] Loss: 0.0354\n",
      "Iter [  6760/200000] Loss: 0.0384\n",
      "Iter [  6770/200000] Loss: 0.0369\n",
      "Iter [  6780/200000] Loss: 0.0419\n",
      "Iter [  6790/200000] Loss: 0.0388\n",
      "Iter [  6800/200000] Loss: 0.0385\n",
      "  ‚îî‚îÄ Test Loss: 0.0587\n",
      "Iter [  6810/200000] Loss: 0.0403\n",
      "Iter [  6820/200000] Loss: 0.0374\n",
      "Iter [  6830/200000] Loss: 0.0365\n",
      "Iter [  6840/200000] Loss: 0.0397\n",
      "Iter [  6850/200000] Loss: 0.0397\n",
      "Iter [  6860/200000] Loss: 0.0405\n",
      "Iter [  6870/200000] Loss: 0.0393\n",
      "Iter [  6880/200000] Loss: 0.0393\n",
      "Iter [  6890/200000] Loss: 0.0365\n",
      "Iter [  6900/200000] Loss: 0.0347\n",
      "  ‚îî‚îÄ Test Loss: 0.0546\n",
      "Iter [  6910/200000] Loss: 0.0393\n",
      "Iter [  6920/200000] Loss: 0.0355\n",
      "Iter [  6930/200000] Loss: 0.0356\n",
      "Iter [  6940/200000] Loss: 0.0372\n",
      "Iter [  6950/200000] Loss: 0.0359\n",
      "Iter [  6960/200000] Loss: 0.0342\n",
      "Iter [  6970/200000] Loss: 0.0384\n",
      "Iter [  6980/200000] Loss: 0.0376\n",
      "Iter [  6990/200000] Loss: 0.0398\n",
      "Iter [  7000/200000] Loss: 0.0387\n",
      "  ‚îî‚îÄ Test Loss: 0.0541\n",
      "Iter [  7010/200000] Loss: 0.0394\n",
      "Iter [  7020/200000] Loss: 0.0392\n",
      "Iter [  7030/200000] Loss: 0.0365\n",
      "Iter [  7040/200000] Loss: 0.0364\n",
      "Iter [  7050/200000] Loss: 0.0388\n",
      "Iter [  7060/200000] Loss: 0.0381\n",
      "Iter [  7070/200000] Loss: 0.0403\n",
      "Iter [  7080/200000] Loss: 0.0376\n",
      "Iter [  7090/200000] Loss: 0.0371\n",
      "Iter [  7100/200000] Loss: 0.0350\n",
      "  ‚îî‚îÄ Test Loss: 0.0575\n",
      "Iter [  7110/200000] Loss: 0.0342\n",
      "Iter [  7120/200000] Loss: 0.0372\n",
      "Iter [  7130/200000] Loss: 0.0349\n",
      "Iter [  7140/200000] Loss: 0.0339\n",
      "Iter [  7150/200000] Loss: 0.0355\n",
      "Iter [  7160/200000] Loss: 0.0347\n",
      "Iter [  7170/200000] Loss: 0.0351\n",
      "Iter [  7180/200000] Loss: 0.0348\n",
      "Iter [  7190/200000] Loss: 0.0372\n",
      "Iter [  7200/200000] Loss: 0.0391\n",
      "  ‚îî‚îÄ Test Loss: 0.0691\n",
      "Iter [  7210/200000] Loss: 0.0379\n",
      "Iter [  7220/200000] Loss: 0.0382\n",
      "Iter [  7230/200000] Loss: 0.0383\n",
      "Iter [  7240/200000] Loss: 0.0347\n",
      "Iter [  7250/200000] Loss: 0.0359\n",
      "Iter [  7260/200000] Loss: 0.0370\n",
      "Iter [  7270/200000] Loss: 0.0374\n",
      "Iter [  7280/200000] Loss: 0.0401\n",
      "Iter [  7290/200000] Loss: 0.0350\n",
      "Iter [  7300/200000] Loss: 0.0367\n",
      "  ‚îî‚îÄ Test Loss: 0.0560\n",
      "Iter [  7310/200000] Loss: 0.0341\n",
      "Iter [  7320/200000] Loss: 0.0327\n",
      "Iter [  7330/200000] Loss: 0.0354\n",
      "Iter [  7340/200000] Loss: 0.0335\n",
      "Iter [  7350/200000] Loss: 0.0337\n",
      "Iter [  7360/200000] Loss: 0.0344\n",
      "Iter [  7370/200000] Loss: 0.0323\n",
      "Iter [  7380/200000] Loss: 0.0334\n",
      "Iter [  7390/200000] Loss: 0.0351\n",
      "Iter [  7400/200000] Loss: 0.0345\n",
      "  ‚îî‚îÄ Test Loss: 0.0569\n",
      "Iter [  7410/200000] Loss: 0.0371\n",
      "Iter [  7420/200000] Loss: 0.0368\n",
      "Iter [  7430/200000] Loss: 0.0376\n",
      "Iter [  7440/200000] Loss: 0.0358\n",
      "Iter [  7450/200000] Loss: 0.0346\n",
      "Iter [  7460/200000] Loss: 0.0358\n",
      "Iter [  7470/200000] Loss: 0.0362\n",
      "Iter [  7480/200000] Loss: 0.0350\n",
      "Iter [  7490/200000] Loss: 0.0387\n",
      "Iter [  7500/200000] Loss: 0.0341\n",
      "  ‚îî‚îÄ Test Loss: 0.0536\n",
      "Iter [  7510/200000] Loss: 0.0366\n",
      "Iter [  7520/200000] Loss: 0.0320\n",
      "Iter [  7530/200000] Loss: 0.0310\n",
      "Iter [  7540/200000] Loss: 0.0340\n",
      "Iter [  7550/200000] Loss: 0.0310\n",
      "Iter [  7560/200000] Loss: 0.0323\n",
      "Iter [  7570/200000] Loss: 0.0333\n",
      "Iter [  7580/200000] Loss: 0.0307\n",
      "Iter [  7590/200000] Loss: 0.0322\n",
      "Iter [  7600/200000] Loss: 0.0334\n",
      "  ‚îî‚îÄ Test Loss: 0.0723\n",
      "Iter [  7610/200000] Loss: 0.0343\n",
      "Iter [  7620/200000] Loss: 0.0335\n",
      "Iter [  7630/200000] Loss: 0.0348\n",
      "Iter [  7640/200000] Loss: 0.0368\n",
      "Iter [  7650/200000] Loss: 0.0343\n",
      "Iter [  7660/200000] Loss: 0.0332\n",
      "Iter [  7670/200000] Loss: 0.0339\n",
      "Iter [  7680/200000] Loss: 0.0350\n",
      "Iter [  7690/200000] Loss: 0.0342\n",
      "Iter [  7700/200000] Loss: 0.0369\n",
      "  ‚îî‚îÄ Test Loss: 0.0620\n",
      "Iter [  7710/200000] Loss: 0.0331\n",
      "Iter [  7720/200000] Loss: 0.0349\n",
      "Iter [  7730/200000] Loss: 0.0305\n",
      "Iter [  7740/200000] Loss: 0.0308\n",
      "Iter [  7750/200000] Loss: 0.0321\n",
      "Iter [  7760/200000] Loss: 0.0289\n",
      "Iter [  7770/200000] Loss: 0.0314\n",
      "Iter [  7780/200000] Loss: 0.0313\n",
      "Iter [  7790/200000] Loss: 0.0295\n",
      "Iter [  7800/200000] Loss: 0.0314\n",
      "  ‚îî‚îÄ Test Loss: 0.0662\n",
      "Iter [  7810/200000] Loss: 0.0312\n",
      "Iter [  7820/200000] Loss: 0.0327\n",
      "Iter [  7830/200000] Loss: 0.0327\n",
      "Iter [  7840/200000] Loss: 0.0330\n",
      "Iter [  7850/200000] Loss: 0.0352\n",
      "Iter [  7860/200000] Loss: 0.0327\n",
      "Iter [  7870/200000] Loss: 0.0319\n",
      "Iter [  7880/200000] Loss: 0.0336\n",
      "Iter [  7890/200000] Loss: 0.0328\n",
      "Iter [  7900/200000] Loss: 0.0327\n",
      "  ‚îî‚îÄ Test Loss: 0.0562\n",
      "Iter [  7910/200000] Loss: 0.0344\n",
      "Iter [  7920/200000] Loss: 0.0318\n",
      "Iter [  7930/200000] Loss: 0.0320\n",
      "Iter [  7940/200000] Loss: 0.0292\n",
      "Iter [  7950/200000] Loss: 0.0306\n",
      "Iter [  7960/200000] Loss: 0.0283\n",
      "Iter [  7970/200000] Loss: 0.0285\n",
      "Iter [  7980/200000] Loss: 0.0291\n",
      "Iter [  7990/200000] Loss: 0.0304\n",
      "Iter [  8000/200000] Loss: 0.0276\n",
      "  ‚îî‚îÄ Test Loss: 0.0664\n",
      "Iter [  8010/200000] Loss: 0.0304\n",
      "Iter [  8020/200000] Loss: 0.0304\n",
      "Iter [  8030/200000] Loss: 0.0316\n",
      "Iter [  8040/200000] Loss: 0.0316\n",
      "Iter [  8050/200000] Loss: 0.0299\n",
      "Iter [  8060/200000] Loss: 0.0338\n",
      "Iter [  8070/200000] Loss: 0.0298\n",
      "Iter [  8080/200000] Loss: 0.0318\n",
      "Iter [  8090/200000] Loss: 0.0312\n",
      "Iter [  8100/200000] Loss: 0.0311\n",
      "  ‚îî‚îÄ Test Loss: 0.0670\n",
      "Iter [  8110/200000] Loss: 0.0306\n",
      "Iter [  8120/200000] Loss: 0.0330\n",
      "Iter [  8130/200000] Loss: 0.0302\n",
      "Iter [  8140/200000] Loss: 0.0304\n",
      "Iter [  8150/200000] Loss: 0.0269\n",
      "Iter [  8160/200000] Loss: 0.0294\n",
      "Iter [  8170/200000] Loss: 0.0262\n",
      "Iter [  8180/200000] Loss: 0.0267\n",
      "Iter [  8190/200000] Loss: 0.0280\n",
      "Iter [  8200/200000] Loss: 0.0286\n",
      "  ‚îî‚îÄ Test Loss: 0.0737\n",
      "Iter [  8210/200000] Loss: 0.0264\n",
      "Iter [  8220/200000] Loss: 0.0293\n",
      "Iter [  8230/200000] Loss: 0.0285\n",
      "Iter [  8240/200000] Loss: 0.0314\n",
      "Iter [  8250/200000] Loss: 0.0294\n",
      "Iter [  8260/200000] Loss: 0.0296\n",
      "Iter [  8270/200000] Loss: 0.0316\n",
      "Iter [  8280/200000] Loss: 0.0280\n",
      "Iter [  8290/200000] Loss: 0.0307\n",
      "Iter [  8300/200000] Loss: 0.0305\n",
      "  ‚îî‚îÄ Test Loss: 0.0677\n",
      "Iter [  8310/200000] Loss: 0.0293\n",
      "Iter [  8320/200000] Loss: 0.0293\n",
      "Iter [  8330/200000] Loss: 0.0312\n",
      "Iter [  8340/200000] Loss: 0.0294\n",
      "Iter [  8350/200000] Loss: 0.0279\n",
      "Iter [  8360/200000] Loss: 0.0258\n",
      "Iter [  8370/200000] Loss: 0.0285\n",
      "Iter [  8380/200000] Loss: 0.0245\n",
      "Iter [  8390/200000] Loss: 0.0253\n",
      "Iter [  8400/200000] Loss: 0.0265\n",
      "  ‚îî‚îÄ Test Loss: 0.0812\n",
      "Iter [  8410/200000] Loss: 0.0274\n",
      "Iter [  8420/200000] Loss: 0.0261\n",
      "Iter [  8430/200000] Loss: 0.0276\n",
      "Iter [  8440/200000] Loss: 0.0271\n",
      "Iter [  8450/200000] Loss: 0.0296\n",
      "Iter [  8460/200000] Loss: 0.0278\n",
      "Iter [  8470/200000] Loss: 0.0276\n",
      "Iter [  8480/200000] Loss: 0.0302\n",
      "Iter [  8490/200000] Loss: 0.0275\n",
      "Iter [  8500/200000] Loss: 0.0289\n",
      "  ‚îî‚îÄ Test Loss: 0.0765\n",
      "Iter [  8510/200000] Loss: 0.0296\n",
      "Iter [  8520/200000] Loss: 0.0287\n",
      "Iter [  8530/200000] Loss: 0.0290\n",
      "Iter [  8540/200000] Loss: 0.0271\n",
      "Iter [  8550/200000] Loss: 0.0280\n",
      "Iter [  8560/200000] Loss: 0.0252\n",
      "Iter [  8570/200000] Loss: 0.0249\n",
      "Iter [  8580/200000] Loss: 0.0275\n",
      "Iter [  8590/200000] Loss: 0.0232\n",
      "Iter [  8600/200000] Loss: 0.0246\n",
      "  ‚îî‚îÄ Test Loss: 0.0808\n",
      "Iter [  8610/200000] Loss: 0.0252\n",
      "Iter [  8620/200000] Loss: 0.0263\n",
      "Iter [  8630/200000] Loss: 0.0242\n",
      "Iter [  8640/200000] Loss: 0.0273\n",
      "Iter [  8650/200000] Loss: 0.0254\n",
      "Iter [  8660/200000] Loss: 0.0276\n",
      "Iter [  8670/200000] Loss: 0.0251\n",
      "Iter [  8680/200000] Loss: 0.0261\n",
      "Iter [  8690/200000] Loss: 0.0291\n",
      "Iter [  8700/200000] Loss: 0.0267\n",
      "  ‚îî‚îÄ Test Loss: 0.0733\n",
      "Iter [  8710/200000] Loss: 0.0271\n",
      "Iter [  8720/200000] Loss: 0.0286\n",
      "Iter [  8730/200000] Loss: 0.0275\n",
      "Iter [  8740/200000] Loss: 0.0271\n",
      "Iter [  8750/200000] Loss: 0.0256\n",
      "Iter [  8760/200000] Loss: 0.0261\n",
      "Iter [  8770/200000] Loss: 0.0237\n",
      "Iter [  8780/200000] Loss: 0.0230\n",
      "Iter [  8790/200000] Loss: 0.0261\n",
      "Iter [  8800/200000] Loss: 0.0215\n",
      "  ‚îî‚îÄ Test Loss: 0.0827\n",
      "Iter [  8810/200000] Loss: 0.0234\n",
      "Iter [  8820/200000] Loss: 0.0240\n",
      "Iter [  8830/200000] Loss: 0.0246\n",
      "Iter [  8840/200000] Loss: 0.0238\n",
      "Iter [  8850/200000] Loss: 0.0247\n",
      "Iter [  8860/200000] Loss: 0.0245\n",
      "Iter [  8870/200000] Loss: 0.0253\n",
      "Iter [  8880/200000] Loss: 0.0237\n",
      "Iter [  8890/200000] Loss: 0.0246\n",
      "Iter [  8900/200000] Loss: 0.0270\n",
      "  ‚îî‚îÄ Test Loss: 0.0811\n",
      "Iter [  8910/200000] Loss: 0.0249\n",
      "Iter [  8920/200000] Loss: 0.0251\n",
      "Iter [  8930/200000] Loss: 0.0267\n",
      "Iter [  8940/200000] Loss: 0.0262\n",
      "Iter [  8950/200000] Loss: 0.0253\n",
      "Iter [  8960/200000] Loss: 0.0241\n",
      "Iter [  8970/200000] Loss: 0.0246\n",
      "Iter [  8980/200000] Loss: 0.0209\n",
      "Iter [  8990/200000] Loss: 0.0219\n",
      "Iter [  9000/200000] Loss: 0.0242\n",
      "  ‚îî‚îÄ Test Loss: 0.0750\n",
      "Iter [  9010/200000] Loss: 0.0211\n",
      "Iter [  9020/200000] Loss: 0.0218\n",
      "Iter [  9030/200000] Loss: 0.0231\n",
      "Iter [  9040/200000] Loss: 0.0224\n",
      "Iter [  9050/200000] Loss: 0.0232\n",
      "Iter [  9060/200000] Loss: 0.0237\n",
      "Iter [  9070/200000] Loss: 0.0236\n",
      "Iter [  9080/200000] Loss: 0.0227\n",
      "Iter [  9090/200000] Loss: 0.0224\n",
      "Iter [  9100/200000] Loss: 0.0237\n",
      "  ‚îî‚îÄ Test Loss: 0.0847\n",
      "Iter [  9110/200000] Loss: 0.0246\n",
      "Iter [  9120/200000] Loss: 0.0233\n",
      "Iter [  9130/200000] Loss: 0.0242\n",
      "Iter [  9140/200000] Loss: 0.0247\n",
      "Iter [  9150/200000] Loss: 0.0244\n",
      "Iter [  9160/200000] Loss: 0.0237\n",
      "Iter [  9170/200000] Loss: 0.0222\n",
      "Iter [  9180/200000] Loss: 0.0214\n",
      "Iter [  9190/200000] Loss: 0.0207\n",
      "Iter [  9200/200000] Loss: 0.0212\n",
      "  ‚îî‚îÄ Test Loss: 0.0855\n",
      "Iter [  9210/200000] Loss: 0.0221\n",
      "Iter [  9220/200000] Loss: 0.0205\n",
      "Iter [  9230/200000] Loss: 0.0197\n",
      "Iter [  9240/200000] Loss: 0.0220\n",
      "Iter [  9250/200000] Loss: 0.0205\n",
      "Iter [  9260/200000] Loss: 0.0224\n",
      "Iter [  9270/200000] Loss: 0.0218\n",
      "Iter [  9280/200000] Loss: 0.0221\n",
      "Iter [  9290/200000] Loss: 0.0218\n",
      "Iter [  9300/200000] Loss: 0.0214\n",
      "  ‚îî‚îÄ Test Loss: 0.0851\n",
      "Iter [  9310/200000] Loss: 0.0223\n",
      "Iter [  9320/200000] Loss: 0.0235\n",
      "Iter [  9330/200000] Loss: 0.0218\n",
      "Iter [  9340/200000] Loss: 0.0225\n",
      "Iter [  9350/200000] Loss: 0.0228\n",
      "Iter [  9360/200000] Loss: 0.0222\n",
      "Iter [  9370/200000] Loss: 0.0226\n",
      "Iter [  9380/200000] Loss: 0.0199\n",
      "Iter [  9390/200000] Loss: 0.0206\n",
      "Iter [  9400/200000] Loss: 0.0196\n",
      "  ‚îî‚îÄ Test Loss: 0.0928\n",
      "Iter [  9410/200000] Loss: 0.0192\n",
      "Iter [  9420/200000] Loss: 0.0206\n",
      "Iter [  9430/200000] Loss: 0.0194\n",
      "Iter [  9440/200000] Loss: 0.0187\n",
      "Iter [  9450/200000] Loss: 0.0199\n",
      "Iter [  9460/200000] Loss: 0.0190\n",
      "Iter [  9470/200000] Loss: 0.0202\n",
      "Iter [  9480/200000] Loss: 0.0204\n",
      "Iter [  9490/200000] Loss: 0.0200\n",
      "Iter [  9500/200000] Loss: 0.0199\n",
      "  ‚îî‚îÄ Test Loss: 0.0943\n",
      "Iter [  9510/200000] Loss: 0.0200\n",
      "Iter [  9520/200000] Loss: 0.0207\n",
      "Iter [  9530/200000] Loss: 0.0207\n",
      "Iter [  9540/200000] Loss: 0.0210\n",
      "Iter [  9550/200000] Loss: 0.0213\n",
      "Iter [  9560/200000] Loss: 0.0208\n",
      "Iter [  9570/200000] Loss: 0.0191\n",
      "Iter [  9580/200000] Loss: 0.0210\n",
      "Iter [  9590/200000] Loss: 0.0186\n",
      "Iter [  9600/200000] Loss: 0.0201\n",
      "  ‚îî‚îÄ Test Loss: 0.0846\n",
      "Iter [  9610/200000] Loss: 0.0183\n",
      "Iter [  9620/200000] Loss: 0.0169\n",
      "Iter [  9630/200000] Loss: 0.0200\n",
      "Iter [  9640/200000] Loss: 0.0181\n",
      "Iter [  9650/200000] Loss: 0.0180\n",
      "Iter [  9660/200000] Loss: 0.0193\n",
      "Iter [  9670/200000] Loss: 0.0173\n",
      "Iter [  9680/200000] Loss: 0.0186\n",
      "Iter [  9690/200000] Loss: 0.0185\n",
      "Iter [  9700/200000] Loss: 0.0190\n",
      "  ‚îî‚îÄ Test Loss: 0.0856\n",
      "Iter [  9710/200000] Loss: 0.0180\n",
      "Iter [  9720/200000] Loss: 0.0186\n",
      "Iter [  9730/200000] Loss: 0.0193\n",
      "Iter [  9740/200000] Loss: 0.0195\n",
      "Iter [  9750/200000] Loss: 0.0193\n",
      "Iter [  9760/200000] Loss: 0.0203\n",
      "Iter [  9770/200000] Loss: 0.0191\n",
      "Iter [  9780/200000] Loss: 0.0180\n",
      "Iter [  9790/200000] Loss: 0.0198\n",
      "Iter [  9800/200000] Loss: 0.0174\n",
      "  ‚îî‚îÄ Test Loss: 0.0936\n",
      "Iter [  9810/200000] Loss: 0.0182\n",
      "Iter [  9820/200000] Loss: 0.0165\n",
      "Iter [  9830/200000] Loss: 0.0159\n",
      "Iter [  9840/200000] Loss: 0.0179\n",
      "Iter [  9850/200000] Loss: 0.0176\n",
      "Iter [  9860/200000] Loss: 0.0183\n",
      "Iter [  9870/200000] Loss: 0.0182\n",
      "Iter [  9880/200000] Loss: 0.0168\n",
      "Iter [  9890/200000] Loss: 0.0176\n",
      "Iter [  9900/200000] Loss: 0.0170\n",
      "  ‚îî‚îÄ Test Loss: 0.0905\n",
      "Iter [  9910/200000] Loss: 0.0178\n",
      "Iter [  9920/200000] Loss: 0.0176\n",
      "Iter [  9930/200000] Loss: 0.0176\n",
      "Iter [  9940/200000] Loss: 0.0181\n",
      "Iter [  9950/200000] Loss: 0.0181\n",
      "Iter [  9960/200000] Loss: 0.0167\n",
      "Iter [  9970/200000] Loss: 0.0186\n",
      "Iter [  9980/200000] Loss: 0.0178\n",
      "Iter [  9990/200000] Loss: 0.0160\n",
      "Iter [ 10000/200000] Loss: 0.0175\n",
      "  ‚îî‚îÄ Test Loss: 0.1015\n",
      "üíæ Checkpoint saved: ./checkpoints\\checkpoint_epoch_10000.pth\n",
      "  ‚îî‚îÄ Checkpoint saved\n",
      "Iter [ 10010/200000] Loss: 0.0167\n",
      "Iter [ 10020/200000] Loss: 0.0173\n",
      "Iter [ 10030/200000] Loss: 0.0155\n",
      "Iter [ 10040/200000] Loss: 0.0157\n",
      "Iter [ 10050/200000] Loss: 0.0159\n",
      "Iter [ 10060/200000] Loss: 0.0168\n",
      "Iter [ 10070/200000] Loss: 0.0174\n",
      "Iter [ 10080/200000] Loss: 0.0172\n",
      "Iter [ 10090/200000] Loss: 0.0158\n",
      "Iter [ 10100/200000] Loss: 0.0164\n",
      "  ‚îî‚îÄ Test Loss: 0.0898\n",
      "Iter [ 10110/200000] Loss: 0.0152\n",
      "Iter [ 10120/200000] Loss: 0.0162\n",
      "Iter [ 10130/200000] Loss: 0.0164\n",
      "Iter [ 10140/200000] Loss: 0.0154\n",
      "Iter [ 10150/200000] Loss: 0.0171\n",
      "Iter [ 10160/200000] Loss: 0.0166\n",
      "Iter [ 10170/200000] Loss: 0.0162\n",
      "Iter [ 10180/200000] Loss: 0.0167\n",
      "Iter [ 10190/200000] Loss: 0.0167\n",
      "Iter [ 10200/200000] Loss: 0.0149\n",
      "  ‚îî‚îÄ Test Loss: 0.1041\n",
      "Iter [ 10210/200000] Loss: 0.0161\n",
      "Iter [ 10220/200000] Loss: 0.0154\n",
      "Iter [ 10230/200000] Loss: 0.0160\n",
      "Iter [ 10240/200000] Loss: 0.0146\n",
      "Iter [ 10250/200000] Loss: 0.0154\n",
      "Iter [ 10260/200000] Loss: 0.0142\n",
      "Iter [ 10270/200000] Loss: 0.0145\n",
      "Iter [ 10280/200000] Loss: 0.0170\n",
      "Iter [ 10290/200000] Loss: 0.0156\n",
      "Iter [ 10300/200000] Loss: 0.0143\n",
      "  ‚îî‚îÄ Test Loss: 0.0909\n",
      "Iter [ 10310/200000] Loss: 0.0164\n",
      "Iter [ 10320/200000] Loss: 0.0151\n",
      "Iter [ 10330/200000] Loss: 0.0154\n",
      "Iter [ 10340/200000] Loss: 0.0150\n",
      "Iter [ 10350/200000] Loss: 0.0147\n",
      "Iter [ 10360/200000] Loss: 0.0160\n",
      "Iter [ 10370/200000] Loss: 0.0150\n",
      "Iter [ 10380/200000] Loss: 0.0148\n",
      "Iter [ 10390/200000] Loss: 0.0152\n",
      "Iter [ 10400/200000] Loss: 0.0151\n",
      "  ‚îî‚îÄ Test Loss: 0.1027\n",
      "Iter [ 10410/200000] Loss: 0.0140\n",
      "Iter [ 10420/200000] Loss: 0.0149\n",
      "Iter [ 10430/200000] Loss: 0.0153\n",
      "Iter [ 10440/200000] Loss: 0.0150\n",
      "Iter [ 10450/200000] Loss: 0.0140\n",
      "Iter [ 10460/200000] Loss: 0.0152\n",
      "Iter [ 10470/200000] Loss: 0.0139\n",
      "Iter [ 10480/200000] Loss: 0.0134\n",
      "Iter [ 10490/200000] Loss: 0.0158\n",
      "Iter [ 10500/200000] Loss: 0.0152\n",
      "  ‚îî‚îÄ Test Loss: 0.1155\n",
      "Iter [ 10510/200000] Loss: 0.0141\n",
      "Iter [ 10520/200000] Loss: 0.0147\n",
      "Iter [ 10530/200000] Loss: 0.0149\n",
      "Iter [ 10540/200000] Loss: 0.0140\n",
      "Iter [ 10550/200000] Loss: 0.0135\n",
      "Iter [ 10560/200000] Loss: 0.0137\n",
      "Iter [ 10570/200000] Loss: 0.0150\n",
      "Iter [ 10580/200000] Loss: 0.0145\n",
      "Iter [ 10590/200000] Loss: 0.0142\n",
      "Iter [ 10600/200000] Loss: 0.0146\n",
      "  ‚îî‚îÄ Test Loss: 0.0981\n",
      "Iter [ 10610/200000] Loss: 0.0139\n",
      "Iter [ 10620/200000] Loss: 0.0133\n",
      "Iter [ 10630/200000] Loss: 0.0133\n",
      "Iter [ 10640/200000] Loss: 0.0139\n",
      "Iter [ 10650/200000] Loss: 0.0144\n",
      "Iter [ 10660/200000] Loss: 0.0137\n",
      "Iter [ 10670/200000] Loss: 0.0138\n",
      "Iter [ 10680/200000] Loss: 0.0129\n",
      "Iter [ 10690/200000] Loss: 0.0122\n",
      "Iter [ 10700/200000] Loss: 0.0143\n",
      "  ‚îî‚îÄ Test Loss: 0.1184\n",
      "Iter [ 10710/200000] Loss: 0.0142\n",
      "Iter [ 10720/200000] Loss: 0.0132\n",
      "Iter [ 10730/200000] Loss: 0.0136\n",
      "Iter [ 10740/200000] Loss: 0.0140\n",
      "Iter [ 10750/200000] Loss: 0.0137\n",
      "Iter [ 10760/200000] Loss: 0.0127\n",
      "Iter [ 10770/200000] Loss: 0.0128\n",
      "Iter [ 10780/200000] Loss: 0.0135\n",
      "Iter [ 10790/200000] Loss: 0.0132\n",
      "Iter [ 10800/200000] Loss: 0.0128\n",
      "  ‚îî‚îÄ Test Loss: 0.1041\n",
      "Iter [ 10810/200000] Loss: 0.0134\n",
      "Iter [ 10820/200000] Loss: 0.0134\n",
      "Iter [ 10830/200000] Loss: 0.0123\n",
      "Iter [ 10840/200000] Loss: 0.0122\n",
      "Iter [ 10850/200000] Loss: 0.0131\n",
      "Iter [ 10860/200000] Loss: 0.0129\n",
      "Iter [ 10870/200000] Loss: 0.0125\n",
      "Iter [ 10880/200000] Loss: 0.0127\n",
      "Iter [ 10890/200000] Loss: 0.0128\n",
      "Iter [ 10900/200000] Loss: 0.0116\n",
      "  ‚îî‚îÄ Test Loss: 0.1093\n",
      "Iter [ 10910/200000] Loss: 0.0132\n",
      "Iter [ 10920/200000] Loss: 0.0128\n",
      "Iter [ 10930/200000] Loss: 0.0127\n",
      "Iter [ 10940/200000] Loss: 0.0122\n",
      "Iter [ 10950/200000] Loss: 0.0135\n",
      "Iter [ 10960/200000] Loss: 0.0126\n",
      "Iter [ 10970/200000] Loss: 0.0132\n",
      "Iter [ 10980/200000] Loss: 0.0121\n",
      "Iter [ 10990/200000] Loss: 0.0129\n",
      "Iter [ 11000/200000] Loss: 0.0129\n",
      "  ‚îî‚îÄ Test Loss: 0.1194\n",
      "Iter [ 11010/200000] Loss: 0.0121\n",
      "Iter [ 11020/200000] Loss: 0.0123\n",
      "Iter [ 11030/200000] Loss: 0.0138\n",
      "Iter [ 11040/200000] Loss: 0.0113\n",
      "Iter [ 11050/200000] Loss: 0.0113\n",
      "Iter [ 11060/200000] Loss: 0.0124\n",
      "Iter [ 11070/200000] Loss: 0.0111\n",
      "Iter [ 11080/200000] Loss: 0.0119\n",
      "Iter [ 11090/200000] Loss: 0.0124\n",
      "Iter [ 11100/200000] Loss: 0.0133\n",
      "  ‚îî‚îÄ Test Loss: 0.1107\n",
      "Iter [ 11110/200000] Loss: 0.0110\n",
      "Iter [ 11120/200000] Loss: 0.0123\n",
      "Iter [ 11130/200000] Loss: 0.0117\n",
      "Iter [ 11140/200000] Loss: 0.0117\n",
      "Iter [ 11150/200000] Loss: 0.0118\n",
      "Iter [ 11160/200000] Loss: 0.0116\n",
      "Iter [ 11170/200000] Loss: 0.0111\n",
      "Iter [ 11180/200000] Loss: 0.0118\n",
      "Iter [ 11190/200000] Loss: 0.0112\n",
      "Iter [ 11200/200000] Loss: 0.0116\n",
      "  ‚îî‚îÄ Test Loss: 0.1043\n",
      "Iter [ 11210/200000] Loss: 0.0119\n",
      "Iter [ 11220/200000] Loss: 0.0114\n",
      "Iter [ 11230/200000] Loss: 0.0109\n",
      "Iter [ 11240/200000] Loss: 0.0127\n",
      "Iter [ 11250/200000] Loss: 0.0111\n",
      "Iter [ 11260/200000] Loss: 0.0101\n",
      "Iter [ 11270/200000] Loss: 0.0105\n",
      "Iter [ 11280/200000] Loss: 0.0099\n",
      "Iter [ 11290/200000] Loss: 0.0108\n",
      "Iter [ 11300/200000] Loss: 0.0107\n",
      "  ‚îî‚îÄ Test Loss: 0.1166\n",
      "Iter [ 11310/200000] Loss: 0.0109\n",
      "Iter [ 11320/200000] Loss: 0.0104\n",
      "Iter [ 11330/200000] Loss: 0.0113\n",
      "Iter [ 11340/200000] Loss: 0.0099\n",
      "Iter [ 11350/200000] Loss: 0.0107\n",
      "Iter [ 11360/200000] Loss: 0.0103\n",
      "Iter [ 11370/200000] Loss: 0.0107\n",
      "Iter [ 11380/200000] Loss: 0.0106\n",
      "Iter [ 11390/200000] Loss: 0.0107\n",
      "Iter [ 11400/200000] Loss: 0.0104\n",
      "  ‚îî‚îÄ Test Loss: 0.1099\n",
      "Iter [ 11410/200000] Loss: 0.0114\n",
      "Iter [ 11420/200000] Loss: 0.0107\n",
      "Iter [ 11430/200000] Loss: 0.0107\n",
      "Iter [ 11440/200000] Loss: 0.0102\n",
      "Iter [ 11450/200000] Loss: 0.0111\n",
      "Iter [ 11460/200000] Loss: 0.0114\n",
      "Iter [ 11470/200000] Loss: 0.0094\n",
      "Iter [ 11480/200000] Loss: 0.0103\n",
      "Iter [ 11490/200000] Loss: 0.0101\n",
      "Iter [ 11500/200000] Loss: 0.0101\n",
      "  ‚îî‚îÄ Test Loss: 0.1102\n",
      "Iter [ 11510/200000] Loss: 0.0100\n",
      "Iter [ 11520/200000] Loss: 0.0097\n",
      "Iter [ 11530/200000] Loss: 0.0093\n",
      "Iter [ 11540/200000] Loss: 0.0106\n",
      "Iter [ 11550/200000] Loss: 0.0093\n",
      "Iter [ 11560/200000] Loss: 0.0100\n",
      "Iter [ 11570/200000] Loss: 0.0101\n",
      "Iter [ 11580/200000] Loss: 0.0102\n",
      "Iter [ 11590/200000] Loss: 0.0098\n",
      "Iter [ 11600/200000] Loss: 0.0102\n",
      "  ‚îî‚îÄ Test Loss: 0.1207\n",
      "Iter [ 11610/200000] Loss: 0.0102\n",
      "Iter [ 11620/200000] Loss: 0.0104\n",
      "Iter [ 11630/200000] Loss: 0.0100\n",
      "Iter [ 11640/200000] Loss: 0.0102\n",
      "Iter [ 11650/200000] Loss: 0.0099\n",
      "Iter [ 11660/200000] Loss: 0.0093\n",
      "Iter [ 11670/200000] Loss: 0.0101\n",
      "Iter [ 11680/200000] Loss: 0.0084\n",
      "Iter [ 11690/200000] Loss: 0.0094\n",
      "Iter [ 11700/200000] Loss: 0.0088\n",
      "  ‚îî‚îÄ Test Loss: 0.1320\n",
      "Iter [ 11710/200000] Loss: 0.0089\n",
      "Iter [ 11720/200000] Loss: 0.0092\n",
      "Iter [ 11730/200000] Loss: 0.0088\n",
      "Iter [ 11740/200000] Loss: 0.0091\n",
      "Iter [ 11750/200000] Loss: 0.0099\n",
      "Iter [ 11760/200000] Loss: 0.0083\n",
      "Iter [ 11770/200000] Loss: 0.0094\n",
      "Iter [ 11780/200000] Loss: 0.0085\n",
      "Iter [ 11790/200000] Loss: 0.0088\n",
      "Iter [ 11800/200000] Loss: 0.0089\n",
      "  ‚îî‚îÄ Test Loss: 0.1194\n",
      "Iter [ 11810/200000] Loss: 0.0093\n",
      "Iter [ 11820/200000] Loss: 0.0094\n",
      "Iter [ 11830/200000] Loss: 0.0095\n",
      "Iter [ 11840/200000] Loss: 0.0093\n",
      "Iter [ 11850/200000] Loss: 0.0097\n",
      "Iter [ 11860/200000] Loss: 0.0098\n",
      "Iter [ 11870/200000] Loss: 0.0082\n",
      "Iter [ 11880/200000] Loss: 0.0089\n",
      "Iter [ 11890/200000] Loss: 0.0075\n",
      "Iter [ 11900/200000] Loss: 0.0085\n",
      "  ‚îî‚îÄ Test Loss: 0.1277\n",
      "Iter [ 11910/200000] Loss: 0.0083\n",
      "Iter [ 11920/200000] Loss: 0.0077\n",
      "Iter [ 11930/200000] Loss: 0.0080\n",
      "Iter [ 11940/200000] Loss: 0.0076\n",
      "Iter [ 11950/200000] Loss: 0.0079\n",
      "Iter [ 11960/200000] Loss: 0.0088\n",
      "Iter [ 11970/200000] Loss: 0.0081\n",
      "Iter [ 11980/200000] Loss: 0.0085\n",
      "Iter [ 11990/200000] Loss: 0.0078\n",
      "Iter [ 12000/200000] Loss: 0.0077\n",
      "  ‚îî‚îÄ Test Loss: 0.1259\n",
      "Iter [ 12010/200000] Loss: 0.0076\n",
      "Iter [ 12020/200000] Loss: 0.0082\n",
      "Iter [ 12030/200000] Loss: 0.0080\n",
      "Iter [ 12040/200000] Loss: 0.0087\n",
      "Iter [ 12050/200000] Loss: 0.0084\n",
      "Iter [ 12060/200000] Loss: 0.0085\n",
      "Iter [ 12070/200000] Loss: 0.0087\n",
      "Iter [ 12080/200000] Loss: 0.0077\n",
      "Iter [ 12090/200000] Loss: 0.0083\n",
      "Iter [ 12100/200000] Loss: 0.0072\n",
      "  ‚îî‚îÄ Test Loss: 0.1338\n",
      "Iter [ 12110/200000] Loss: 0.0076\n",
      "Iter [ 12120/200000] Loss: 0.0074\n",
      "Iter [ 12130/200000] Loss: 0.0074\n",
      "Iter [ 12140/200000] Loss: 0.0072\n",
      "Iter [ 12150/200000] Loss: 0.0069\n",
      "Iter [ 12160/200000] Loss: 0.0071\n",
      "Iter [ 12170/200000] Loss: 0.0075\n",
      "Iter [ 12180/200000] Loss: 0.0076\n",
      "Iter [ 12190/200000] Loss: 0.0073\n",
      "Iter [ 12200/200000] Loss: 0.0075\n",
      "  ‚îî‚îÄ Test Loss: 0.1241\n",
      "Iter [ 12210/200000] Loss: 0.0072\n",
      "Iter [ 12220/200000] Loss: 0.0073\n",
      "Iter [ 12230/200000] Loss: 0.0071\n",
      "Iter [ 12240/200000] Loss: 0.0074\n",
      "Iter [ 12250/200000] Loss: 0.0079\n",
      "Iter [ 12260/200000] Loss: 0.0077\n",
      "Iter [ 12270/200000] Loss: 0.0078\n",
      "Iter [ 12280/200000] Loss: 0.0075\n",
      "Iter [ 12290/200000] Loss: 0.0071\n",
      "Iter [ 12300/200000] Loss: 0.0075\n",
      "  ‚îî‚îÄ Test Loss: 0.1284\n",
      "Iter [ 12310/200000] Loss: 0.0067\n",
      "Iter [ 12320/200000] Loss: 0.0065\n",
      "Iter [ 12330/200000] Loss: 0.0065\n",
      "Iter [ 12340/200000] Loss: 0.0075\n",
      "Iter [ 12350/200000] Loss: 0.0066\n",
      "Iter [ 12360/200000] Loss: 0.0066\n",
      "Iter [ 12370/200000] Loss: 0.0066\n",
      "Iter [ 12380/200000] Loss: 0.0062\n",
      "Iter [ 12390/200000] Loss: 0.0068\n",
      "Iter [ 12400/200000] Loss: 0.0065\n",
      "  ‚îî‚îÄ Test Loss: 0.1389\n",
      "Iter [ 12410/200000] Loss: 0.0069\n",
      "Iter [ 12420/200000] Loss: 0.0067\n",
      "Iter [ 12430/200000] Loss: 0.0066\n",
      "Iter [ 12440/200000] Loss: 0.0072\n",
      "Iter [ 12450/200000] Loss: 0.0069\n",
      "Iter [ 12460/200000] Loss: 0.0076\n",
      "Iter [ 12470/200000] Loss: 0.0066\n",
      "Iter [ 12480/200000] Loss: 0.0075\n",
      "Iter [ 12490/200000] Loss: 0.0065\n",
      "Iter [ 12500/200000] Loss: 0.0064\n",
      "  ‚îî‚îÄ Test Loss: 0.1364\n",
      "Iter [ 12510/200000] Loss: 0.0070\n",
      "Iter [ 12520/200000] Loss: 0.0065\n",
      "Iter [ 12530/200000] Loss: 0.0064\n",
      "Iter [ 12540/200000] Loss: 0.0064\n",
      "Iter [ 12550/200000] Loss: 0.0065\n",
      "Iter [ 12560/200000] Loss: 0.0062\n",
      "Iter [ 12570/200000] Loss: 0.0063\n",
      "Iter [ 12580/200000] Loss: 0.0060\n",
      "Iter [ 12590/200000] Loss: 0.0061\n",
      "Iter [ 12600/200000] Loss: 0.0063\n",
      "  ‚îî‚îÄ Test Loss: 0.1367\n",
      "Iter [ 12610/200000] Loss: 0.0064\n",
      "Iter [ 12620/200000] Loss: 0.0063\n",
      "Iter [ 12630/200000] Loss: 0.0067\n",
      "Iter [ 12640/200000] Loss: 0.0067\n",
      "Iter [ 12650/200000] Loss: 0.0068\n",
      "Iter [ 12660/200000] Loss: 0.0068\n",
      "Iter [ 12670/200000] Loss: 0.0080\n",
      "Iter [ 12680/200000] Loss: 0.0068\n",
      "Iter [ 12690/200000] Loss: 0.0075\n",
      "Iter [ 12700/200000] Loss: 0.0064\n",
      "  ‚îî‚îÄ Test Loss: 0.1334\n",
      "Iter [ 12710/200000] Loss: 0.0062\n",
      "Iter [ 12720/200000] Loss: 0.0061\n",
      "Iter [ 12730/200000] Loss: 0.0068\n",
      "Iter [ 12740/200000] Loss: 0.0060\n",
      "Iter [ 12750/200000] Loss: 0.0058\n",
      "Iter [ 12760/200000] Loss: 0.0063\n",
      "Iter [ 12770/200000] Loss: 0.0057\n",
      "Iter [ 12780/200000] Loss: 0.0058\n",
      "Iter [ 12790/200000] Loss: 0.0064\n",
      "Iter [ 12800/200000] Loss: 0.0058\n",
      "  ‚îî‚îÄ Test Loss: 0.1361\n",
      "Iter [ 12810/200000] Loss: 0.0057\n",
      "Iter [ 12820/200000] Loss: 0.0058\n",
      "Iter [ 12830/200000] Loss: 0.0061\n",
      "Iter [ 12840/200000] Loss: 0.0064\n",
      "Iter [ 12850/200000] Loss: 0.0062\n",
      "Iter [ 12860/200000] Loss: 0.0065\n",
      "Iter [ 12870/200000] Loss: 0.0073\n",
      "Iter [ 12880/200000] Loss: 0.0075\n",
      "Iter [ 12890/200000] Loss: 0.0067\n",
      "Iter [ 12900/200000] Loss: 0.0076\n",
      "  ‚îî‚îÄ Test Loss: 0.1371\n",
      "Iter [ 12910/200000] Loss: 0.0073\n",
      "Iter [ 12920/200000] Loss: 0.0061\n",
      "Iter [ 12930/200000] Loss: 0.0059\n",
      "Iter [ 12940/200000] Loss: 0.0065\n",
      "Iter [ 12950/200000] Loss: 0.0055\n",
      "Iter [ 12960/200000] Loss: 0.0062\n",
      "Iter [ 12970/200000] Loss: 0.0062\n",
      "Iter [ 12980/200000] Loss: 0.0056\n",
      "Iter [ 12990/200000] Loss: 0.0055\n",
      "Iter [ 13000/200000] Loss: 0.0064\n",
      "  ‚îî‚îÄ Test Loss: 0.1481\n",
      "Iter [ 13010/200000] Loss: 0.0056\n",
      "Iter [ 13020/200000] Loss: 0.0062\n",
      "Iter [ 13030/200000] Loss: 0.0054\n",
      "Iter [ 13040/200000] Loss: 0.0065\n",
      "Iter [ 13050/200000] Loss: 0.0066\n",
      "Iter [ 13060/200000] Loss: 0.0072\n",
      "Iter [ 13070/200000] Loss: 0.0064\n",
      "Iter [ 13080/200000] Loss: 0.0068\n",
      "Iter [ 13090/200000] Loss: 0.0074\n",
      "Iter [ 13100/200000] Loss: 0.0063\n",
      "  ‚îî‚îÄ Test Loss: 0.1288\n",
      "Iter [ 13110/200000] Loss: 0.0072\n",
      "Iter [ 13120/200000] Loss: 0.0067\n",
      "Iter [ 13130/200000] Loss: 0.0059\n",
      "Iter [ 13140/200000] Loss: 0.0063\n",
      "Iter [ 13150/200000] Loss: 0.0066\n",
      "Iter [ 13160/200000] Loss: 0.0053\n",
      "Iter [ 13170/200000] Loss: 0.0057\n",
      "Iter [ 13180/200000] Loss: 0.0051\n",
      "Iter [ 13190/200000] Loss: 0.0057\n",
      "Iter [ 13200/200000] Loss: 0.0049\n",
      "  ‚îî‚îÄ Test Loss: 0.1441\n",
      "Iter [ 13210/200000] Loss: 0.0063\n",
      "Iter [ 13220/200000] Loss: 0.0056\n",
      "Iter [ 13230/200000] Loss: 0.0057\n",
      "Iter [ 13240/200000] Loss: 0.0058\n",
      "Iter [ 13250/200000] Loss: 0.0060\n",
      "Iter [ 13260/200000] Loss: 0.0062\n",
      "Iter [ 13270/200000] Loss: 0.0064\n",
      "Iter [ 13280/200000] Loss: 0.0056\n",
      "Iter [ 13290/200000] Loss: 0.0060\n",
      "Iter [ 13300/200000] Loss: 0.0059\n",
      "  ‚îî‚îÄ Test Loss: 0.1301\n",
      "Iter [ 13310/200000] Loss: 0.0056\n",
      "Iter [ 13320/200000] Loss: 0.0060\n",
      "Iter [ 13330/200000] Loss: 0.0051\n",
      "Iter [ 13340/200000] Loss: 0.0057\n",
      "Iter [ 13350/200000] Loss: 0.0045\n",
      "Iter [ 13360/200000] Loss: 0.0061\n",
      "Iter [ 13370/200000] Loss: 0.0061\n",
      "Iter [ 13380/200000] Loss: 0.0058\n",
      "Iter [ 13390/200000] Loss: 0.0053\n",
      "Iter [ 13400/200000] Loss: 0.0053\n",
      "  ‚îî‚îÄ Test Loss: 0.1560\n",
      "Iter [ 13410/200000] Loss: 0.0045\n",
      "Iter [ 13420/200000] Loss: 0.0064\n",
      "Iter [ 13430/200000] Loss: 0.0048\n",
      "Iter [ 13440/200000] Loss: 0.0056\n",
      "Iter [ 13450/200000] Loss: 0.0052\n",
      "Iter [ 13460/200000] Loss: 0.0059\n",
      "Iter [ 13470/200000] Loss: 0.0055\n",
      "Iter [ 13480/200000] Loss: 0.0062\n",
      "Iter [ 13490/200000] Loss: 0.0049\n",
      "Iter [ 13500/200000] Loss: 0.0058\n",
      "  ‚îî‚îÄ Test Loss: 0.1378\n",
      "Iter [ 13510/200000] Loss: 0.0051\n",
      "Iter [ 13520/200000] Loss: 0.0048\n",
      "Iter [ 13530/200000] Loss: 0.0055\n",
      "Iter [ 13540/200000] Loss: 0.0048\n",
      "Iter [ 13550/200000] Loss: 0.0054\n",
      "Iter [ 13560/200000] Loss: 0.0045\n",
      "Iter [ 13570/200000] Loss: 0.0053\n",
      "Iter [ 13580/200000] Loss: 0.0054\n",
      "Iter [ 13590/200000] Loss: 0.0056\n",
      "Iter [ 13600/200000] Loss: 0.0049\n",
      "  ‚îî‚îÄ Test Loss: 0.1527\n",
      "Iter [ 13610/200000] Loss: 0.0048\n",
      "Iter [ 13620/200000] Loss: 0.0043\n",
      "Iter [ 13630/200000] Loss: 0.0053\n",
      "Iter [ 13640/200000] Loss: 0.0052\n",
      "Iter [ 13650/200000] Loss: 0.0053\n",
      "Iter [ 13660/200000] Loss: 0.0047\n",
      "Iter [ 13670/200000] Loss: 0.0047\n",
      "Iter [ 13680/200000] Loss: 0.0043\n",
      "Iter [ 13690/200000] Loss: 0.0052\n",
      "Iter [ 13700/200000] Loss: 0.0045\n",
      "  ‚îî‚îÄ Test Loss: 0.1538\n",
      "Iter [ 13710/200000] Loss: 0.0051\n",
      "Iter [ 13720/200000] Loss: 0.0048\n",
      "Iter [ 13730/200000] Loss: 0.0048\n",
      "Iter [ 13740/200000] Loss: 0.0051\n",
      "Iter [ 13750/200000] Loss: 0.0048\n",
      "Iter [ 13760/200000] Loss: 0.0055\n",
      "Iter [ 13770/200000] Loss: 0.0044\n",
      "Iter [ 13780/200000] Loss: 0.0053\n",
      "Iter [ 13790/200000] Loss: 0.0048\n",
      "Iter [ 13800/200000] Loss: 0.0052\n",
      "  ‚îî‚îÄ Test Loss: 0.1620\n",
      "Iter [ 13810/200000] Loss: 0.0050\n",
      "Iter [ 13820/200000] Loss: 0.0045\n",
      "Iter [ 13830/200000] Loss: 0.0042\n",
      "Iter [ 13840/200000] Loss: 0.0051\n",
      "Iter [ 13850/200000] Loss: 0.0046\n",
      "Iter [ 13860/200000] Loss: 0.0048\n",
      "Iter [ 13870/200000] Loss: 0.0043\n",
      "Iter [ 13880/200000] Loss: 0.0042\n",
      "Iter [ 13890/200000] Loss: 0.0041\n",
      "Iter [ 13900/200000] Loss: 0.0048\n",
      "  ‚îî‚îÄ Test Loss: 0.1498\n",
      "Iter [ 13910/200000] Loss: 0.0044\n",
      "Iter [ 13920/200000] Loss: 0.0051\n",
      "Iter [ 13930/200000] Loss: 0.0042\n",
      "Iter [ 13940/200000] Loss: 0.0043\n",
      "Iter [ 13950/200000] Loss: 0.0046\n",
      "Iter [ 13960/200000] Loss: 0.0039\n",
      "Iter [ 13970/200000] Loss: 0.0052\n",
      "Iter [ 13980/200000] Loss: 0.0044\n",
      "Iter [ 13990/200000] Loss: 0.0056\n",
      "Iter [ 14000/200000] Loss: 0.0045\n",
      "  ‚îî‚îÄ Test Loss: 0.1662\n",
      "Iter [ 14010/200000] Loss: 0.0044\n",
      "Iter [ 14020/200000] Loss: 0.0049\n",
      "Iter [ 14030/200000] Loss: 0.0044\n",
      "Iter [ 14040/200000] Loss: 0.0043\n",
      "Iter [ 14050/200000] Loss: 0.0049\n",
      "Iter [ 14060/200000] Loss: 0.0041\n",
      "Iter [ 14070/200000] Loss: 0.0042\n",
      "Iter [ 14080/200000] Loss: 0.0049\n",
      "Iter [ 14090/200000] Loss: 0.0038\n",
      "Iter [ 14100/200000] Loss: 0.0040\n",
      "  ‚îî‚îÄ Test Loss: 0.1488\n",
      "Iter [ 14110/200000] Loss: 0.0048\n",
      "Iter [ 14120/200000] Loss: 0.0039\n",
      "Iter [ 14130/200000] Loss: 0.0051\n",
      "Iter [ 14140/200000] Loss: 0.0042\n",
      "Iter [ 14150/200000] Loss: 0.0046\n",
      "Iter [ 14160/200000] Loss: 0.0048\n",
      "Iter [ 14170/200000] Loss: 0.0039\n",
      "Iter [ 14180/200000] Loss: 0.0045\n",
      "Iter [ 14190/200000] Loss: 0.0041\n",
      "Iter [ 14200/200000] Loss: 0.0049\n",
      "  ‚îî‚îÄ Test Loss: 0.1527\n",
      "Iter [ 14210/200000] Loss: 0.0046\n",
      "Iter [ 14220/200000] Loss: 0.0053\n",
      "Iter [ 14230/200000] Loss: 0.0048\n",
      "Iter [ 14240/200000] Loss: 0.0044\n",
      "Iter [ 14250/200000] Loss: 0.0043\n",
      "Iter [ 14260/200000] Loss: 0.0049\n",
      "Iter [ 14270/200000] Loss: 0.0046\n",
      "Iter [ 14280/200000] Loss: 0.0041\n",
      "Iter [ 14290/200000] Loss: 0.0051\n",
      "Iter [ 14300/200000] Loss: 0.0042\n",
      "  ‚îî‚îÄ Test Loss: 0.1612\n",
      "Iter [ 14310/200000] Loss: 0.0046\n",
      "Iter [ 14320/200000] Loss: 0.0045\n",
      "Iter [ 14330/200000] Loss: 0.0041\n",
      "Iter [ 14340/200000] Loss: 0.0048\n",
      "Iter [ 14350/200000] Loss: 0.0040\n",
      "Iter [ 14360/200000] Loss: 0.0044\n",
      "Iter [ 14370/200000] Loss: 0.0052\n",
      "Iter [ 14380/200000] Loss: 0.0044\n",
      "Iter [ 14390/200000] Loss: 0.0051\n",
      "Iter [ 14400/200000] Loss: 0.0042\n",
      "  ‚îî‚îÄ Test Loss: 0.1553\n",
      "Iter [ 14410/200000] Loss: 0.0044\n",
      "Iter [ 14420/200000] Loss: 0.0037\n",
      "Iter [ 14430/200000] Loss: 0.0044\n",
      "Iter [ 14440/200000] Loss: 0.0040\n",
      "Iter [ 14450/200000] Loss: 0.0041\n",
      "Iter [ 14460/200000] Loss: 0.0041\n",
      "Iter [ 14470/200000] Loss: 0.0044\n",
      "Iter [ 14480/200000] Loss: 0.0039\n",
      "Iter [ 14490/200000] Loss: 0.0039\n",
      "Iter [ 14500/200000] Loss: 0.0044\n",
      "  ‚îî‚îÄ Test Loss: 0.1428\n",
      "Iter [ 14510/200000] Loss: 0.0040\n",
      "Iter [ 14520/200000] Loss: 0.0042\n",
      "Iter [ 14530/200000] Loss: 0.0045\n",
      "Iter [ 14540/200000] Loss: 0.0042\n",
      "Iter [ 14550/200000] Loss: 0.0043\n",
      "Iter [ 14560/200000] Loss: 0.0043\n",
      "Iter [ 14570/200000] Loss: 0.0041\n",
      "Iter [ 14580/200000] Loss: 0.0041\n",
      "Iter [ 14590/200000] Loss: 0.0038\n",
      "Iter [ 14600/200000] Loss: 0.0045\n",
      "  ‚îî‚îÄ Test Loss: 0.1599\n",
      "Iter [ 14610/200000] Loss: 0.0039\n",
      "Iter [ 14620/200000] Loss: 0.0043\n",
      "Iter [ 14630/200000] Loss: 0.0035\n",
      "Iter [ 14640/200000] Loss: 0.0037\n",
      "Iter [ 14650/200000] Loss: 0.0034\n",
      "Iter [ 14660/200000] Loss: 0.0036\n",
      "Iter [ 14670/200000] Loss: 0.0038\n",
      "Iter [ 14680/200000] Loss: 0.0040\n",
      "Iter [ 14690/200000] Loss: 0.0040\n",
      "Iter [ 14700/200000] Loss: 0.0037\n",
      "  ‚îî‚îÄ Test Loss: 0.1557\n",
      "Iter [ 14710/200000] Loss: 0.0032\n",
      "Iter [ 14720/200000] Loss: 0.0035\n",
      "Iter [ 14730/200000] Loss: 0.0036\n",
      "Iter [ 14740/200000] Loss: 0.0040\n",
      "Iter [ 14750/200000] Loss: 0.0041\n",
      "Iter [ 14760/200000] Loss: 0.0036\n",
      "Iter [ 14770/200000] Loss: 0.0038\n",
      "Iter [ 14780/200000] Loss: 0.0035\n",
      "Iter [ 14790/200000] Loss: 0.0034\n",
      "Iter [ 14800/200000] Loss: 0.0033\n",
      "  ‚îî‚îÄ Test Loss: 0.1584\n",
      "Iter [ 14810/200000] Loss: 0.0038\n",
      "Iter [ 14820/200000] Loss: 0.0037\n",
      "Iter [ 14830/200000] Loss: 0.0038\n",
      "Iter [ 14840/200000] Loss: 0.0031\n",
      "Iter [ 14850/200000] Loss: 0.0035\n",
      "Iter [ 14860/200000] Loss: 0.0032\n",
      "Iter [ 14870/200000] Loss: 0.0031\n",
      "Iter [ 14880/200000] Loss: 0.0034\n",
      "Iter [ 14890/200000] Loss: 0.0030\n",
      "Iter [ 14900/200000] Loss: 0.0034\n",
      "  ‚îî‚îÄ Test Loss: 0.1551\n",
      "Iter [ 14910/200000] Loss: 0.0031\n",
      "Iter [ 14920/200000] Loss: 0.0031\n",
      "Iter [ 14930/200000] Loss: 0.0029\n",
      "Iter [ 14940/200000] Loss: 0.0033\n",
      "Iter [ 14950/200000] Loss: 0.0038\n",
      "Iter [ 14960/200000] Loss: 0.0035\n",
      "Iter [ 14970/200000] Loss: 0.0036\n",
      "Iter [ 14980/200000] Loss: 0.0032\n",
      "Iter [ 14990/200000] Loss: 0.0034\n",
      "Iter [ 15000/200000] Loss: 0.0031\n",
      "  ‚îî‚îÄ Test Loss: 0.1589\n",
      "üíæ Checkpoint saved: ./checkpoints\\checkpoint_epoch_15000.pth\n",
      "  ‚îî‚îÄ Checkpoint saved\n",
      "Iter [ 15010/200000] Loss: 0.0029\n",
      "Iter [ 15020/200000] Loss: 0.0031\n",
      "Iter [ 15030/200000] Loss: 0.0034\n",
      "Iter [ 15040/200000] Loss: 0.0028\n",
      "Iter [ 15050/200000] Loss: 0.0029\n",
      "Iter [ 15060/200000] Loss: 0.0029\n",
      "Iter [ 15070/200000] Loss: 0.0028\n",
      "Iter [ 15080/200000] Loss: 0.0026\n",
      "Iter [ 15090/200000] Loss: 0.0030\n",
      "Iter [ 15100/200000] Loss: 0.0032\n",
      "  ‚îî‚îÄ Test Loss: 0.1602\n",
      "Iter [ 15110/200000] Loss: 0.0032\n",
      "Iter [ 15120/200000] Loss: 0.0026\n",
      "Iter [ 15130/200000] Loss: 0.0030\n",
      "Iter [ 15140/200000] Loss: 0.0031\n",
      "Iter [ 15150/200000] Loss: 0.0034\n",
      "Iter [ 15160/200000] Loss: 0.0034\n",
      "Iter [ 15170/200000] Loss: 0.0037\n",
      "Iter [ 15180/200000] Loss: 0.0033\n",
      "Iter [ 15190/200000] Loss: 0.0029\n",
      "Iter [ 15200/200000] Loss: 0.0033\n",
      "  ‚îî‚îÄ Test Loss: 0.1467\n",
      "Iter [ 15210/200000] Loss: 0.0030\n",
      "Iter [ 15220/200000] Loss: 0.0033\n",
      "Iter [ 15230/200000] Loss: 0.0030\n",
      "Iter [ 15240/200000] Loss: 0.0031\n",
      "Iter [ 15250/200000] Loss: 0.0028\n",
      "Iter [ 15260/200000] Loss: 0.0028\n",
      "Iter [ 15270/200000] Loss: 0.0029\n",
      "Iter [ 15280/200000] Loss: 0.0028\n",
      "Iter [ 15290/200000] Loss: 0.0026\n",
      "Iter [ 15300/200000] Loss: 0.0030\n",
      "  ‚îî‚îÄ Test Loss: 0.1676\n",
      "Iter [ 15310/200000] Loss: 0.0029\n",
      "Iter [ 15320/200000] Loss: 0.0030\n",
      "Iter [ 15330/200000] Loss: 0.0026\n",
      "Iter [ 15340/200000] Loss: 0.0028\n",
      "Iter [ 15350/200000] Loss: 0.0033\n",
      "Iter [ 15360/200000] Loss: 0.0029\n",
      "Iter [ 15370/200000] Loss: 0.0034\n",
      "Iter [ 15380/200000] Loss: 0.0033\n",
      "Iter [ 15390/200000] Loss: 0.0031\n",
      "Iter [ 15400/200000] Loss: 0.0028\n",
      "  ‚îî‚îÄ Test Loss: 0.1526\n",
      "Iter [ 15410/200000] Loss: 0.0034\n",
      "Iter [ 15420/200000] Loss: 0.0027\n",
      "Iter [ 15430/200000] Loss: 0.0034\n",
      "Iter [ 15440/200000] Loss: 0.0030\n",
      "Iter [ 15450/200000] Loss: 0.0031\n",
      "Iter [ 15460/200000] Loss: 0.0031\n",
      "Iter [ 15470/200000] Loss: 0.0029\n",
      "Iter [ 15480/200000] Loss: 0.0034\n",
      "Iter [ 15490/200000] Loss: 0.0030\n",
      "Iter [ 15500/200000] Loss: 0.0028\n",
      "  ‚îî‚îÄ Test Loss: 0.1851\n",
      "Iter [ 15510/200000] Loss: 0.0033\n",
      "Iter [ 15520/200000] Loss: 0.0028\n",
      "Iter [ 15530/200000] Loss: 0.0031\n",
      "Iter [ 15540/200000] Loss: 0.0031\n",
      "Iter [ 15550/200000] Loss: 0.0025\n",
      "Iter [ 15560/200000] Loss: 0.0034\n",
      "Iter [ 15570/200000] Loss: 0.0033\n",
      "Iter [ 15580/200000] Loss: 0.0032\n",
      "Iter [ 15590/200000] Loss: 0.0038\n",
      "Iter [ 15600/200000] Loss: 0.0030\n",
      "  ‚îî‚îÄ Test Loss: 0.1644\n",
      "Iter [ 15610/200000] Loss: 0.0031\n",
      "Iter [ 15620/200000] Loss: 0.0036\n",
      "Iter [ 15630/200000] Loss: 0.0030\n",
      "Iter [ 15640/200000] Loss: 0.0035\n",
      "Iter [ 15650/200000] Loss: 0.0030\n",
      "Iter [ 15660/200000] Loss: 0.0030\n",
      "Iter [ 15670/200000] Loss: 0.0032\n",
      "Iter [ 15680/200000] Loss: 0.0025\n",
      "Iter [ 15690/200000] Loss: 0.0034\n",
      "Iter [ 15700/200000] Loss: 0.0030\n",
      "  ‚îî‚îÄ Test Loss: 0.1701\n",
      "Iter [ 15710/200000] Loss: 0.0032\n",
      "Iter [ 15720/200000] Loss: 0.0032\n",
      "Iter [ 15730/200000] Loss: 0.0031\n",
      "Iter [ 15740/200000] Loss: 0.0030\n",
      "Iter [ 15750/200000] Loss: 0.0029\n",
      "Iter [ 15760/200000] Loss: 0.0026\n",
      "Iter [ 15770/200000] Loss: 0.0031\n",
      "Iter [ 15780/200000] Loss: 0.0033\n",
      "Iter [ 15790/200000] Loss: 0.0032\n",
      "Iter [ 15800/200000] Loss: 0.0031\n",
      "  ‚îî‚îÄ Test Loss: 0.1660\n",
      "Iter [ 15810/200000] Loss: 0.0029\n",
      "Iter [ 15820/200000] Loss: 0.0029\n",
      "Iter [ 15830/200000] Loss: 0.0033\n",
      "Iter [ 15840/200000] Loss: 0.0026\n",
      "Iter [ 15850/200000] Loss: 0.0038\n",
      "Iter [ 15860/200000] Loss: 0.0036\n",
      "Iter [ 15870/200000] Loss: 0.0036\n",
      "Iter [ 15880/200000] Loss: 0.0033\n",
      "Iter [ 15890/200000] Loss: 0.0025\n",
      "Iter [ 15900/200000] Loss: 0.0028\n",
      "  ‚îî‚îÄ Test Loss: 0.1667\n",
      "Iter [ 15910/200000] Loss: 0.0029\n",
      "Iter [ 15920/200000] Loss: 0.0032\n",
      "Iter [ 15930/200000] Loss: 0.0034\n",
      "Iter [ 15940/200000] Loss: 0.0030\n",
      "Iter [ 15950/200000] Loss: 0.0031\n",
      "Iter [ 15960/200000] Loss: 0.0026\n",
      "Iter [ 15970/200000] Loss: 0.0031\n",
      "Iter [ 15980/200000] Loss: 0.0025\n",
      "Iter [ 15990/200000] Loss: 0.0032\n",
      "Iter [ 16000/200000] Loss: 0.0027\n",
      "  ‚îî‚îÄ Test Loss: 0.1772\n",
      "Iter [ 16010/200000] Loss: 0.0032\n",
      "Iter [ 16020/200000] Loss: 0.0025\n",
      "Iter [ 16030/200000] Loss: 0.0027\n",
      "Iter [ 16040/200000] Loss: 0.0033\n",
      "Iter [ 16050/200000] Loss: 0.0030\n",
      "Iter [ 16060/200000] Loss: 0.0032\n",
      "Iter [ 16070/200000] Loss: 0.0032\n",
      "Iter [ 16080/200000] Loss: 0.0029\n",
      "Iter [ 16090/200000] Loss: 0.0031\n",
      "Iter [ 16100/200000] Loss: 0.0029\n",
      "  ‚îî‚îÄ Test Loss: 0.1849\n",
      "Iter [ 16110/200000] Loss: 0.0028\n",
      "Iter [ 16120/200000] Loss: 0.0027\n",
      "Iter [ 16130/200000] Loss: 0.0029\n",
      "Iter [ 16140/200000] Loss: 0.0029\n",
      "Iter [ 16150/200000] Loss: 0.0025\n",
      "Iter [ 16160/200000] Loss: 0.0024\n",
      "Iter [ 16170/200000] Loss: 0.0026\n",
      "Iter [ 16180/200000] Loss: 0.0025\n",
      "Iter [ 16190/200000] Loss: 0.0026\n",
      "Iter [ 16200/200000] Loss: 0.0027\n",
      "  ‚îî‚îÄ Test Loss: 0.1656\n",
      "Iter [ 16210/200000] Loss: 0.0028\n",
      "Iter [ 16220/200000] Loss: 0.0028\n",
      "Iter [ 16230/200000] Loss: 0.0024\n",
      "Iter [ 16240/200000] Loss: 0.0026\n",
      "Iter [ 16250/200000] Loss: 0.0028\n",
      "Iter [ 16260/200000] Loss: 0.0024\n",
      "Iter [ 16270/200000] Loss: 0.0028\n",
      "Iter [ 16280/200000] Loss: 0.0030\n",
      "Iter [ 16290/200000] Loss: 0.0027\n",
      "Iter [ 16300/200000] Loss: 0.0031\n",
      "  ‚îî‚îÄ Test Loss: 0.1751\n",
      "Iter [ 16310/200000] Loss: 0.0025\n",
      "Iter [ 16320/200000] Loss: 0.0028\n",
      "Iter [ 16330/200000] Loss: 0.0025\n",
      "Iter [ 16340/200000] Loss: 0.0026\n",
      "Iter [ 16350/200000] Loss: 0.0031\n",
      "Iter [ 16360/200000] Loss: 0.0024\n",
      "Iter [ 16370/200000] Loss: 0.0024\n",
      "Iter [ 16380/200000] Loss: 0.0023\n",
      "Iter [ 16390/200000] Loss: 0.0023\n",
      "Iter [ 16400/200000] Loss: 0.0027\n",
      "  ‚îî‚îÄ Test Loss: 0.1701\n",
      "Iter [ 16410/200000] Loss: 0.0025\n",
      "Iter [ 16420/200000] Loss: 0.0025\n",
      "Iter [ 16430/200000] Loss: 0.0025\n",
      "Iter [ 16440/200000] Loss: 0.0024\n",
      "Iter [ 16450/200000] Loss: 0.0026\n",
      "Iter [ 16460/200000] Loss: 0.0028\n",
      "Iter [ 16470/200000] Loss: 0.0027\n",
      "Iter [ 16480/200000] Loss: 0.0030\n",
      "Iter [ 16490/200000] Loss: 0.0025\n",
      "Iter [ 16500/200000] Loss: 0.0026\n",
      "  ‚îî‚îÄ Test Loss: 0.1718\n",
      "Iter [ 16510/200000] Loss: 0.0028\n",
      "Iter [ 16520/200000] Loss: 0.0025\n",
      "Iter [ 16530/200000] Loss: 0.0024\n",
      "Iter [ 16540/200000] Loss: 0.0028\n",
      "Iter [ 16550/200000] Loss: 0.0024\n",
      "Iter [ 16560/200000] Loss: 0.0029\n",
      "Iter [ 16570/200000] Loss: 0.0023\n",
      "Iter [ 16580/200000] Loss: 0.0025\n",
      "Iter [ 16590/200000] Loss: 0.0020\n",
      "Iter [ 16600/200000] Loss: 0.0026\n",
      "  ‚îî‚îÄ Test Loss: 0.1704\n",
      "Iter [ 16610/200000] Loss: 0.0026\n",
      "Iter [ 16620/200000] Loss: 0.0028\n",
      "Iter [ 16630/200000] Loss: 0.0027\n",
      "Iter [ 16640/200000] Loss: 0.0026\n",
      "Iter [ 16650/200000] Loss: 0.0024\n",
      "Iter [ 16660/200000] Loss: 0.0024\n",
      "Iter [ 16670/200000] Loss: 0.0028\n",
      "Iter [ 16680/200000] Loss: 0.0026\n",
      "Iter [ 16690/200000] Loss: 0.0033\n",
      "Iter [ 16700/200000] Loss: 0.0031\n",
      "  ‚îî‚îÄ Test Loss: 0.1815\n",
      "Iter [ 16710/200000] Loss: 0.0027\n",
      "Iter [ 16720/200000] Loss: 0.0025\n",
      "Iter [ 16730/200000] Loss: 0.0028\n",
      "Iter [ 16740/200000] Loss: 0.0025\n",
      "Iter [ 16750/200000] Loss: 0.0022\n",
      "Iter [ 16760/200000] Loss: 0.0029\n",
      "Iter [ 16770/200000] Loss: 0.0027\n",
      "Iter [ 16780/200000] Loss: 0.0028\n",
      "Iter [ 16790/200000] Loss: 0.0026\n",
      "Iter [ 16800/200000] Loss: 0.0024\n",
      "  ‚îî‚îÄ Test Loss: 0.1753\n",
      "Iter [ 16810/200000] Loss: 0.0026\n",
      "Iter [ 16820/200000] Loss: 0.0028\n",
      "Iter [ 16830/200000] Loss: 0.0030\n",
      "Iter [ 16840/200000] Loss: 0.0026\n",
      "Iter [ 16850/200000] Loss: 0.0026\n",
      "Iter [ 16860/200000] Loss: 0.0021\n",
      "Iter [ 16870/200000] Loss: 0.0025\n",
      "Iter [ 16880/200000] Loss: 0.0026\n",
      "Iter [ 16890/200000] Loss: 0.0026\n",
      "Iter [ 16900/200000] Loss: 0.0028\n",
      "  ‚îî‚îÄ Test Loss: 0.1801\n",
      "Iter [ 16910/200000] Loss: 0.0026\n",
      "Iter [ 16920/200000] Loss: 0.0024\n",
      "Iter [ 16930/200000] Loss: 0.0020\n",
      "Iter [ 16940/200000] Loss: 0.0027\n",
      "Iter [ 16950/200000] Loss: 0.0024\n",
      "Iter [ 16960/200000] Loss: 0.0025\n",
      "Iter [ 16970/200000] Loss: 0.0032\n",
      "Iter [ 16980/200000] Loss: 0.0029\n",
      "Iter [ 16990/200000] Loss: 0.0027\n",
      "Iter [ 17000/200000] Loss: 0.0026\n",
      "  ‚îî‚îÄ Test Loss: 0.1629\n",
      "Iter [ 17010/200000] Loss: 0.0029\n",
      "Iter [ 17020/200000] Loss: 0.0025\n",
      "Iter [ 17030/200000] Loss: 0.0024\n",
      "Iter [ 17040/200000] Loss: 0.0026\n",
      "Iter [ 17050/200000] Loss: 0.0031\n",
      "Iter [ 17060/200000] Loss: 0.0024\n",
      "Iter [ 17070/200000] Loss: 0.0022\n",
      "Iter [ 17080/200000] Loss: 0.0023\n",
      "Iter [ 17090/200000] Loss: 0.0031\n",
      "Iter [ 17100/200000] Loss: 0.0027\n",
      "  ‚îî‚îÄ Test Loss: 0.1860\n",
      "Iter [ 17110/200000] Loss: 0.0027\n",
      "Iter [ 17120/200000] Loss: 0.0026\n",
      "Iter [ 17130/200000] Loss: 0.0025\n",
      "Iter [ 17140/200000] Loss: 0.0022\n",
      "Iter [ 17150/200000] Loss: 0.0025\n",
      "Iter [ 17160/200000] Loss: 0.0023\n",
      "Iter [ 17170/200000] Loss: 0.0022\n",
      "Iter [ 17180/200000] Loss: 0.0030\n",
      "Iter [ 17190/200000] Loss: 0.0030\n",
      "Iter [ 17200/200000] Loss: 0.0027\n",
      "  ‚îî‚îÄ Test Loss: 0.1767\n",
      "Iter [ 17210/200000] Loss: 0.0026\n",
      "Iter [ 17220/200000] Loss: 0.0030\n",
      "Iter [ 17230/200000] Loss: 0.0024\n",
      "Iter [ 17240/200000] Loss: 0.0024\n",
      "Iter [ 17250/200000] Loss: 0.0028\n",
      "Iter [ 17260/200000] Loss: 0.0030\n",
      "Iter [ 17270/200000] Loss: 0.0029\n",
      "Iter [ 17280/200000] Loss: 0.0026\n",
      "Iter [ 17290/200000] Loss: 0.0027\n",
      "Iter [ 17300/200000] Loss: 0.0026\n",
      "  ‚îî‚îÄ Test Loss: 0.1716\n",
      "Iter [ 17310/200000] Loss: 0.0029\n",
      "Iter [ 17320/200000] Loss: 0.0027\n",
      "Iter [ 17330/200000] Loss: 0.0027\n",
      "Iter [ 17340/200000] Loss: 0.0021\n",
      "Iter [ 17350/200000] Loss: 0.0025\n",
      "Iter [ 17360/200000] Loss: 0.0027\n",
      "Iter [ 17370/200000] Loss: 0.0025\n",
      "Iter [ 17380/200000] Loss: 0.0023\n",
      "Iter [ 17390/200000] Loss: 0.0029\n",
      "Iter [ 17400/200000] Loss: 0.0029\n",
      "  ‚îî‚îÄ Test Loss: 0.1761\n",
      "Iter [ 17410/200000] Loss: 0.0025\n",
      "Iter [ 17420/200000] Loss: 0.0026\n",
      "Iter [ 17430/200000] Loss: 0.0030\n",
      "Iter [ 17440/200000] Loss: 0.0025\n",
      "Iter [ 17450/200000] Loss: 0.0026\n",
      "Iter [ 17460/200000] Loss: 0.0025\n",
      "Iter [ 17470/200000] Loss: 0.0027\n",
      "Iter [ 17480/200000] Loss: 0.0023\n",
      "Iter [ 17490/200000] Loss: 0.0021\n",
      "Iter [ 17500/200000] Loss: 0.0024\n",
      "  ‚îî‚îÄ Test Loss: 0.1724\n",
      "Iter [ 17510/200000] Loss: 0.0023\n",
      "Iter [ 17520/200000] Loss: 0.0028\n",
      "Iter [ 17530/200000] Loss: 0.0025\n",
      "Iter [ 17540/200000] Loss: 0.0023\n",
      "Iter [ 17550/200000] Loss: 0.0023\n",
      "Iter [ 17560/200000] Loss: 0.0020\n",
      "Iter [ 17570/200000] Loss: 0.0024\n",
      "Iter [ 17580/200000] Loss: 0.0023\n",
      "Iter [ 17590/200000] Loss: 0.0019\n",
      "Iter [ 17600/200000] Loss: 0.0027\n",
      "  ‚îî‚îÄ Test Loss: 0.1737\n",
      "Iter [ 17610/200000] Loss: 0.0024\n",
      "Iter [ 17620/200000] Loss: 0.0023\n",
      "Iter [ 17630/200000] Loss: 0.0020\n",
      "Iter [ 17640/200000] Loss: 0.0023\n",
      "Iter [ 17650/200000] Loss: 0.0021\n",
      "Iter [ 17660/200000] Loss: 0.0022\n",
      "Iter [ 17670/200000] Loss: 0.0022\n",
      "Iter [ 17680/200000] Loss: 0.0025\n",
      "Iter [ 17690/200000] Loss: 0.0022\n",
      "Iter [ 17700/200000] Loss: 0.0021\n",
      "  ‚îî‚îÄ Test Loss: 0.1707\n",
      "Iter [ 17710/200000] Loss: 0.0025\n",
      "Iter [ 17720/200000] Loss: 0.0021\n",
      "Iter [ 17730/200000] Loss: 0.0025\n",
      "Iter [ 17740/200000] Loss: 0.0020\n",
      "Iter [ 17750/200000] Loss: 0.0019\n",
      "Iter [ 17760/200000] Loss: 0.0018\n",
      "Iter [ 17770/200000] Loss: 0.0018\n",
      "Iter [ 17780/200000] Loss: 0.0020\n",
      "Iter [ 17790/200000] Loss: 0.0021\n",
      "Iter [ 17800/200000] Loss: 0.0018\n",
      "  ‚îî‚îÄ Test Loss: 0.1969\n",
      "Iter [ 17810/200000] Loss: 0.0024\n",
      "Iter [ 17820/200000] Loss: 0.0019\n",
      "Iter [ 17830/200000] Loss: 0.0019\n",
      "Iter [ 17840/200000] Loss: 0.0018\n",
      "Iter [ 17850/200000] Loss: 0.0017\n",
      "Iter [ 17860/200000] Loss: 0.0021\n",
      "Iter [ 17870/200000] Loss: 0.0020\n",
      "Iter [ 17880/200000] Loss: 0.0020\n",
      "Iter [ 17890/200000] Loss: 0.0027\n",
      "Iter [ 17900/200000] Loss: 0.0022\n",
      "  ‚îî‚îÄ Test Loss: 0.1891\n",
      "Iter [ 17910/200000] Loss: 0.0020\n",
      "Iter [ 17920/200000] Loss: 0.0022\n",
      "Iter [ 17930/200000] Loss: 0.0019\n",
      "Iter [ 17940/200000] Loss: 0.0024\n",
      "Iter [ 17950/200000] Loss: 0.0020\n",
      "Iter [ 17960/200000] Loss: 0.0021\n",
      "Iter [ 17970/200000] Loss: 0.0017\n",
      "Iter [ 17980/200000] Loss: 0.0017\n",
      "Iter [ 17990/200000] Loss: 0.0017\n",
      "Iter [ 18000/200000] Loss: 0.0020\n",
      "  ‚îî‚îÄ Test Loss: 0.1845\n",
      "Iter [ 18010/200000] Loss: 0.0017\n",
      "Iter [ 18020/200000] Loss: 0.0022\n",
      "Iter [ 18030/200000] Loss: 0.0016\n",
      "Iter [ 18040/200000] Loss: 0.0018\n",
      "Iter [ 18050/200000] Loss: 0.0017\n",
      "Iter [ 18060/200000] Loss: 0.0017\n",
      "Iter [ 18070/200000] Loss: 0.0017\n",
      "Iter [ 18080/200000] Loss: 0.0018\n",
      "Iter [ 18090/200000] Loss: 0.0017\n",
      "Iter [ 18100/200000] Loss: 0.0023\n",
      "  ‚îî‚îÄ Test Loss: 0.1796\n",
      "Iter [ 18110/200000] Loss: 0.0017\n",
      "Iter [ 18120/200000] Loss: 0.0019\n",
      "Iter [ 18130/200000] Loss: 0.0022\n",
      "Iter [ 18140/200000] Loss: 0.0021\n",
      "Iter [ 18150/200000] Loss: 0.0022\n",
      "Iter [ 18160/200000] Loss: 0.0020\n",
      "Iter [ 18170/200000] Loss: 0.0020\n",
      "Iter [ 18180/200000] Loss: 0.0017\n",
      "Iter [ 18190/200000] Loss: 0.0014\n",
      "Iter [ 18200/200000] Loss: 0.0016\n",
      "  ‚îî‚îÄ Test Loss: 0.1831\n",
      "Iter [ 18210/200000] Loss: 0.0016\n",
      "Iter [ 18220/200000] Loss: 0.0016\n",
      "Iter [ 18230/200000] Loss: 0.0020\n",
      "Iter [ 18240/200000] Loss: 0.0017\n",
      "Iter [ 18250/200000] Loss: 0.0016\n",
      "Iter [ 18260/200000] Loss: 0.0017\n",
      "Iter [ 18270/200000] Loss: 0.0015\n",
      "Iter [ 18280/200000] Loss: 0.0021\n",
      "Iter [ 18290/200000] Loss: 0.0019\n",
      "Iter [ 18300/200000] Loss: 0.0017\n",
      "  ‚îî‚îÄ Test Loss: 0.1863\n",
      "Iter [ 18310/200000] Loss: 0.0020\n",
      "Iter [ 18320/200000] Loss: 0.0016\n",
      "Iter [ 18330/200000] Loss: 0.0018\n",
      "Iter [ 18340/200000] Loss: 0.0020\n",
      "Iter [ 18350/200000] Loss: 0.0018\n",
      "Iter [ 18360/200000] Loss: 0.0020\n",
      "Iter [ 18370/200000] Loss: 0.0018\n",
      "Iter [ 18380/200000] Loss: 0.0019\n",
      "Iter [ 18390/200000] Loss: 0.0018\n",
      "Iter [ 18400/200000] Loss: 0.0016\n",
      "  ‚îî‚îÄ Test Loss: 0.2069\n",
      "Iter [ 18410/200000] Loss: 0.0016\n",
      "Iter [ 18420/200000] Loss: 0.0018\n",
      "Iter [ 18430/200000] Loss: 0.0017\n",
      "Iter [ 18440/200000] Loss: 0.0019\n",
      "Iter [ 18450/200000] Loss: 0.0018\n",
      "Iter [ 18460/200000] Loss: 0.0015\n",
      "Iter [ 18470/200000] Loss: 0.0015\n",
      "Iter [ 18480/200000] Loss: 0.0017\n",
      "Iter [ 18490/200000] Loss: 0.0022\n",
      "Iter [ 18500/200000] Loss: 0.0018\n",
      "  ‚îî‚îÄ Test Loss: 0.1891\n",
      "Iter [ 18510/200000] Loss: 0.0019\n",
      "Iter [ 18520/200000] Loss: 0.0019\n",
      "Iter [ 18530/200000] Loss: 0.0019\n",
      "Iter [ 18540/200000] Loss: 0.0016\n",
      "Iter [ 18550/200000] Loss: 0.0018\n",
      "Iter [ 18560/200000] Loss: 0.0015\n",
      "Iter [ 18570/200000] Loss: 0.0021\n",
      "Iter [ 18580/200000] Loss: 0.0019\n",
      "Iter [ 18590/200000] Loss: 0.0022\n",
      "Iter [ 18600/200000] Loss: 0.0019\n",
      "  ‚îî‚îÄ Test Loss: 0.1835\n",
      "Iter [ 18610/200000] Loss: 0.0021\n",
      "Iter [ 18620/200000] Loss: 0.0018\n",
      "Iter [ 18630/200000] Loss: 0.0021\n",
      "Iter [ 18640/200000] Loss: 0.0019\n",
      "Iter [ 18650/200000] Loss: 0.0020\n",
      "Iter [ 18660/200000] Loss: 0.0019\n",
      "Iter [ 18670/200000] Loss: 0.0018\n",
      "Iter [ 18680/200000] Loss: 0.0018\n",
      "Iter [ 18690/200000] Loss: 0.0017\n",
      "Iter [ 18700/200000] Loss: 0.0020\n",
      "  ‚îî‚îÄ Test Loss: 0.1769\n",
      "Iter [ 18710/200000] Loss: 0.0018\n",
      "Iter [ 18720/200000] Loss: 0.0022\n",
      "Iter [ 18730/200000] Loss: 0.0019\n",
      "Iter [ 18740/200000] Loss: 0.0016\n",
      "Iter [ 18750/200000] Loss: 0.0021\n",
      "Iter [ 18760/200000] Loss: 0.0020\n",
      "Iter [ 18770/200000] Loss: 0.0019\n",
      "Iter [ 18780/200000] Loss: 0.0021\n",
      "Iter [ 18790/200000] Loss: 0.0019\n",
      "Iter [ 18800/200000] Loss: 0.0022\n",
      "  ‚îî‚îÄ Test Loss: 0.1913\n",
      "Iter [ 18810/200000] Loss: 0.0020\n",
      "Iter [ 18820/200000] Loss: 0.0016\n",
      "Iter [ 18830/200000] Loss: 0.0019\n",
      "Iter [ 18840/200000] Loss: 0.0019\n",
      "Iter [ 18850/200000] Loss: 0.0018\n",
      "Iter [ 18860/200000] Loss: 0.0021\n",
      "Iter [ 18870/200000] Loss: 0.0020\n",
      "Iter [ 18880/200000] Loss: 0.0018\n",
      "Iter [ 18890/200000] Loss: 0.0019\n",
      "Iter [ 18900/200000] Loss: 0.0018\n",
      "  ‚îî‚îÄ Test Loss: 0.1709\n",
      "Iter [ 18910/200000] Loss: 0.0018\n",
      "Iter [ 18920/200000] Loss: 0.0020\n",
      "Iter [ 18930/200000] Loss: 0.0019\n",
      "Iter [ 18940/200000] Loss: 0.0022\n",
      "Iter [ 18950/200000] Loss: 0.0017\n",
      "Iter [ 18960/200000] Loss: 0.0023\n",
      "Iter [ 18970/200000] Loss: 0.0023\n",
      "Iter [ 18980/200000] Loss: 0.0020\n",
      "Iter [ 18990/200000] Loss: 0.0022\n",
      "Iter [ 19000/200000] Loss: 0.0019\n",
      "  ‚îî‚îÄ Test Loss: 0.1947\n",
      "Iter [ 19010/200000] Loss: 0.0021\n",
      "Iter [ 19020/200000] Loss: 0.0021\n",
      "Iter [ 19030/200000] Loss: 0.0019\n",
      "Iter [ 19040/200000] Loss: 0.0017\n",
      "Iter [ 19050/200000] Loss: 0.0020\n",
      "Iter [ 19060/200000] Loss: 0.0018\n",
      "Iter [ 19070/200000] Loss: 0.0021\n",
      "Iter [ 19080/200000] Loss: 0.0020\n",
      "Iter [ 19090/200000] Loss: 0.0019\n",
      "Iter [ 19100/200000] Loss: 0.0020\n",
      "  ‚îî‚îÄ Test Loss: 0.1843\n",
      "Iter [ 19110/200000] Loss: 0.0017\n",
      "Iter [ 19120/200000] Loss: 0.0016\n",
      "Iter [ 19130/200000] Loss: 0.0021\n",
      "Iter [ 19140/200000] Loss: 0.0021\n",
      "Iter [ 19150/200000] Loss: 0.0022\n",
      "Iter [ 19160/200000] Loss: 0.0017\n",
      "Iter [ 19170/200000] Loss: 0.0018\n",
      "Iter [ 19180/200000] Loss: 0.0023\n",
      "Iter [ 19190/200000] Loss: 0.0022\n",
      "Iter [ 19200/200000] Loss: 0.0025\n",
      "  ‚îî‚îÄ Test Loss: 0.1819\n",
      "Iter [ 19210/200000] Loss: 0.0019\n",
      "Iter [ 19220/200000] Loss: 0.0018\n",
      "Iter [ 19230/200000] Loss: 0.0019\n",
      "Iter [ 19240/200000] Loss: 0.0020\n",
      "Iter [ 19250/200000] Loss: 0.0019\n",
      "Iter [ 19260/200000] Loss: 0.0017\n",
      "Iter [ 19270/200000] Loss: 0.0020\n",
      "Iter [ 19280/200000] Loss: 0.0019\n",
      "Iter [ 19290/200000] Loss: 0.0017\n",
      "Iter [ 19300/200000] Loss: 0.0018\n",
      "  ‚îî‚îÄ Test Loss: 0.1831\n",
      "Iter [ 19310/200000] Loss: 0.0019\n",
      "Iter [ 19320/200000] Loss: 0.0018\n",
      "Iter [ 19330/200000] Loss: 0.0015\n",
      "Iter [ 19340/200000] Loss: 0.0019\n",
      "Iter [ 19350/200000] Loss: 0.0022\n",
      "Iter [ 19360/200000] Loss: 0.0024\n",
      "Iter [ 19370/200000] Loss: 0.0019\n",
      "Iter [ 19380/200000] Loss: 0.0021\n",
      "Iter [ 19390/200000] Loss: 0.0021\n",
      "Iter [ 19400/200000] Loss: 0.0020\n",
      "  ‚îî‚îÄ Test Loss: 0.1904\n",
      "Iter [ 19410/200000] Loss: 0.0029\n",
      "Iter [ 19420/200000] Loss: 0.0027\n",
      "Iter [ 19430/200000] Loss: 0.0018\n",
      "Iter [ 19440/200000] Loss: 0.0020\n",
      "Iter [ 19450/200000] Loss: 0.0018\n",
      "Iter [ 19460/200000] Loss: 0.0021\n",
      "Iter [ 19470/200000] Loss: 0.0017\n",
      "Iter [ 19480/200000] Loss: 0.0020\n",
      "Iter [ 19490/200000] Loss: 0.0017\n",
      "Iter [ 19500/200000] Loss: 0.0017\n",
      "  ‚îî‚îÄ Test Loss: 0.1722\n",
      "Iter [ 19510/200000] Loss: 0.0019\n",
      "Iter [ 19520/200000] Loss: 0.0017\n",
      "Iter [ 19530/200000] Loss: 0.0018\n",
      "Iter [ 19540/200000] Loss: 0.0018\n",
      "Iter [ 19550/200000] Loss: 0.0020\n",
      "Iter [ 19560/200000] Loss: 0.0021\n",
      "Iter [ 19570/200000] Loss: 0.0019\n",
      "Iter [ 19580/200000] Loss: 0.0019\n",
      "Iter [ 19590/200000] Loss: 0.0021\n",
      "Iter [ 19600/200000] Loss: 0.0020\n",
      "  ‚îî‚îÄ Test Loss: 0.1787\n",
      "Iter [ 19610/200000] Loss: 0.0027\n",
      "Iter [ 19620/200000] Loss: 0.0021\n",
      "Iter [ 19630/200000] Loss: 0.0019\n",
      "Iter [ 19640/200000] Loss: 0.0016\n",
      "Iter [ 19650/200000] Loss: 0.0018\n",
      "Iter [ 19660/200000] Loss: 0.0019\n",
      "Iter [ 19670/200000] Loss: 0.0020\n",
      "Iter [ 19680/200000] Loss: 0.0017\n",
      "Iter [ 19690/200000] Loss: 0.0020\n",
      "Iter [ 19700/200000] Loss: 0.0017\n",
      "  ‚îî‚îÄ Test Loss: 0.1940\n",
      "Iter [ 19710/200000] Loss: 0.0017\n",
      "Iter [ 19720/200000] Loss: 0.0016\n",
      "Iter [ 19730/200000] Loss: 0.0017\n",
      "Iter [ 19740/200000] Loss: 0.0018\n",
      "Iter [ 19750/200000] Loss: 0.0018\n",
      "Iter [ 19760/200000] Loss: 0.0018\n",
      "Iter [ 19770/200000] Loss: 0.0020\n",
      "Iter [ 19780/200000] Loss: 0.0017\n",
      "Iter [ 19790/200000] Loss: 0.0017\n",
      "Iter [ 19800/200000] Loss: 0.0019\n",
      "  ‚îî‚îÄ Test Loss: 0.1913\n",
      "Iter [ 19810/200000] Loss: 0.0018\n",
      "Iter [ 19820/200000] Loss: 0.0026\n",
      "Iter [ 19830/200000] Loss: 0.0023\n",
      "Iter [ 19840/200000] Loss: 0.0020\n",
      "Iter [ 19850/200000] Loss: 0.0015\n",
      "Iter [ 19860/200000] Loss: 0.0015\n",
      "Iter [ 19870/200000] Loss: 0.0016\n",
      "Iter [ 19880/200000] Loss: 0.0017\n",
      "Iter [ 19890/200000] Loss: 0.0018\n",
      "Iter [ 19900/200000] Loss: 0.0021\n",
      "  ‚îî‚îÄ Test Loss: 0.1882\n",
      "Iter [ 19910/200000] Loss: 0.0018\n",
      "Iter [ 19920/200000] Loss: 0.0016\n",
      "Iter [ 19930/200000] Loss: 0.0014\n",
      "Iter [ 19940/200000] Loss: 0.0015\n",
      "Iter [ 19950/200000] Loss: 0.0017\n",
      "Iter [ 19960/200000] Loss: 0.0015\n",
      "Iter [ 19970/200000] Loss: 0.0016\n",
      "Iter [ 19980/200000] Loss: 0.0017\n",
      "Iter [ 19990/200000] Loss: 0.0016\n",
      "Iter [ 20000/200000] Loss: 0.0015\n",
      "  ‚îî‚îÄ Test Loss: 0.1852\n",
      "üíæ Checkpoint saved: ./checkpoints\\checkpoint_epoch_20000.pth\n",
      "  ‚îî‚îÄ Checkpoint saved\n",
      "Iter [ 20010/200000] Loss: 0.0017\n",
      "Iter [ 20020/200000] Loss: 0.0019\n",
      "Iter [ 20030/200000] Loss: 0.0025\n",
      "Iter [ 20040/200000] Loss: 0.0022\n",
      "Iter [ 20050/200000] Loss: 0.0021\n",
      "Iter [ 20060/200000] Loss: 0.0022\n",
      "Iter [ 20070/200000] Loss: 0.0016\n",
      "Iter [ 20080/200000] Loss: 0.0016\n",
      "Iter [ 20090/200000] Loss: 0.0015\n",
      "Iter [ 20100/200000] Loss: 0.0016\n",
      "  ‚îî‚îÄ Test Loss: 0.2049\n",
      "Iter [ 20110/200000] Loss: 0.0020\n",
      "Iter [ 20120/200000] Loss: 0.0016\n",
      "Iter [ 20130/200000] Loss: 0.0015\n",
      "Iter [ 20140/200000] Loss: 0.0014\n",
      "Iter [ 20150/200000] Loss: 0.0013\n",
      "Iter [ 20160/200000] Loss: 0.0016\n",
      "Iter [ 20170/200000] Loss: 0.0015\n",
      "Iter [ 20180/200000] Loss: 0.0013\n",
      "Iter [ 20190/200000] Loss: 0.0015\n",
      "Iter [ 20200/200000] Loss: 0.0015\n",
      "  ‚îî‚îÄ Test Loss: 0.1937\n",
      "Iter [ 20210/200000] Loss: 0.0015\n",
      "Iter [ 20220/200000] Loss: 0.0019\n",
      "Iter [ 20230/200000] Loss: 0.0018\n",
      "Iter [ 20240/200000] Loss: 0.0023\n",
      "Iter [ 20250/200000] Loss: 0.0017\n",
      "Iter [ 20260/200000] Loss: 0.0019\n",
      "Iter [ 20270/200000] Loss: 0.0019\n",
      "Iter [ 20280/200000] Loss: 0.0016\n",
      "Iter [ 20290/200000] Loss: 0.0018\n",
      "Iter [ 20300/200000] Loss: 0.0018\n",
      "  ‚îî‚îÄ Test Loss: 0.2042\n",
      "Iter [ 20310/200000] Loss: 0.0015\n",
      "Iter [ 20320/200000] Loss: 0.0018\n",
      "Iter [ 20330/200000] Loss: 0.0018\n",
      "Iter [ 20340/200000] Loss: 0.0014\n",
      "Iter [ 20350/200000] Loss: 0.0014\n",
      "Iter [ 20360/200000] Loss: 0.0014\n",
      "Iter [ 20370/200000] Loss: 0.0016\n",
      "Iter [ 20380/200000] Loss: 0.0016\n",
      "Iter [ 20390/200000] Loss: 0.0018\n",
      "Iter [ 20400/200000] Loss: 0.0019\n",
      "  ‚îî‚îÄ Test Loss: 0.2013\n",
      "Iter [ 20410/200000] Loss: 0.0015\n",
      "Iter [ 20420/200000] Loss: 0.0016\n",
      "Iter [ 20430/200000] Loss: 0.0020\n",
      "Iter [ 20440/200000] Loss: 0.0016\n",
      "Iter [ 20450/200000] Loss: 0.0023\n",
      "Iter [ 20460/200000] Loss: 0.0020\n",
      "Iter [ 20470/200000] Loss: 0.0019\n",
      "Iter [ 20480/200000] Loss: 0.0016\n",
      "Iter [ 20490/200000] Loss: 0.0017\n",
      "Iter [ 20500/200000] Loss: 0.0014\n",
      "  ‚îî‚îÄ Test Loss: 0.2023\n",
      "Iter [ 20510/200000] Loss: 0.0018\n",
      "Iter [ 20520/200000] Loss: 0.0013\n",
      "Iter [ 20530/200000] Loss: 0.0018\n",
      "Iter [ 20540/200000] Loss: 0.0014\n",
      "Iter [ 20550/200000] Loss: 0.0014\n",
      "Iter [ 20560/200000] Loss: 0.0014\n",
      "Iter [ 20570/200000] Loss: 0.0012\n",
      "Iter [ 20580/200000] Loss: 0.0015\n",
      "Iter [ 20590/200000] Loss: 0.0013\n",
      "Iter [ 20600/200000] Loss: 0.0019\n",
      "  ‚îî‚îÄ Test Loss: 0.1930\n",
      "Iter [ 20610/200000] Loss: 0.0018\n",
      "Iter [ 20620/200000] Loss: 0.0018\n",
      "Iter [ 20630/200000] Loss: 0.0016\n",
      "Iter [ 20640/200000] Loss: 0.0021\n",
      "Iter [ 20650/200000] Loss: 0.0016\n",
      "Iter [ 20660/200000] Loss: 0.0020\n",
      "Iter [ 20670/200000] Loss: 0.0017\n",
      "Iter [ 20680/200000] Loss: 0.0018\n",
      "Iter [ 20690/200000] Loss: 0.0015\n",
      "Iter [ 20700/200000] Loss: 0.0015\n",
      "  ‚îî‚îÄ Test Loss: 0.2057\n",
      "Iter [ 20710/200000] Loss: 0.0015\n",
      "Iter [ 20720/200000] Loss: 0.0016\n",
      "Iter [ 20730/200000] Loss: 0.0015\n",
      "Iter [ 20740/200000] Loss: 0.0016\n",
      "Iter [ 20750/200000] Loss: 0.0013\n",
      "Iter [ 20760/200000] Loss: 0.0014\n",
      "Iter [ 20770/200000] Loss: 0.0013\n",
      "Iter [ 20780/200000] Loss: 0.0013\n",
      "Iter [ 20790/200000] Loss: 0.0014\n",
      "Iter [ 20800/200000] Loss: 0.0014\n",
      "  ‚îî‚îÄ Test Loss: 0.1886\n",
      "Iter [ 20810/200000] Loss: 0.0017\n",
      "Iter [ 20820/200000] Loss: 0.0019\n",
      "Iter [ 20830/200000] Loss: 0.0014\n",
      "Iter [ 20840/200000] Loss: 0.0015\n",
      "Iter [ 20850/200000] Loss: 0.0016\n",
      "Iter [ 20860/200000] Loss: 0.0015\n",
      "Iter [ 20870/200000] Loss: 0.0016\n",
      "Iter [ 20880/200000] Loss: 0.0013\n",
      "Iter [ 20890/200000] Loss: 0.0013\n",
      "Iter [ 20900/200000] Loss: 0.0013\n",
      "  ‚îî‚îÄ Test Loss: 0.2067\n",
      "Iter [ 20910/200000] Loss: 0.0014\n",
      "Iter [ 20920/200000] Loss: 0.0012\n",
      "Iter [ 20930/200000] Loss: 0.0014\n",
      "Iter [ 20940/200000] Loss: 0.0014\n",
      "Iter [ 20950/200000] Loss: 0.0018\n",
      "Iter [ 20960/200000] Loss: 0.0015\n",
      "Iter [ 20970/200000] Loss: 0.0013\n",
      "Iter [ 20980/200000] Loss: 0.0012\n",
      "Iter [ 20990/200000] Loss: 0.0014\n",
      "Iter [ 21000/200000] Loss: 0.0012\n",
      "  ‚îî‚îÄ Test Loss: 0.1844\n",
      "Iter [ 21010/200000] Loss: 0.0015\n",
      "Iter [ 21020/200000] Loss: 0.0013\n",
      "Iter [ 21030/200000] Loss: 0.0016\n",
      "Iter [ 21040/200000] Loss: 0.0015\n",
      "Iter [ 21050/200000] Loss: 0.0014\n",
      "Iter [ 21060/200000] Loss: 0.0018\n",
      "Iter [ 21070/200000] Loss: 0.0016\n",
      "Iter [ 21080/200000] Loss: 0.0019\n",
      "Iter [ 21090/200000] Loss: 0.0013\n",
      "Iter [ 21100/200000] Loss: 0.0014\n",
      "  ‚îî‚îÄ Test Loss: 0.1985\n",
      "Iter [ 21110/200000] Loss: 0.0014\n",
      "Iter [ 21120/200000] Loss: 0.0013\n",
      "Iter [ 21130/200000] Loss: 0.0013\n",
      "Iter [ 21140/200000] Loss: 0.0014\n",
      "Iter [ 21150/200000] Loss: 0.0015\n",
      "Iter [ 21160/200000] Loss: 0.0017\n",
      "Iter [ 21170/200000] Loss: 0.0017\n",
      "Iter [ 21180/200000] Loss: 0.0014\n",
      "Iter [ 21190/200000] Loss: 0.0014\n",
      "Iter [ 21200/200000] Loss: 0.0014\n",
      "  ‚îî‚îÄ Test Loss: 0.1803\n",
      "Iter [ 21210/200000] Loss: 0.0013\n",
      "Iter [ 21220/200000] Loss: 0.0015\n",
      "Iter [ 21230/200000] Loss: 0.0020\n",
      "Iter [ 21240/200000] Loss: 0.0018\n",
      "Iter [ 21250/200000] Loss: 0.0014\n",
      "Iter [ 21260/200000] Loss: 0.0016\n",
      "Iter [ 21270/200000] Loss: 0.0017\n",
      "Iter [ 21280/200000] Loss: 0.0016\n",
      "Iter [ 21290/200000] Loss: 0.0017\n",
      "Iter [ 21300/200000] Loss: 0.0016\n",
      "  ‚îî‚îÄ Test Loss: 0.1991\n",
      "Iter [ 21310/200000] Loss: 0.0015\n",
      "Iter [ 21320/200000] Loss: 0.0015\n",
      "Iter [ 21330/200000] Loss: 0.0018\n",
      "Iter [ 21340/200000] Loss: 0.0015\n",
      "Iter [ 21350/200000] Loss: 0.0018\n",
      "Iter [ 21360/200000] Loss: 0.0020\n",
      "Iter [ 21370/200000] Loss: 0.0016\n",
      "Iter [ 21380/200000] Loss: 0.0018\n",
      "Iter [ 21390/200000] Loss: 0.0016\n",
      "Iter [ 21400/200000] Loss: 0.0017\n",
      "  ‚îî‚îÄ Test Loss: 0.1937\n",
      "Iter [ 21410/200000] Loss: 0.0017\n",
      "Iter [ 21420/200000] Loss: 0.0015\n",
      "Iter [ 21430/200000] Loss: 0.0014\n",
      "Iter [ 21440/200000] Loss: 0.0021\n",
      "Iter [ 21450/200000] Loss: 0.0025\n",
      "Iter [ 21460/200000] Loss: 0.0018\n",
      "Iter [ 21470/200000] Loss: 0.0018\n",
      "Iter [ 21480/200000] Loss: 0.0017\n",
      "Iter [ 21490/200000] Loss: 0.0019\n",
      "Iter [ 21500/200000] Loss: 0.0020\n",
      "  ‚îî‚îÄ Test Loss: 0.1966\n",
      "Iter [ 21510/200000] Loss: 0.0018\n",
      "Iter [ 21520/200000] Loss: 0.0014\n",
      "Iter [ 21530/200000] Loss: 0.0016\n",
      "Iter [ 21540/200000] Loss: 0.0016\n",
      "Iter [ 21550/200000] Loss: 0.0015\n",
      "Iter [ 21560/200000] Loss: 0.0013\n",
      "Iter [ 21570/200000] Loss: 0.0018\n",
      "Iter [ 21580/200000] Loss: 0.0016\n",
      "Iter [ 21590/200000] Loss: 0.0017\n",
      "Iter [ 21600/200000] Loss: 0.0019\n",
      "  ‚îî‚îÄ Test Loss: 0.1956\n",
      "Iter [ 21610/200000] Loss: 0.0016\n",
      "Iter [ 21620/200000] Loss: 0.0021\n",
      "Iter [ 21630/200000] Loss: 0.0017\n",
      "Iter [ 21640/200000] Loss: 0.0017\n",
      "Iter [ 21650/200000] Loss: 0.0024\n",
      "Iter [ 21660/200000] Loss: 0.0020\n",
      "Iter [ 21670/200000] Loss: 0.0026\n",
      "Iter [ 21680/200000] Loss: 0.0023\n",
      "Iter [ 21690/200000] Loss: 0.0020\n",
      "Iter [ 21700/200000] Loss: 0.0022\n",
      "  ‚îî‚îÄ Test Loss: 0.1965\n",
      "Iter [ 21710/200000] Loss: 0.0019\n",
      "Iter [ 21720/200000] Loss: 0.0018\n",
      "Iter [ 21730/200000] Loss: 0.0015\n",
      "Iter [ 21740/200000] Loss: 0.0016\n",
      "Iter [ 21750/200000] Loss: 0.0016\n",
      "Iter [ 21760/200000] Loss: 0.0017\n",
      "Iter [ 21770/200000] Loss: 0.0014\n",
      "Iter [ 21780/200000] Loss: 0.0016\n",
      "Iter [ 21790/200000] Loss: 0.0014\n",
      "Iter [ 21800/200000] Loss: 0.0014\n",
      "  ‚îî‚îÄ Test Loss: 0.1846\n",
      "Iter [ 21810/200000] Loss: 0.0017\n",
      "Iter [ 21820/200000] Loss: 0.0018\n",
      "Iter [ 21830/200000] Loss: 0.0023\n",
      "Iter [ 21840/200000] Loss: 0.0021\n",
      "Iter [ 21850/200000] Loss: 0.0017\n",
      "Iter [ 21860/200000] Loss: 0.0022\n",
      "Iter [ 21870/200000] Loss: 0.0016\n",
      "Iter [ 21880/200000] Loss: 0.0018\n",
      "Iter [ 21890/200000] Loss: 0.0019\n",
      "Iter [ 21900/200000] Loss: 0.0017\n",
      "  ‚îî‚îÄ Test Loss: 0.1837\n",
      "Iter [ 21910/200000] Loss: 0.0022\n",
      "Iter [ 21920/200000] Loss: 0.0017\n",
      "Iter [ 21930/200000] Loss: 0.0015\n",
      "Iter [ 21940/200000] Loss: 0.0014\n",
      "Iter [ 21950/200000] Loss: 0.0015\n",
      "Iter [ 21960/200000] Loss: 0.0013\n",
      "Iter [ 21970/200000] Loss: 0.0016\n",
      "Iter [ 21980/200000] Loss: 0.0014\n",
      "Iter [ 21990/200000] Loss: 0.0015\n",
      "Iter [ 22000/200000] Loss: 0.0013\n",
      "  ‚îî‚îÄ Test Loss: 0.1927\n",
      "Iter [ 22010/200000] Loss: 0.0013\n",
      "Iter [ 22020/200000] Loss: 0.0015\n",
      "Iter [ 22030/200000] Loss: 0.0014\n",
      "Iter [ 22040/200000] Loss: 0.0020\n",
      "Iter [ 22050/200000] Loss: 0.0019\n",
      "Iter [ 22060/200000] Loss: 0.0017\n",
      "Iter [ 22070/200000] Loss: 0.0019\n",
      "Iter [ 22080/200000] Loss: 0.0014\n",
      "Iter [ 22090/200000] Loss: 0.0014\n",
      "Iter [ 22100/200000] Loss: 0.0018\n",
      "  ‚îî‚îÄ Test Loss: 0.1877\n",
      "Iter [ 22110/200000] Loss: 0.0015\n",
      "Iter [ 22120/200000] Loss: 0.0019\n",
      "Iter [ 22130/200000] Loss: 0.0017\n",
      "Iter [ 22140/200000] Loss: 0.0016\n",
      "Iter [ 22150/200000] Loss: 0.0012\n",
      "Iter [ 22160/200000] Loss: 0.0012\n",
      "Iter [ 22170/200000] Loss: 0.0010\n",
      "Iter [ 22180/200000] Loss: 0.0015\n",
      "Iter [ 22190/200000] Loss: 0.0012\n",
      "Iter [ 22200/200000] Loss: 0.0014\n",
      "  ‚îî‚îÄ Test Loss: 0.2099\n",
      "Iter [ 22210/200000] Loss: 0.0011\n",
      "Iter [ 22220/200000] Loss: 0.0012\n",
      "Iter [ 22230/200000] Loss: 0.0013\n",
      "Iter [ 22240/200000] Loss: 0.0013\n",
      "Iter [ 22250/200000] Loss: 0.0014\n",
      "Iter [ 22260/200000] Loss: 0.0016\n",
      "Iter [ 22270/200000] Loss: 0.0013\n",
      "Iter [ 22280/200000] Loss: 0.0016\n",
      "Iter [ 22290/200000] Loss: 0.0013\n",
      "Iter [ 22300/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2037\n",
      "Iter [ 22310/200000] Loss: 0.0013\n",
      "Iter [ 22320/200000] Loss: 0.0013\n",
      "Iter [ 22330/200000] Loss: 0.0013\n",
      "Iter [ 22340/200000] Loss: 0.0013\n",
      "Iter [ 22350/200000] Loss: 0.0013\n",
      "Iter [ 22360/200000] Loss: 0.0013\n",
      "Iter [ 22370/200000] Loss: 0.0011\n",
      "Iter [ 22380/200000] Loss: 0.0011\n",
      "Iter [ 22390/200000] Loss: 0.0013\n",
      "Iter [ 22400/200000] Loss: 0.0012\n",
      "  ‚îî‚îÄ Test Loss: 0.2076\n",
      "Iter [ 22410/200000] Loss: 0.0013\n",
      "Iter [ 22420/200000] Loss: 0.0012\n",
      "Iter [ 22430/200000] Loss: 0.0011\n",
      "Iter [ 22440/200000] Loss: 0.0012\n",
      "Iter [ 22450/200000] Loss: 0.0010\n",
      "Iter [ 22460/200000] Loss: 0.0013\n",
      "Iter [ 22470/200000] Loss: 0.0013\n",
      "Iter [ 22480/200000] Loss: 0.0011\n",
      "Iter [ 22490/200000] Loss: 0.0016\n",
      "Iter [ 22500/200000] Loss: 0.0012\n",
      "  ‚îî‚îÄ Test Loss: 0.1931\n",
      "Iter [ 22510/200000] Loss: 0.0012\n",
      "Iter [ 22520/200000] Loss: 0.0015\n",
      "Iter [ 22530/200000] Loss: 0.0012\n",
      "Iter [ 22540/200000] Loss: 0.0016\n",
      "Iter [ 22550/200000] Loss: 0.0012\n",
      "Iter [ 22560/200000] Loss: 0.0011\n",
      "Iter [ 22570/200000] Loss: 0.0011\n",
      "Iter [ 22580/200000] Loss: 0.0011\n",
      "Iter [ 22590/200000] Loss: 0.0011\n",
      "Iter [ 22600/200000] Loss: 0.0016\n",
      "  ‚îî‚îÄ Test Loss: 0.2080\n",
      "Iter [ 22610/200000] Loss: 0.0013\n",
      "Iter [ 22620/200000] Loss: 0.0014\n",
      "Iter [ 22630/200000] Loss: 0.0010\n",
      "Iter [ 22640/200000] Loss: 0.0012\n",
      "Iter [ 22650/200000] Loss: 0.0012\n",
      "Iter [ 22660/200000] Loss: 0.0010\n",
      "Iter [ 22670/200000] Loss: 0.0013\n",
      "Iter [ 22680/200000] Loss: 0.0012\n",
      "Iter [ 22690/200000] Loss: 0.0015\n",
      "Iter [ 22700/200000] Loss: 0.0013\n",
      "  ‚îî‚îÄ Test Loss: 0.2191\n",
      "Iter [ 22710/200000] Loss: 0.0012\n",
      "Iter [ 22720/200000] Loss: 0.0012\n",
      "Iter [ 22730/200000] Loss: 0.0013\n",
      "Iter [ 22740/200000] Loss: 0.0011\n",
      "Iter [ 22750/200000] Loss: 0.0015\n",
      "Iter [ 22760/200000] Loss: 0.0012\n",
      "Iter [ 22770/200000] Loss: 0.0013\n",
      "Iter [ 22780/200000] Loss: 0.0010\n",
      "Iter [ 22790/200000] Loss: 0.0009\n",
      "Iter [ 22800/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2168\n",
      "Iter [ 22810/200000] Loss: 0.0015\n",
      "Iter [ 22820/200000] Loss: 0.0014\n",
      "Iter [ 22830/200000] Loss: 0.0017\n",
      "Iter [ 22840/200000] Loss: 0.0014\n",
      "Iter [ 22850/200000] Loss: 0.0013\n",
      "Iter [ 22860/200000] Loss: 0.0013\n",
      "Iter [ 22870/200000] Loss: 0.0011\n",
      "Iter [ 22880/200000] Loss: 0.0013\n",
      "Iter [ 22890/200000] Loss: 0.0012\n",
      "Iter [ 22900/200000] Loss: 0.0015\n",
      "  ‚îî‚îÄ Test Loss: 0.2054\n",
      "Iter [ 22910/200000] Loss: 0.0013\n",
      "Iter [ 22920/200000] Loss: 0.0011\n",
      "Iter [ 22930/200000] Loss: 0.0012\n",
      "Iter [ 22940/200000] Loss: 0.0014\n",
      "Iter [ 22950/200000] Loss: 0.0013\n",
      "Iter [ 22960/200000] Loss: 0.0016\n",
      "Iter [ 22970/200000] Loss: 0.0013\n",
      "Iter [ 22980/200000] Loss: 0.0011\n",
      "Iter [ 22990/200000] Loss: 0.0012\n",
      "Iter [ 23000/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2001\n",
      "Iter [ 23010/200000] Loss: 0.0010\n",
      "Iter [ 23020/200000] Loss: 0.0015\n",
      "Iter [ 23030/200000] Loss: 0.0013\n",
      "Iter [ 23040/200000] Loss: 0.0015\n",
      "Iter [ 23050/200000] Loss: 0.0013\n",
      "Iter [ 23060/200000] Loss: 0.0017\n",
      "Iter [ 23070/200000] Loss: 0.0012\n",
      "Iter [ 23080/200000] Loss: 0.0013\n",
      "Iter [ 23090/200000] Loss: 0.0014\n",
      "Iter [ 23100/200000] Loss: 0.0014\n",
      "  ‚îî‚îÄ Test Loss: 0.2000\n",
      "Iter [ 23110/200000] Loss: 0.0014\n",
      "Iter [ 23120/200000] Loss: 0.0014\n",
      "Iter [ 23130/200000] Loss: 0.0013\n",
      "Iter [ 23140/200000] Loss: 0.0013\n",
      "Iter [ 23150/200000] Loss: 0.0013\n",
      "Iter [ 23160/200000] Loss: 0.0012\n",
      "Iter [ 23170/200000] Loss: 0.0016\n",
      "Iter [ 23180/200000] Loss: 0.0017\n",
      "Iter [ 23190/200000] Loss: 0.0014\n",
      "Iter [ 23200/200000] Loss: 0.0014\n",
      "  ‚îî‚îÄ Test Loss: 0.2169\n",
      "Iter [ 23210/200000] Loss: 0.0013\n",
      "Iter [ 23220/200000] Loss: 0.0011\n",
      "Iter [ 23230/200000] Loss: 0.0015\n",
      "Iter [ 23240/200000] Loss: 0.0013\n",
      "Iter [ 23250/200000] Loss: 0.0014\n",
      "Iter [ 23260/200000] Loss: 0.0014\n",
      "Iter [ 23270/200000] Loss: 0.0014\n",
      "Iter [ 23280/200000] Loss: 0.0013\n",
      "Iter [ 23290/200000] Loss: 0.0013\n",
      "Iter [ 23300/200000] Loss: 0.0015\n",
      "  ‚îî‚îÄ Test Loss: 0.1995\n",
      "Iter [ 23310/200000] Loss: 0.0015\n",
      "Iter [ 23320/200000] Loss: 0.0015\n",
      "Iter [ 23330/200000] Loss: 0.0013\n",
      "Iter [ 23340/200000] Loss: 0.0014\n",
      "Iter [ 23350/200000] Loss: 0.0013\n",
      "Iter [ 23360/200000] Loss: 0.0015\n",
      "Iter [ 23370/200000] Loss: 0.0015\n",
      "Iter [ 23380/200000] Loss: 0.0016\n",
      "Iter [ 23390/200000] Loss: 0.0014\n",
      "Iter [ 23400/200000] Loss: 0.0014\n",
      "  ‚îî‚îÄ Test Loss: 0.2041\n",
      "Iter [ 23410/200000] Loss: 0.0014\n",
      "Iter [ 23420/200000] Loss: 0.0014\n",
      "Iter [ 23430/200000] Loss: 0.0015\n",
      "Iter [ 23440/200000] Loss: 0.0012\n",
      "Iter [ 23450/200000] Loss: 0.0016\n",
      "Iter [ 23460/200000] Loss: 0.0015\n",
      "Iter [ 23470/200000] Loss: 0.0011\n",
      "Iter [ 23480/200000] Loss: 0.0013\n",
      "Iter [ 23490/200000] Loss: 0.0011\n",
      "Iter [ 23500/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.1935\n",
      "Iter [ 23510/200000] Loss: 0.0011\n",
      "Iter [ 23520/200000] Loss: 0.0014\n",
      "Iter [ 23530/200000] Loss: 0.0015\n",
      "Iter [ 23540/200000] Loss: 0.0013\n",
      "Iter [ 23550/200000] Loss: 0.0014\n",
      "Iter [ 23560/200000] Loss: 0.0013\n",
      "Iter [ 23570/200000] Loss: 0.0015\n",
      "Iter [ 23580/200000] Loss: 0.0012\n",
      "Iter [ 23590/200000] Loss: 0.0018\n",
      "Iter [ 23600/200000] Loss: 0.0015\n",
      "  ‚îî‚îÄ Test Loss: 0.2109\n",
      "Iter [ 23610/200000] Loss: 0.0012\n",
      "Iter [ 23620/200000] Loss: 0.0012\n",
      "Iter [ 23630/200000] Loss: 0.0012\n",
      "Iter [ 23640/200000] Loss: 0.0014\n",
      "Iter [ 23650/200000] Loss: 0.0013\n",
      "Iter [ 23660/200000] Loss: 0.0016\n",
      "Iter [ 23670/200000] Loss: 0.0011\n",
      "Iter [ 23680/200000] Loss: 0.0012\n",
      "Iter [ 23690/200000] Loss: 0.0011\n",
      "Iter [ 23700/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.1960\n",
      "Iter [ 23710/200000] Loss: 0.0014\n",
      "Iter [ 23720/200000] Loss: 0.0012\n",
      "Iter [ 23730/200000] Loss: 0.0016\n",
      "Iter [ 23740/200000] Loss: 0.0016\n",
      "Iter [ 23750/200000] Loss: 0.0011\n",
      "Iter [ 23760/200000] Loss: 0.0013\n",
      "Iter [ 23770/200000] Loss: 0.0015\n",
      "Iter [ 23780/200000] Loss: 0.0014\n",
      "Iter [ 23790/200000] Loss: 0.0014\n",
      "Iter [ 23800/200000] Loss: 0.0013\n",
      "  ‚îî‚îÄ Test Loss: 0.2108\n",
      "Iter [ 23810/200000] Loss: 0.0014\n",
      "Iter [ 23820/200000] Loss: 0.0011\n",
      "Iter [ 23830/200000] Loss: 0.0012\n",
      "Iter [ 23840/200000] Loss: 0.0011\n",
      "Iter [ 23850/200000] Loss: 0.0013\n",
      "Iter [ 23860/200000] Loss: 0.0012\n",
      "Iter [ 23870/200000] Loss: 0.0015\n",
      "Iter [ 23880/200000] Loss: 0.0012\n",
      "Iter [ 23890/200000] Loss: 0.0011\n",
      "Iter [ 23900/200000] Loss: 0.0012\n",
      "  ‚îî‚îÄ Test Loss: 0.2029\n",
      "Iter [ 23910/200000] Loss: 0.0011\n",
      "Iter [ 23920/200000] Loss: 0.0010\n",
      "Iter [ 23930/200000] Loss: 0.0010\n",
      "Iter [ 23940/200000] Loss: 0.0011\n",
      "Iter [ 23950/200000] Loss: 0.0015\n",
      "Iter [ 23960/200000] Loss: 0.0012\n",
      "Iter [ 23970/200000] Loss: 0.0012\n",
      "Iter [ 23980/200000] Loss: 0.0015\n",
      "Iter [ 23990/200000] Loss: 0.0011\n",
      "Iter [ 24000/200000] Loss: 0.0014\n",
      "  ‚îî‚îÄ Test Loss: 0.2115\n",
      "Iter [ 24010/200000] Loss: 0.0013\n",
      "Iter [ 24020/200000] Loss: 0.0012\n",
      "Iter [ 24030/200000] Loss: 0.0012\n",
      "Iter [ 24040/200000] Loss: 0.0014\n",
      "Iter [ 24050/200000] Loss: 0.0010\n",
      "Iter [ 24060/200000] Loss: 0.0012\n",
      "Iter [ 24070/200000] Loss: 0.0013\n",
      "Iter [ 24080/200000] Loss: 0.0016\n",
      "Iter [ 24090/200000] Loss: 0.0012\n",
      "Iter [ 24100/200000] Loss: 0.0010\n",
      "  ‚îî‚îÄ Test Loss: 0.2062\n",
      "Iter [ 24110/200000] Loss: 0.0012\n",
      "Iter [ 24120/200000] Loss: 0.0011\n",
      "Iter [ 24130/200000] Loss: 0.0013\n",
      "Iter [ 24140/200000] Loss: 0.0011\n",
      "Iter [ 24150/200000] Loss: 0.0013\n",
      "Iter [ 24160/200000] Loss: 0.0018\n",
      "Iter [ 24170/200000] Loss: 0.0012\n",
      "Iter [ 24180/200000] Loss: 0.0011\n",
      "Iter [ 24190/200000] Loss: 0.0014\n",
      "Iter [ 24200/200000] Loss: 0.0013\n",
      "  ‚îî‚îÄ Test Loss: 0.2082\n",
      "Iter [ 24210/200000] Loss: 0.0016\n",
      "Iter [ 24220/200000] Loss: 0.0014\n",
      "Iter [ 24230/200000] Loss: 0.0017\n",
      "Iter [ 24240/200000] Loss: 0.0015\n",
      "Iter [ 24250/200000] Loss: 0.0014\n",
      "Iter [ 24260/200000] Loss: 0.0014\n",
      "Iter [ 24270/200000] Loss: 0.0012\n",
      "Iter [ 24280/200000] Loss: 0.0012\n",
      "Iter [ 24290/200000] Loss: 0.0014\n",
      "Iter [ 24300/200000] Loss: 0.0010\n",
      "  ‚îî‚îÄ Test Loss: 0.2092\n",
      "Iter [ 24310/200000] Loss: 0.0011\n",
      "Iter [ 24320/200000] Loss: 0.0013\n",
      "Iter [ 24330/200000] Loss: 0.0015\n",
      "Iter [ 24340/200000] Loss: 0.0014\n",
      "Iter [ 24350/200000] Loss: 0.0013\n",
      "Iter [ 24360/200000] Loss: 0.0014\n",
      "Iter [ 24370/200000] Loss: 0.0017\n",
      "Iter [ 24380/200000] Loss: 0.0012\n",
      "Iter [ 24390/200000] Loss: 0.0011\n",
      "Iter [ 24400/200000] Loss: 0.0012\n",
      "  ‚îî‚îÄ Test Loss: 0.2049\n",
      "Iter [ 24410/200000] Loss: 0.0016\n",
      "Iter [ 24420/200000] Loss: 0.0016\n",
      "Iter [ 24430/200000] Loss: 0.0015\n",
      "Iter [ 24440/200000] Loss: 0.0015\n",
      "Iter [ 24450/200000] Loss: 0.0013\n",
      "Iter [ 24460/200000] Loss: 0.0015\n",
      "Iter [ 24470/200000] Loss: 0.0013\n",
      "Iter [ 24480/200000] Loss: 0.0012\n",
      "Iter [ 24490/200000] Loss: 0.0012\n",
      "Iter [ 24500/200000] Loss: 0.0017\n",
      "  ‚îî‚îÄ Test Loss: 0.2269\n",
      "Iter [ 24510/200000] Loss: 0.0012\n",
      "Iter [ 24520/200000] Loss: 0.0012\n",
      "Iter [ 24530/200000] Loss: 0.0012\n",
      "Iter [ 24540/200000] Loss: 0.0014\n",
      "Iter [ 24550/200000] Loss: 0.0015\n",
      "Iter [ 24560/200000] Loss: 0.0016\n",
      "Iter [ 24570/200000] Loss: 0.0013\n",
      "Iter [ 24580/200000] Loss: 0.0017\n",
      "Iter [ 24590/200000] Loss: 0.0013\n",
      "Iter [ 24600/200000] Loss: 0.0012\n",
      "  ‚îî‚îÄ Test Loss: 0.2244\n",
      "Iter [ 24610/200000] Loss: 0.0014\n",
      "Iter [ 24620/200000] Loss: 0.0013\n",
      "Iter [ 24630/200000] Loss: 0.0017\n",
      "Iter [ 24640/200000] Loss: 0.0012\n",
      "Iter [ 24650/200000] Loss: 0.0014\n",
      "Iter [ 24660/200000] Loss: 0.0014\n",
      "Iter [ 24670/200000] Loss: 0.0014\n",
      "Iter [ 24680/200000] Loss: 0.0014\n",
      "Iter [ 24690/200000] Loss: 0.0014\n",
      "Iter [ 24700/200000] Loss: 0.0016\n",
      "  ‚îî‚îÄ Test Loss: 0.2249\n",
      "Iter [ 24710/200000] Loss: 0.0017\n",
      "Iter [ 24720/200000] Loss: 0.0011\n",
      "Iter [ 24730/200000] Loss: 0.0013\n",
      "Iter [ 24740/200000] Loss: 0.0013\n",
      "Iter [ 24750/200000] Loss: 0.0011\n",
      "Iter [ 24760/200000] Loss: 0.0016\n",
      "Iter [ 24770/200000] Loss: 0.0014\n",
      "Iter [ 24780/200000] Loss: 0.0014\n",
      "Iter [ 24790/200000] Loss: 0.0015\n",
      "Iter [ 24800/200000] Loss: 0.0013\n",
      "  ‚îî‚îÄ Test Loss: 0.2115\n",
      "Iter [ 24810/200000] Loss: 0.0013\n",
      "Iter [ 24820/200000] Loss: 0.0012\n",
      "Iter [ 24830/200000] Loss: 0.0013\n",
      "Iter [ 24840/200000] Loss: 0.0013\n",
      "Iter [ 24850/200000] Loss: 0.0011\n",
      "Iter [ 24860/200000] Loss: 0.0013\n",
      "Iter [ 24870/200000] Loss: 0.0013\n",
      "Iter [ 24880/200000] Loss: 0.0011\n",
      "Iter [ 24890/200000] Loss: 0.0013\n",
      "Iter [ 24900/200000] Loss: 0.0012\n",
      "  ‚îî‚îÄ Test Loss: 0.2233\n",
      "Iter [ 24910/200000] Loss: 0.0014\n",
      "Iter [ 24920/200000] Loss: 0.0014\n",
      "Iter [ 24930/200000] Loss: 0.0011\n",
      "Iter [ 24940/200000] Loss: 0.0012\n",
      "Iter [ 24950/200000] Loss: 0.0012\n",
      "Iter [ 24960/200000] Loss: 0.0011\n",
      "Iter [ 24970/200000] Loss: 0.0013\n",
      "Iter [ 24980/200000] Loss: 0.0012\n",
      "Iter [ 24990/200000] Loss: 0.0013\n",
      "Iter [ 25000/200000] Loss: 0.0016\n",
      "  ‚îî‚îÄ Test Loss: 0.2201\n",
      "üíæ Checkpoint saved: ./checkpoints\\checkpoint_epoch_25000.pth\n",
      "üóëÔ∏è  Removed old checkpoint: ./checkpoints\\checkpoint_epoch_5000.pth\n",
      "  ‚îî‚îÄ Checkpoint saved\n",
      "Iter [ 25010/200000] Loss: 0.0011\n",
      "Iter [ 25020/200000] Loss: 0.0013\n",
      "Iter [ 25030/200000] Loss: 0.0013\n",
      "Iter [ 25040/200000] Loss: 0.0013\n",
      "Iter [ 25050/200000] Loss: 0.0016\n",
      "Iter [ 25060/200000] Loss: 0.0013\n",
      "Iter [ 25070/200000] Loss: 0.0014\n",
      "Iter [ 25080/200000] Loss: 0.0012\n",
      "Iter [ 25090/200000] Loss: 0.0011\n",
      "Iter [ 25100/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2101\n",
      "Iter [ 25110/200000] Loss: 0.0012\n",
      "Iter [ 25120/200000] Loss: 0.0010\n",
      "Iter [ 25130/200000] Loss: 0.0014\n",
      "Iter [ 25140/200000] Loss: 0.0012\n",
      "Iter [ 25150/200000] Loss: 0.0009\n",
      "Iter [ 25160/200000] Loss: 0.0011\n",
      "Iter [ 25170/200000] Loss: 0.0012\n",
      "Iter [ 25180/200000] Loss: 0.0011\n",
      "Iter [ 25190/200000] Loss: 0.0009\n",
      "Iter [ 25200/200000] Loss: 0.0012\n",
      "  ‚îî‚îÄ Test Loss: 0.2038\n",
      "Iter [ 25210/200000] Loss: 0.0012\n",
      "Iter [ 25220/200000] Loss: 0.0009\n",
      "Iter [ 25230/200000] Loss: 0.0011\n",
      "Iter [ 25240/200000] Loss: 0.0015\n",
      "Iter [ 25250/200000] Loss: 0.0013\n",
      "Iter [ 25260/200000] Loss: 0.0016\n",
      "Iter [ 25270/200000] Loss: 0.0012\n",
      "Iter [ 25280/200000] Loss: 0.0012\n",
      "Iter [ 25290/200000] Loss: 0.0011\n",
      "Iter [ 25300/200000] Loss: 0.0010\n",
      "  ‚îî‚îÄ Test Loss: 0.2121\n",
      "Iter [ 25310/200000] Loss: 0.0009\n",
      "Iter [ 25320/200000] Loss: 0.0012\n",
      "Iter [ 25330/200000] Loss: 0.0011\n",
      "Iter [ 25340/200000] Loss: 0.0014\n",
      "Iter [ 25350/200000] Loss: 0.0010\n",
      "Iter [ 25360/200000] Loss: 0.0010\n",
      "Iter [ 25370/200000] Loss: 0.0010\n",
      "Iter [ 25380/200000] Loss: 0.0011\n",
      "Iter [ 25390/200000] Loss: 0.0011\n",
      "Iter [ 25400/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2118\n",
      "Iter [ 25410/200000] Loss: 0.0012\n",
      "Iter [ 25420/200000] Loss: 0.0009\n",
      "Iter [ 25430/200000] Loss: 0.0009\n",
      "Iter [ 25440/200000] Loss: 0.0011\n",
      "Iter [ 25450/200000] Loss: 0.0011\n",
      "Iter [ 25460/200000] Loss: 0.0011\n",
      "Iter [ 25470/200000] Loss: 0.0014\n",
      "Iter [ 25480/200000] Loss: 0.0014\n",
      "Iter [ 25490/200000] Loss: 0.0011\n",
      "Iter [ 25500/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2203\n",
      "Iter [ 25510/200000] Loss: 0.0012\n",
      "Iter [ 25520/200000] Loss: 0.0011\n",
      "Iter [ 25530/200000] Loss: 0.0009\n",
      "Iter [ 25540/200000] Loss: 0.0010\n",
      "Iter [ 25550/200000] Loss: 0.0010\n",
      "Iter [ 25560/200000] Loss: 0.0010\n",
      "Iter [ 25570/200000] Loss: 0.0010\n",
      "Iter [ 25580/200000] Loss: 0.0009\n",
      "Iter [ 25590/200000] Loss: 0.0010\n",
      "Iter [ 25600/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2058\n",
      "Iter [ 25610/200000] Loss: 0.0010\n",
      "Iter [ 25620/200000] Loss: 0.0012\n",
      "Iter [ 25630/200000] Loss: 0.0011\n",
      "Iter [ 25640/200000] Loss: 0.0009\n",
      "Iter [ 25650/200000] Loss: 0.0009\n",
      "Iter [ 25660/200000] Loss: 0.0010\n",
      "Iter [ 25670/200000] Loss: 0.0011\n",
      "Iter [ 25680/200000] Loss: 0.0011\n",
      "Iter [ 25690/200000] Loss: 0.0011\n",
      "Iter [ 25700/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2266\n",
      "Iter [ 25710/200000] Loss: 0.0012\n",
      "Iter [ 25720/200000] Loss: 0.0012\n",
      "Iter [ 25730/200000] Loss: 0.0012\n",
      "Iter [ 25740/200000] Loss: 0.0011\n",
      "Iter [ 25750/200000] Loss: 0.0012\n",
      "Iter [ 25760/200000] Loss: 0.0010\n",
      "Iter [ 25770/200000] Loss: 0.0010\n",
      "Iter [ 25780/200000] Loss: 0.0010\n",
      "Iter [ 25790/200000] Loss: 0.0009\n",
      "Iter [ 25800/200000] Loss: 0.0013\n",
      "  ‚îî‚îÄ Test Loss: 0.1995\n",
      "Iter [ 25810/200000] Loss: 0.0010\n",
      "Iter [ 25820/200000] Loss: 0.0013\n",
      "Iter [ 25830/200000] Loss: 0.0017\n",
      "Iter [ 25840/200000] Loss: 0.0012\n",
      "Iter [ 25850/200000] Loss: 0.0010\n",
      "Iter [ 25860/200000] Loss: 0.0012\n",
      "Iter [ 25870/200000] Loss: 0.0008\n",
      "Iter [ 25880/200000] Loss: 0.0013\n",
      "Iter [ 25890/200000] Loss: 0.0012\n",
      "Iter [ 25900/200000] Loss: 0.0013\n",
      "  ‚îî‚îÄ Test Loss: 0.2159\n",
      "Iter [ 25910/200000] Loss: 0.0010\n",
      "Iter [ 25920/200000] Loss: 0.0011\n",
      "Iter [ 25930/200000] Loss: 0.0012\n",
      "Iter [ 25940/200000] Loss: 0.0010\n",
      "Iter [ 25950/200000] Loss: 0.0009\n",
      "Iter [ 25960/200000] Loss: 0.0011\n",
      "Iter [ 25970/200000] Loss: 0.0010\n",
      "Iter [ 25980/200000] Loss: 0.0011\n",
      "Iter [ 25990/200000] Loss: 0.0009\n",
      "Iter [ 26000/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2098\n",
      "Iter [ 26010/200000] Loss: 0.0010\n",
      "Iter [ 26020/200000] Loss: 0.0011\n",
      "Iter [ 26030/200000] Loss: 0.0010\n",
      "Iter [ 26040/200000] Loss: 0.0015\n",
      "Iter [ 26050/200000] Loss: 0.0010\n",
      "Iter [ 26060/200000] Loss: 0.0011\n",
      "Iter [ 26070/200000] Loss: 0.0011\n",
      "Iter [ 26080/200000] Loss: 0.0009\n",
      "Iter [ 26090/200000] Loss: 0.0014\n",
      "Iter [ 26100/200000] Loss: 0.0010\n",
      "  ‚îî‚îÄ Test Loss: 0.2173\n",
      "Iter [ 26110/200000] Loss: 0.0013\n",
      "Iter [ 26120/200000] Loss: 0.0009\n",
      "Iter [ 26130/200000] Loss: 0.0010\n",
      "Iter [ 26140/200000] Loss: 0.0010\n",
      "Iter [ 26150/200000] Loss: 0.0009\n",
      "Iter [ 26160/200000] Loss: 0.0010\n",
      "Iter [ 26170/200000] Loss: 0.0011\n",
      "Iter [ 26180/200000] Loss: 0.0009\n",
      "Iter [ 26190/200000] Loss: 0.0009\n",
      "Iter [ 26200/200000] Loss: 0.0010\n",
      "  ‚îî‚îÄ Test Loss: 0.2206\n",
      "Iter [ 26210/200000] Loss: 0.0008\n",
      "Iter [ 26220/200000] Loss: 0.0009\n",
      "Iter [ 26230/200000] Loss: 0.0010\n",
      "Iter [ 26240/200000] Loss: 0.0009\n",
      "Iter [ 26250/200000] Loss: 0.0014\n",
      "Iter [ 26260/200000] Loss: 0.0009\n",
      "Iter [ 26270/200000] Loss: 0.0011\n",
      "Iter [ 26280/200000] Loss: 0.0011\n",
      "Iter [ 26290/200000] Loss: 0.0009\n",
      "Iter [ 26300/200000] Loss: 0.0014\n",
      "  ‚îî‚îÄ Test Loss: 0.2178\n",
      "Iter [ 26310/200000] Loss: 0.0012\n",
      "Iter [ 26320/200000] Loss: 0.0010\n",
      "Iter [ 26330/200000] Loss: 0.0009\n",
      "Iter [ 26340/200000] Loss: 0.0009\n",
      "Iter [ 26350/200000] Loss: 0.0008\n",
      "Iter [ 26360/200000] Loss: 0.0009\n",
      "Iter [ 26370/200000] Loss: 0.0009\n",
      "Iter [ 26380/200000] Loss: 0.0011\n",
      "Iter [ 26390/200000] Loss: 0.0009\n",
      "Iter [ 26400/200000] Loss: 0.0008\n",
      "  ‚îî‚îÄ Test Loss: 0.2172\n",
      "Iter [ 26410/200000] Loss: 0.0010\n",
      "Iter [ 26420/200000] Loss: 0.0008\n",
      "Iter [ 26430/200000] Loss: 0.0009\n",
      "Iter [ 26440/200000] Loss: 0.0010\n",
      "Iter [ 26450/200000] Loss: 0.0010\n",
      "Iter [ 26460/200000] Loss: 0.0014\n",
      "Iter [ 26470/200000] Loss: 0.0010\n",
      "Iter [ 26480/200000] Loss: 0.0008\n",
      "Iter [ 26490/200000] Loss: 0.0011\n",
      "Iter [ 26500/200000] Loss: 0.0010\n",
      "  ‚îî‚îÄ Test Loss: 0.2124\n",
      "Iter [ 26510/200000] Loss: 0.0013\n",
      "Iter [ 26520/200000] Loss: 0.0012\n",
      "Iter [ 26530/200000] Loss: 0.0010\n",
      "Iter [ 26540/200000] Loss: 0.0010\n",
      "Iter [ 26550/200000] Loss: 0.0009\n",
      "Iter [ 26560/200000] Loss: 0.0011\n",
      "Iter [ 26570/200000] Loss: 0.0011\n",
      "Iter [ 26580/200000] Loss: 0.0011\n",
      "Iter [ 26590/200000] Loss: 0.0010\n",
      "Iter [ 26600/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2217\n",
      "Iter [ 26610/200000] Loss: 0.0011\n",
      "Iter [ 26620/200000] Loss: 0.0010\n",
      "Iter [ 26630/200000] Loss: 0.0011\n",
      "Iter [ 26640/200000] Loss: 0.0010\n",
      "Iter [ 26650/200000] Loss: 0.0013\n",
      "Iter [ 26660/200000] Loss: 0.0014\n",
      "Iter [ 26670/200000] Loss: 0.0014\n",
      "Iter [ 26680/200000] Loss: 0.0011\n",
      "Iter [ 26690/200000] Loss: 0.0010\n",
      "Iter [ 26700/200000] Loss: 0.0010\n",
      "  ‚îî‚îÄ Test Loss: 0.2074\n",
      "Iter [ 26710/200000] Loss: 0.0012\n",
      "Iter [ 26720/200000] Loss: 0.0012\n",
      "Iter [ 26730/200000] Loss: 0.0013\n",
      "Iter [ 26740/200000] Loss: 0.0011\n",
      "Iter [ 26750/200000] Loss: 0.0011\n",
      "Iter [ 26760/200000] Loss: 0.0010\n",
      "Iter [ 26770/200000] Loss: 0.0010\n",
      "Iter [ 26780/200000] Loss: 0.0011\n",
      "Iter [ 26790/200000] Loss: 0.0011\n",
      "Iter [ 26800/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2245\n",
      "Iter [ 26810/200000] Loss: 0.0009\n",
      "Iter [ 26820/200000] Loss: 0.0009\n",
      "Iter [ 26830/200000] Loss: 0.0011\n",
      "Iter [ 26840/200000] Loss: 0.0009\n",
      "Iter [ 26850/200000] Loss: 0.0010\n",
      "Iter [ 26860/200000] Loss: 0.0013\n",
      "Iter [ 26870/200000] Loss: 0.0014\n",
      "Iter [ 26880/200000] Loss: 0.0014\n",
      "Iter [ 26890/200000] Loss: 0.0013\n",
      "Iter [ 26900/200000] Loss: 0.0013\n",
      "  ‚îî‚îÄ Test Loss: 0.2206\n",
      "Iter [ 26910/200000] Loss: 0.0013\n",
      "Iter [ 26920/200000] Loss: 0.0012\n",
      "Iter [ 26930/200000] Loss: 0.0012\n",
      "Iter [ 26940/200000] Loss: 0.0011\n",
      "Iter [ 26950/200000] Loss: 0.0014\n",
      "Iter [ 26960/200000] Loss: 0.0012\n",
      "Iter [ 26970/200000] Loss: 0.0010\n",
      "Iter [ 26980/200000] Loss: 0.0011\n",
      "Iter [ 26990/200000] Loss: 0.0010\n",
      "Iter [ 27000/200000] Loss: 0.0012\n",
      "  ‚îî‚îÄ Test Loss: 0.2248\n",
      "Iter [ 27010/200000] Loss: 0.0012\n",
      "Iter [ 27020/200000] Loss: 0.0011\n",
      "Iter [ 27030/200000] Loss: 0.0010\n",
      "Iter [ 27040/200000] Loss: 0.0014\n",
      "Iter [ 27050/200000] Loss: 0.0011\n",
      "Iter [ 27060/200000] Loss: 0.0009\n",
      "Iter [ 27070/200000] Loss: 0.0011\n",
      "Iter [ 27080/200000] Loss: 0.0012\n",
      "Iter [ 27090/200000] Loss: 0.0014\n",
      "Iter [ 27100/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2280\n",
      "Iter [ 27110/200000] Loss: 0.0015\n",
      "Iter [ 27120/200000] Loss: 0.0014\n",
      "Iter [ 27130/200000] Loss: 0.0014\n",
      "Iter [ 27140/200000] Loss: 0.0015\n",
      "Iter [ 27150/200000] Loss: 0.0012\n",
      "Iter [ 27160/200000] Loss: 0.0012\n",
      "Iter [ 27170/200000] Loss: 0.0010\n",
      "Iter [ 27180/200000] Loss: 0.0012\n",
      "Iter [ 27190/200000] Loss: 0.0009\n",
      "Iter [ 27200/200000] Loss: 0.0013\n",
      "  ‚îî‚îÄ Test Loss: 0.2250\n",
      "Iter [ 27210/200000] Loss: 0.0010\n",
      "Iter [ 27220/200000] Loss: 0.0013\n",
      "Iter [ 27230/200000] Loss: 0.0010\n",
      "Iter [ 27240/200000] Loss: 0.0011\n",
      "Iter [ 27250/200000] Loss: 0.0012\n",
      "Iter [ 27260/200000] Loss: 0.0013\n",
      "Iter [ 27270/200000] Loss: 0.0013\n",
      "Iter [ 27280/200000] Loss: 0.0012\n",
      "Iter [ 27290/200000] Loss: 0.0015\n",
      "Iter [ 27300/200000] Loss: 0.0012\n",
      "  ‚îî‚îÄ Test Loss: 0.2309\n",
      "Iter [ 27310/200000] Loss: 0.0012\n",
      "Iter [ 27320/200000] Loss: 0.0013\n",
      "Iter [ 27330/200000] Loss: 0.0013\n",
      "Iter [ 27340/200000] Loss: 0.0013\n",
      "Iter [ 27350/200000] Loss: 0.0015\n",
      "Iter [ 27360/200000] Loss: 0.0014\n",
      "Iter [ 27370/200000] Loss: 0.0013\n",
      "Iter [ 27380/200000] Loss: 0.0011\n",
      "Iter [ 27390/200000] Loss: 0.0014\n",
      "Iter [ 27400/200000] Loss: 0.0016\n",
      "  ‚îî‚îÄ Test Loss: 0.2103\n",
      "Iter [ 27410/200000] Loss: 0.0021\n",
      "Iter [ 27420/200000] Loss: 0.0016\n",
      "Iter [ 27430/200000] Loss: 0.0015\n",
      "Iter [ 27440/200000] Loss: 0.0012\n",
      "Iter [ 27450/200000] Loss: 0.0014\n",
      "Iter [ 27460/200000] Loss: 0.0013\n",
      "Iter [ 27470/200000] Loss: 0.0015\n",
      "Iter [ 27480/200000] Loss: 0.0011\n",
      "Iter [ 27490/200000] Loss: 0.0014\n",
      "Iter [ 27500/200000] Loss: 0.0016\n",
      "  ‚îî‚îÄ Test Loss: 0.2264\n",
      "Iter [ 27510/200000] Loss: 0.0013\n",
      "Iter [ 27520/200000] Loss: 0.0010\n",
      "Iter [ 27530/200000] Loss: 0.0011\n",
      "Iter [ 27540/200000] Loss: 0.0014\n",
      "Iter [ 27550/200000] Loss: 0.0014\n",
      "Iter [ 27560/200000] Loss: 0.0016\n",
      "Iter [ 27570/200000] Loss: 0.0014\n",
      "Iter [ 27580/200000] Loss: 0.0013\n",
      "Iter [ 27590/200000] Loss: 0.0014\n",
      "Iter [ 27600/200000] Loss: 0.0014\n",
      "  ‚îî‚îÄ Test Loss: 0.2276\n",
      "Iter [ 27610/200000] Loss: 0.0015\n",
      "Iter [ 27620/200000] Loss: 0.0013\n",
      "Iter [ 27630/200000] Loss: 0.0013\n",
      "Iter [ 27640/200000] Loss: 0.0013\n",
      "Iter [ 27650/200000] Loss: 0.0012\n",
      "Iter [ 27660/200000] Loss: 0.0012\n",
      "Iter [ 27670/200000] Loss: 0.0012\n",
      "Iter [ 27680/200000] Loss: 0.0014\n",
      "Iter [ 27690/200000] Loss: 0.0011\n",
      "Iter [ 27700/200000] Loss: 0.0010\n",
      "  ‚îî‚îÄ Test Loss: 0.2248\n",
      "Iter [ 27710/200000] Loss: 0.0017\n",
      "Iter [ 27720/200000] Loss: 0.0011\n",
      "Iter [ 27730/200000] Loss: 0.0012\n",
      "Iter [ 27740/200000] Loss: 0.0011\n",
      "Iter [ 27750/200000] Loss: 0.0012\n",
      "Iter [ 27760/200000] Loss: 0.0014\n",
      "Iter [ 27770/200000] Loss: 0.0016\n",
      "Iter [ 27780/200000] Loss: 0.0013\n",
      "Iter [ 27790/200000] Loss: 0.0010\n",
      "Iter [ 27800/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2314\n",
      "Iter [ 27810/200000] Loss: 0.0011\n",
      "Iter [ 27820/200000] Loss: 0.0012\n",
      "Iter [ 27830/200000] Loss: 0.0010\n",
      "Iter [ 27840/200000] Loss: 0.0013\n",
      "Iter [ 27850/200000] Loss: 0.0009\n",
      "Iter [ 27860/200000] Loss: 0.0010\n",
      "Iter [ 27870/200000] Loss: 0.0010\n",
      "Iter [ 27880/200000] Loss: 0.0010\n",
      "Iter [ 27890/200000] Loss: 0.0010\n",
      "Iter [ 27900/200000] Loss: 0.0012\n",
      "  ‚îî‚îÄ Test Loss: 0.2033\n",
      "Iter [ 27910/200000] Loss: 0.0012\n",
      "Iter [ 27920/200000] Loss: 0.0014\n",
      "Iter [ 27930/200000] Loss: 0.0012\n",
      "Iter [ 27940/200000] Loss: 0.0011\n",
      "Iter [ 27950/200000] Loss: 0.0014\n",
      "Iter [ 27960/200000] Loss: 0.0010\n",
      "Iter [ 27970/200000] Loss: 0.0016\n",
      "Iter [ 27980/200000] Loss: 0.0013\n",
      "Iter [ 27990/200000] Loss: 0.0014\n",
      "Iter [ 28000/200000] Loss: 0.0010\n",
      "  ‚îî‚îÄ Test Loss: 0.2197\n",
      "Iter [ 28010/200000] Loss: 0.0010\n",
      "Iter [ 28020/200000] Loss: 0.0009\n",
      "Iter [ 28030/200000] Loss: 0.0011\n",
      "Iter [ 28040/200000] Loss: 0.0009\n",
      "Iter [ 28050/200000] Loss: 0.0012\n",
      "Iter [ 28060/200000] Loss: 0.0009\n",
      "Iter [ 28070/200000] Loss: 0.0009\n",
      "Iter [ 28080/200000] Loss: 0.0011\n",
      "Iter [ 28090/200000] Loss: 0.0010\n",
      "Iter [ 28100/200000] Loss: 0.0008\n",
      "  ‚îî‚îÄ Test Loss: 0.2095\n",
      "Iter [ 28110/200000] Loss: 0.0010\n",
      "Iter [ 28120/200000] Loss: 0.0009\n",
      "Iter [ 28130/200000] Loss: 0.0012\n",
      "Iter [ 28140/200000] Loss: 0.0011\n",
      "Iter [ 28150/200000] Loss: 0.0010\n",
      "Iter [ 28160/200000] Loss: 0.0013\n",
      "Iter [ 28170/200000] Loss: 0.0011\n",
      "Iter [ 28180/200000] Loss: 0.0011\n",
      "Iter [ 28190/200000] Loss: 0.0012\n",
      "Iter [ 28200/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2183\n",
      "Iter [ 28210/200000] Loss: 0.0008\n",
      "Iter [ 28220/200000] Loss: 0.0012\n",
      "Iter [ 28230/200000] Loss: 0.0009\n",
      "Iter [ 28240/200000] Loss: 0.0011\n",
      "Iter [ 28250/200000] Loss: 0.0010\n",
      "Iter [ 28260/200000] Loss: 0.0013\n",
      "Iter [ 28270/200000] Loss: 0.0009\n",
      "Iter [ 28280/200000] Loss: 0.0009\n",
      "Iter [ 28290/200000] Loss: 0.0011\n",
      "Iter [ 28300/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2154\n",
      "Iter [ 28310/200000] Loss: 0.0009\n",
      "Iter [ 28320/200000] Loss: 0.0009\n",
      "Iter [ 28330/200000] Loss: 0.0011\n",
      "Iter [ 28340/200000] Loss: 0.0014\n",
      "Iter [ 28350/200000] Loss: 0.0009\n",
      "Iter [ 28360/200000] Loss: 0.0011\n",
      "Iter [ 28370/200000] Loss: 0.0012\n",
      "Iter [ 28380/200000] Loss: 0.0010\n",
      "Iter [ 28390/200000] Loss: 0.0013\n",
      "Iter [ 28400/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2190\n",
      "Iter [ 28410/200000] Loss: 0.0012\n",
      "Iter [ 28420/200000] Loss: 0.0011\n",
      "Iter [ 28430/200000] Loss: 0.0008\n",
      "Iter [ 28440/200000] Loss: 0.0009\n",
      "Iter [ 28450/200000] Loss: 0.0008\n",
      "Iter [ 28460/200000] Loss: 0.0008\n",
      "Iter [ 28470/200000] Loss: 0.0011\n",
      "Iter [ 28480/200000] Loss: 0.0008\n",
      "Iter [ 28490/200000] Loss: 0.0008\n",
      "Iter [ 28500/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2200\n",
      "Iter [ 28510/200000] Loss: 0.0009\n",
      "Iter [ 28520/200000] Loss: 0.0011\n",
      "Iter [ 28530/200000] Loss: 0.0010\n",
      "Iter [ 28540/200000] Loss: 0.0011\n",
      "Iter [ 28550/200000] Loss: 0.0011\n",
      "Iter [ 28560/200000] Loss: 0.0008\n",
      "Iter [ 28570/200000] Loss: 0.0008\n",
      "Iter [ 28580/200000] Loss: 0.0010\n",
      "Iter [ 28590/200000] Loss: 0.0009\n",
      "Iter [ 28600/200000] Loss: 0.0012\n",
      "  ‚îî‚îÄ Test Loss: 0.2176\n",
      "Iter [ 28610/200000] Loss: 0.0012\n",
      "Iter [ 28620/200000] Loss: 0.0014\n",
      "Iter [ 28630/200000] Loss: 0.0012\n",
      "Iter [ 28640/200000] Loss: 0.0009\n",
      "Iter [ 28650/200000] Loss: 0.0011\n",
      "Iter [ 28660/200000] Loss: 0.0009\n",
      "Iter [ 28670/200000] Loss: 0.0011\n",
      "Iter [ 28680/200000] Loss: 0.0011\n",
      "Iter [ 28690/200000] Loss: 0.0008\n",
      "Iter [ 28700/200000] Loss: 0.0007\n",
      "  ‚îî‚îÄ Test Loss: 0.2270\n",
      "Iter [ 28710/200000] Loss: 0.0009\n",
      "Iter [ 28720/200000] Loss: 0.0009\n",
      "Iter [ 28730/200000] Loss: 0.0011\n",
      "Iter [ 28740/200000] Loss: 0.0011\n",
      "Iter [ 28750/200000] Loss: 0.0011\n",
      "Iter [ 28760/200000] Loss: 0.0016\n",
      "Iter [ 28770/200000] Loss: 0.0009\n",
      "Iter [ 28780/200000] Loss: 0.0010\n",
      "Iter [ 28790/200000] Loss: 0.0011\n",
      "Iter [ 28800/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2157\n",
      "Iter [ 28810/200000] Loss: 0.0013\n",
      "Iter [ 28820/200000] Loss: 0.0010\n",
      "Iter [ 28830/200000] Loss: 0.0010\n",
      "Iter [ 28840/200000] Loss: 0.0010\n",
      "Iter [ 28850/200000] Loss: 0.0009\n",
      "Iter [ 28860/200000] Loss: 0.0009\n",
      "Iter [ 28870/200000] Loss: 0.0012\n",
      "Iter [ 28880/200000] Loss: 0.0011\n",
      "Iter [ 28890/200000] Loss: 0.0011\n",
      "Iter [ 28900/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2296\n",
      "Iter [ 28910/200000] Loss: 0.0008\n",
      "Iter [ 28920/200000] Loss: 0.0010\n",
      "Iter [ 28930/200000] Loss: 0.0009\n",
      "Iter [ 28940/200000] Loss: 0.0010\n",
      "Iter [ 28950/200000] Loss: 0.0010\n",
      "Iter [ 28960/200000] Loss: 0.0012\n",
      "Iter [ 28970/200000] Loss: 0.0013\n",
      "Iter [ 28980/200000] Loss: 0.0010\n",
      "Iter [ 28990/200000] Loss: 0.0009\n",
      "Iter [ 29000/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2280\n",
      "Iter [ 29010/200000] Loss: 0.0010\n",
      "Iter [ 29020/200000] Loss: 0.0015\n",
      "Iter [ 29030/200000] Loss: 0.0009\n",
      "Iter [ 29040/200000] Loss: 0.0013\n",
      "Iter [ 29050/200000] Loss: 0.0009\n",
      "Iter [ 29060/200000] Loss: 0.0008\n",
      "Iter [ 29070/200000] Loss: 0.0008\n",
      "Iter [ 29080/200000] Loss: 0.0010\n",
      "Iter [ 29090/200000] Loss: 0.0010\n",
      "Iter [ 29100/200000] Loss: 0.0010\n",
      "  ‚îî‚îÄ Test Loss: 0.2304\n",
      "Iter [ 29110/200000] Loss: 0.0008\n",
      "Iter [ 29120/200000] Loss: 0.0010\n",
      "Iter [ 29130/200000] Loss: 0.0012\n",
      "Iter [ 29140/200000] Loss: 0.0009\n",
      "Iter [ 29150/200000] Loss: 0.0009\n",
      "Iter [ 29160/200000] Loss: 0.0010\n",
      "Iter [ 29170/200000] Loss: 0.0011\n",
      "Iter [ 29180/200000] Loss: 0.0013\n",
      "Iter [ 29190/200000] Loss: 0.0010\n",
      "Iter [ 29200/200000] Loss: 0.0008\n",
      "  ‚îî‚îÄ Test Loss: 0.2440\n",
      "Iter [ 29210/200000] Loss: 0.0011\n",
      "Iter [ 29220/200000] Loss: 0.0012\n",
      "Iter [ 29230/200000] Loss: 0.0012\n",
      "Iter [ 29240/200000] Loss: 0.0009\n",
      "Iter [ 29250/200000] Loss: 0.0012\n",
      "Iter [ 29260/200000] Loss: 0.0008\n",
      "Iter [ 29270/200000] Loss: 0.0009\n",
      "Iter [ 29280/200000] Loss: 0.0009\n",
      "Iter [ 29290/200000] Loss: 0.0009\n",
      "Iter [ 29300/200000] Loss: 0.0008\n",
      "  ‚îî‚îÄ Test Loss: 0.2252\n",
      "Iter [ 29310/200000] Loss: 0.0010\n",
      "Iter [ 29320/200000] Loss: 0.0007\n",
      "Iter [ 29330/200000] Loss: 0.0007\n",
      "Iter [ 29340/200000] Loss: 0.0009\n",
      "Iter [ 29350/200000] Loss: 0.0009\n",
      "Iter [ 29360/200000] Loss: 0.0009\n",
      "Iter [ 29370/200000] Loss: 0.0008\n",
      "Iter [ 29380/200000] Loss: 0.0010\n",
      "Iter [ 29390/200000] Loss: 0.0012\n",
      "Iter [ 29400/200000] Loss: 0.0007\n",
      "  ‚îî‚îÄ Test Loss: 0.2389\n",
      "Iter [ 29410/200000] Loss: 0.0007\n",
      "Iter [ 29420/200000] Loss: 0.0011\n",
      "Iter [ 29430/200000] Loss: 0.0010\n",
      "Iter [ 29440/200000] Loss: 0.0012\n",
      "Iter [ 29450/200000] Loss: 0.0012\n",
      "Iter [ 29460/200000] Loss: 0.0010\n",
      "Iter [ 29470/200000] Loss: 0.0010\n",
      "Iter [ 29480/200000] Loss: 0.0009\n",
      "Iter [ 29490/200000] Loss: 0.0008\n",
      "Iter [ 29500/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2286\n",
      "Iter [ 29510/200000] Loss: 0.0010\n",
      "Iter [ 29520/200000] Loss: 0.0010\n",
      "Iter [ 29530/200000] Loss: 0.0007\n",
      "Iter [ 29540/200000] Loss: 0.0008\n",
      "Iter [ 29550/200000] Loss: 0.0008\n",
      "Iter [ 29560/200000] Loss: 0.0007\n",
      "Iter [ 29570/200000] Loss: 0.0010\n",
      "Iter [ 29580/200000] Loss: 0.0008\n",
      "Iter [ 29590/200000] Loss: 0.0012\n",
      "Iter [ 29600/200000] Loss: 0.0010\n",
      "  ‚îî‚îÄ Test Loss: 0.2367\n",
      "Iter [ 29610/200000] Loss: 0.0008\n",
      "Iter [ 29620/200000] Loss: 0.0008\n",
      "Iter [ 29630/200000] Loss: 0.0011\n",
      "Iter [ 29640/200000] Loss: 0.0011\n",
      "Iter [ 29650/200000] Loss: 0.0016\n",
      "Iter [ 29660/200000] Loss: 0.0010\n",
      "Iter [ 29670/200000] Loss: 0.0008\n",
      "Iter [ 29680/200000] Loss: 0.0008\n",
      "Iter [ 29690/200000] Loss: 0.0008\n",
      "Iter [ 29700/200000] Loss: 0.0008\n",
      "  ‚îî‚îÄ Test Loss: 0.2328\n",
      "Iter [ 29710/200000] Loss: 0.0007\n",
      "Iter [ 29720/200000] Loss: 0.0009\n",
      "Iter [ 29730/200000] Loss: 0.0009\n",
      "Iter [ 29740/200000] Loss: 0.0007\n",
      "Iter [ 29750/200000] Loss: 0.0008\n",
      "Iter [ 29760/200000] Loss: 0.0008\n",
      "Iter [ 29770/200000] Loss: 0.0008\n",
      "Iter [ 29780/200000] Loss: 0.0008\n",
      "Iter [ 29790/200000] Loss: 0.0010\n",
      "Iter [ 29800/200000] Loss: 0.0012\n",
      "  ‚îî‚îÄ Test Loss: 0.2332\n",
      "Iter [ 29810/200000] Loss: 0.0009\n",
      "Iter [ 29820/200000] Loss: 0.0009\n",
      "Iter [ 29830/200000] Loss: 0.0011\n",
      "Iter [ 29840/200000] Loss: 0.0013\n",
      "Iter [ 29850/200000] Loss: 0.0011\n",
      "Iter [ 29860/200000] Loss: 0.0014\n",
      "Iter [ 29870/200000] Loss: 0.0011\n",
      "Iter [ 29880/200000] Loss: 0.0008\n",
      "Iter [ 29890/200000] Loss: 0.0009\n",
      "Iter [ 29900/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2422\n",
      "Iter [ 29910/200000] Loss: 0.0008\n",
      "Iter [ 29920/200000] Loss: 0.0009\n",
      "Iter [ 29930/200000] Loss: 0.0012\n",
      "Iter [ 29940/200000] Loss: 0.0008\n",
      "Iter [ 29950/200000] Loss: 0.0009\n",
      "Iter [ 29960/200000] Loss: 0.0008\n",
      "Iter [ 29970/200000] Loss: 0.0009\n",
      "Iter [ 29980/200000] Loss: 0.0010\n",
      "Iter [ 29990/200000] Loss: 0.0006\n",
      "Iter [ 30000/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2339\n",
      "üíæ Checkpoint saved: ./checkpoints\\checkpoint_epoch_30000.pth\n",
      "üóëÔ∏è  Removed old checkpoint: ./checkpoints\\checkpoint_epoch_10000.pth\n",
      "  ‚îî‚îÄ Checkpoint saved\n",
      "Iter [ 30010/200000] Loss: 0.0015\n",
      "Iter [ 30020/200000] Loss: 0.0011\n",
      "Iter [ 30030/200000] Loss: 0.0009\n",
      "Iter [ 30040/200000] Loss: 0.0013\n",
      "Iter [ 30050/200000] Loss: 0.0009\n",
      "Iter [ 30060/200000] Loss: 0.0013\n",
      "Iter [ 30070/200000] Loss: 0.0011\n",
      "Iter [ 30080/200000] Loss: 0.0009\n",
      "Iter [ 30090/200000] Loss: 0.0008\n",
      "Iter [ 30100/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2384\n",
      "Iter [ 30110/200000] Loss: 0.0008\n",
      "Iter [ 30120/200000] Loss: 0.0008\n",
      "Iter [ 30130/200000] Loss: 0.0007\n",
      "Iter [ 30140/200000] Loss: 0.0010\n",
      "Iter [ 30150/200000] Loss: 0.0009\n",
      "Iter [ 30160/200000] Loss: 0.0008\n",
      "Iter [ 30170/200000] Loss: 0.0008\n",
      "Iter [ 30180/200000] Loss: 0.0008\n",
      "Iter [ 30190/200000] Loss: 0.0008\n",
      "Iter [ 30200/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2297\n",
      "Iter [ 30210/200000] Loss: 0.0009\n",
      "Iter [ 30220/200000] Loss: 0.0016\n",
      "Iter [ 30230/200000] Loss: 0.0009\n",
      "Iter [ 30240/200000] Loss: 0.0011\n",
      "Iter [ 30250/200000] Loss: 0.0012\n",
      "Iter [ 30260/200000] Loss: 0.0009\n",
      "Iter [ 30270/200000] Loss: 0.0011\n",
      "Iter [ 30280/200000] Loss: 0.0011\n",
      "Iter [ 30290/200000] Loss: 0.0010\n",
      "Iter [ 30300/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2436\n",
      "Iter [ 30310/200000] Loss: 0.0009\n",
      "Iter [ 30320/200000] Loss: 0.0009\n",
      "Iter [ 30330/200000] Loss: 0.0010\n",
      "Iter [ 30340/200000] Loss: 0.0009\n",
      "Iter [ 30350/200000] Loss: 0.0011\n",
      "Iter [ 30360/200000] Loss: 0.0008\n",
      "Iter [ 30370/200000] Loss: 0.0008\n",
      "Iter [ 30380/200000] Loss: 0.0009\n",
      "Iter [ 30390/200000] Loss: 0.0008\n",
      "Iter [ 30400/200000] Loss: 0.0008\n",
      "  ‚îî‚îÄ Test Loss: 0.2309\n",
      "Iter [ 30410/200000] Loss: 0.0009\n",
      "Iter [ 30420/200000] Loss: 0.0011\n",
      "Iter [ 30430/200000] Loss: 0.0011\n",
      "Iter [ 30440/200000] Loss: 0.0010\n",
      "Iter [ 30450/200000] Loss: 0.0008\n",
      "Iter [ 30460/200000] Loss: 0.0012\n",
      "Iter [ 30470/200000] Loss: 0.0010\n",
      "Iter [ 30480/200000] Loss: 0.0015\n",
      "Iter [ 30490/200000] Loss: 0.0013\n",
      "Iter [ 30500/200000] Loss: 0.0015\n",
      "  ‚îî‚îÄ Test Loss: 0.2290\n",
      "Iter [ 30510/200000] Loss: 0.0013\n",
      "Iter [ 30520/200000] Loss: 0.0011\n",
      "Iter [ 30530/200000] Loss: 0.0011\n",
      "Iter [ 30540/200000] Loss: 0.0010\n",
      "Iter [ 30550/200000] Loss: 0.0010\n",
      "Iter [ 30560/200000] Loss: 0.0010\n",
      "Iter [ 30570/200000] Loss: 0.0009\n",
      "Iter [ 30580/200000] Loss: 0.0010\n",
      "Iter [ 30590/200000] Loss: 0.0010\n",
      "Iter [ 30600/200000] Loss: 0.0010\n",
      "  ‚îî‚îÄ Test Loss: 0.2284\n",
      "Iter [ 30610/200000] Loss: 0.0010\n",
      "Iter [ 30620/200000] Loss: 0.0009\n",
      "Iter [ 30630/200000] Loss: 0.0011\n",
      "Iter [ 30640/200000] Loss: 0.0011\n",
      "Iter [ 30650/200000] Loss: 0.0009\n",
      "Iter [ 30660/200000] Loss: 0.0010\n",
      "Iter [ 30670/200000] Loss: 0.0015\n",
      "Iter [ 30680/200000] Loss: 0.0010\n",
      "Iter [ 30690/200000] Loss: 0.0016\n",
      "Iter [ 30700/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2395\n",
      "Iter [ 30710/200000] Loss: 0.0010\n",
      "Iter [ 30720/200000] Loss: 0.0009\n",
      "Iter [ 30730/200000] Loss: 0.0009\n",
      "Iter [ 30740/200000] Loss: 0.0009\n",
      "Iter [ 30750/200000] Loss: 0.0010\n",
      "Iter [ 30760/200000] Loss: 0.0008\n",
      "Iter [ 30770/200000] Loss: 0.0010\n",
      "Iter [ 30780/200000] Loss: 0.0010\n",
      "Iter [ 30790/200000] Loss: 0.0008\n",
      "Iter [ 30800/200000] Loss: 0.0007\n",
      "  ‚îî‚îÄ Test Loss: 0.2409\n",
      "Iter [ 30810/200000] Loss: 0.0009\n",
      "Iter [ 30820/200000] Loss: 0.0008\n",
      "Iter [ 30830/200000] Loss: 0.0010\n",
      "Iter [ 30840/200000] Loss: 0.0011\n",
      "Iter [ 30850/200000] Loss: 0.0014\n",
      "Iter [ 30860/200000] Loss: 0.0010\n",
      "Iter [ 30870/200000] Loss: 0.0008\n",
      "Iter [ 30880/200000] Loss: 0.0013\n",
      "Iter [ 30890/200000] Loss: 0.0010\n",
      "Iter [ 30900/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2345\n",
      "Iter [ 30910/200000] Loss: 0.0010\n",
      "Iter [ 30920/200000] Loss: 0.0010\n",
      "Iter [ 30930/200000] Loss: 0.0010\n",
      "Iter [ 30940/200000] Loss: 0.0010\n",
      "Iter [ 30950/200000] Loss: 0.0008\n",
      "Iter [ 30960/200000] Loss: 0.0010\n",
      "Iter [ 30970/200000] Loss: 0.0007\n",
      "Iter [ 30980/200000] Loss: 0.0009\n",
      "Iter [ 30990/200000] Loss: 0.0009\n",
      "Iter [ 31000/200000] Loss: 0.0008\n",
      "  ‚îî‚îÄ Test Loss: 0.2351\n",
      "Iter [ 31010/200000] Loss: 0.0009\n",
      "Iter [ 31020/200000] Loss: 0.0008\n",
      "Iter [ 31030/200000] Loss: 0.0008\n",
      "Iter [ 31040/200000] Loss: 0.0010\n",
      "Iter [ 31050/200000] Loss: 0.0012\n",
      "Iter [ 31060/200000] Loss: 0.0010\n",
      "Iter [ 31070/200000] Loss: 0.0008\n",
      "Iter [ 31080/200000] Loss: 0.0008\n",
      "Iter [ 31090/200000] Loss: 0.0010\n",
      "Iter [ 31100/200000] Loss: 0.0008\n",
      "  ‚îî‚îÄ Test Loss: 0.2263\n",
      "Iter [ 31110/200000] Loss: 0.0012\n",
      "Iter [ 31120/200000] Loss: 0.0007\n",
      "Iter [ 31130/200000] Loss: 0.0009\n",
      "Iter [ 31140/200000] Loss: 0.0009\n",
      "Iter [ 31150/200000] Loss: 0.0008\n",
      "Iter [ 31160/200000] Loss: 0.0009\n",
      "Iter [ 31170/200000] Loss: 0.0009\n",
      "Iter [ 31180/200000] Loss: 0.0010\n",
      "Iter [ 31190/200000] Loss: 0.0010\n",
      "Iter [ 31200/200000] Loss: 0.0007\n",
      "  ‚îî‚îÄ Test Loss: 0.2283\n",
      "Iter [ 31210/200000] Loss: 0.0007\n",
      "Iter [ 31220/200000] Loss: 0.0008\n",
      "Iter [ 31230/200000] Loss: 0.0008\n",
      "Iter [ 31240/200000] Loss: 0.0009\n",
      "Iter [ 31250/200000] Loss: 0.0007\n",
      "Iter [ 31260/200000] Loss: 0.0010\n",
      "Iter [ 31270/200000] Loss: 0.0009\n",
      "Iter [ 31280/200000] Loss: 0.0010\n",
      "Iter [ 31290/200000] Loss: 0.0008\n",
      "Iter [ 31300/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2349\n",
      "Iter [ 31310/200000] Loss: 0.0008\n",
      "Iter [ 31320/200000] Loss: 0.0012\n",
      "Iter [ 31330/200000] Loss: 0.0009\n",
      "Iter [ 31340/200000] Loss: 0.0011\n",
      "Iter [ 31350/200000] Loss: 0.0008\n",
      "Iter [ 31360/200000] Loss: 0.0009\n",
      "Iter [ 31370/200000] Loss: 0.0007\n",
      "Iter [ 31380/200000] Loss: 0.0009\n",
      "Iter [ 31390/200000] Loss: 0.0009\n",
      "Iter [ 31400/200000] Loss: 0.0007\n",
      "  ‚îî‚îÄ Test Loss: 0.2386\n",
      "Iter [ 31410/200000] Loss: 0.0008\n",
      "Iter [ 31420/200000] Loss: 0.0006\n",
      "Iter [ 31430/200000] Loss: 0.0007\n",
      "Iter [ 31440/200000] Loss: 0.0007\n",
      "Iter [ 31450/200000] Loss: 0.0006\n",
      "Iter [ 31460/200000] Loss: 0.0008\n",
      "Iter [ 31470/200000] Loss: 0.0008\n",
      "Iter [ 31480/200000] Loss: 0.0009\n",
      "Iter [ 31490/200000] Loss: 0.0008\n",
      "Iter [ 31500/200000] Loss: 0.0010\n",
      "  ‚îî‚îÄ Test Loss: 0.2425\n",
      "Iter [ 31510/200000] Loss: 0.0011\n",
      "Iter [ 31520/200000] Loss: 0.0012\n",
      "Iter [ 31530/200000] Loss: 0.0015\n",
      "Iter [ 31540/200000] Loss: 0.0010\n",
      "Iter [ 31550/200000] Loss: 0.0009\n",
      "Iter [ 31560/200000] Loss: 0.0009\n",
      "Iter [ 31570/200000] Loss: 0.0008\n",
      "Iter [ 31580/200000] Loss: 0.0007\n",
      "Iter [ 31590/200000] Loss: 0.0009\n",
      "Iter [ 31600/200000] Loss: 0.0010\n",
      "  ‚îî‚îÄ Test Loss: 0.2387\n",
      "Iter [ 31610/200000] Loss: 0.0008\n",
      "Iter [ 31620/200000] Loss: 0.0007\n",
      "Iter [ 31630/200000] Loss: 0.0007\n",
      "Iter [ 31640/200000] Loss: 0.0008\n",
      "Iter [ 31650/200000] Loss: 0.0007\n",
      "Iter [ 31660/200000] Loss: 0.0006\n",
      "Iter [ 31670/200000] Loss: 0.0008\n",
      "Iter [ 31680/200000] Loss: 0.0012\n",
      "Iter [ 31690/200000] Loss: 0.0008\n",
      "Iter [ 31700/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2486\n",
      "Iter [ 31710/200000] Loss: 0.0010\n",
      "Iter [ 31720/200000] Loss: 0.0010\n",
      "Iter [ 31730/200000] Loss: 0.0010\n",
      "Iter [ 31740/200000] Loss: 0.0011\n",
      "Iter [ 31750/200000] Loss: 0.0010\n",
      "Iter [ 31760/200000] Loss: 0.0010\n",
      "Iter [ 31770/200000] Loss: 0.0009\n",
      "Iter [ 31780/200000] Loss: 0.0007\n",
      "Iter [ 31790/200000] Loss: 0.0008\n",
      "Iter [ 31800/200000] Loss: 0.0007\n",
      "  ‚îî‚îÄ Test Loss: 0.2312\n",
      "Iter [ 31810/200000] Loss: 0.0007\n",
      "Iter [ 31820/200000] Loss: 0.0008\n",
      "Iter [ 31830/200000] Loss: 0.0007\n",
      "Iter [ 31840/200000] Loss: 0.0007\n",
      "Iter [ 31850/200000] Loss: 0.0007\n",
      "Iter [ 31860/200000] Loss: 0.0007\n",
      "Iter [ 31870/200000] Loss: 0.0007\n",
      "Iter [ 31880/200000] Loss: 0.0010\n",
      "Iter [ 31890/200000] Loss: 0.0015\n",
      "Iter [ 31900/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2429\n",
      "Iter [ 31910/200000] Loss: 0.0006\n",
      "Iter [ 31920/200000] Loss: 0.0009\n",
      "Iter [ 31930/200000] Loss: 0.0010\n",
      "Iter [ 31940/200000] Loss: 0.0010\n",
      "Iter [ 31950/200000] Loss: 0.0010\n",
      "Iter [ 31960/200000] Loss: 0.0011\n",
      "Iter [ 31970/200000] Loss: 0.0008\n",
      "Iter [ 31980/200000] Loss: 0.0007\n",
      "Iter [ 31990/200000] Loss: 0.0007\n",
      "Iter [ 32000/200000] Loss: 0.0008\n",
      "  ‚îî‚îÄ Test Loss: 0.2319\n",
      "Iter [ 32010/200000] Loss: 0.0008\n",
      "Iter [ 32020/200000] Loss: 0.0012\n",
      "Iter [ 32030/200000] Loss: 0.0009\n",
      "Iter [ 32040/200000] Loss: 0.0009\n",
      "Iter [ 32050/200000] Loss: 0.0007\n",
      "Iter [ 32060/200000] Loss: 0.0008\n",
      "Iter [ 32070/200000] Loss: 0.0010\n",
      "Iter [ 32080/200000] Loss: 0.0008\n",
      "Iter [ 32090/200000] Loss: 0.0009\n",
      "Iter [ 32100/200000] Loss: 0.0013\n",
      "  ‚îî‚îÄ Test Loss: 0.2303\n",
      "Iter [ 32110/200000] Loss: 0.0008\n",
      "Iter [ 32120/200000] Loss: 0.0008\n",
      "Iter [ 32130/200000] Loss: 0.0011\n",
      "Iter [ 32140/200000] Loss: 0.0007\n",
      "Iter [ 32150/200000] Loss: 0.0012\n",
      "Iter [ 32160/200000] Loss: 0.0010\n",
      "Iter [ 32170/200000] Loss: 0.0010\n",
      "Iter [ 32180/200000] Loss: 0.0009\n",
      "Iter [ 32190/200000] Loss: 0.0010\n",
      "Iter [ 32200/200000] Loss: 0.0007\n",
      "  ‚îî‚îÄ Test Loss: 0.2351\n",
      "Iter [ 32210/200000] Loss: 0.0010\n",
      "Iter [ 32220/200000] Loss: 0.0009\n",
      "Iter [ 32230/200000] Loss: 0.0009\n",
      "Iter [ 32240/200000] Loss: 0.0007\n",
      "Iter [ 32250/200000] Loss: 0.0009\n",
      "Iter [ 32260/200000] Loss: 0.0008\n",
      "Iter [ 32270/200000] Loss: 0.0008\n",
      "Iter [ 32280/200000] Loss: 0.0009\n",
      "Iter [ 32290/200000] Loss: 0.0010\n",
      "Iter [ 32300/200000] Loss: 0.0008\n",
      "  ‚îî‚îÄ Test Loss: 0.2387\n",
      "Iter [ 32310/200000] Loss: 0.0012\n",
      "Iter [ 32320/200000] Loss: 0.0009\n",
      "Iter [ 32330/200000] Loss: 0.0008\n",
      "Iter [ 32340/200000] Loss: 0.0011\n",
      "Iter [ 32350/200000] Loss: 0.0008\n",
      "Iter [ 32360/200000] Loss: 0.0011\n",
      "Iter [ 32370/200000] Loss: 0.0010\n",
      "Iter [ 32380/200000] Loss: 0.0010\n",
      "Iter [ 32390/200000] Loss: 0.0010\n",
      "Iter [ 32400/200000] Loss: 0.0008\n",
      "  ‚îî‚îÄ Test Loss: 0.2484\n",
      "Iter [ 32410/200000] Loss: 0.0009\n",
      "Iter [ 32420/200000] Loss: 0.0009\n",
      "Iter [ 32430/200000] Loss: 0.0010\n",
      "Iter [ 32440/200000] Loss: 0.0009\n",
      "Iter [ 32450/200000] Loss: 0.0008\n",
      "Iter [ 32460/200000] Loss: 0.0008\n",
      "Iter [ 32470/200000] Loss: 0.0008\n",
      "Iter [ 32480/200000] Loss: 0.0008\n",
      "Iter [ 32490/200000] Loss: 0.0008\n",
      "Iter [ 32500/200000] Loss: 0.0007\n",
      "  ‚îî‚îÄ Test Loss: 0.2383\n",
      "Iter [ 32510/200000] Loss: 0.0009\n",
      "Iter [ 32520/200000] Loss: 0.0013\n",
      "Iter [ 32530/200000] Loss: 0.0009\n",
      "Iter [ 32540/200000] Loss: 0.0009\n",
      "Iter [ 32550/200000] Loss: 0.0012\n",
      "Iter [ 32560/200000] Loss: 0.0008\n",
      "Iter [ 32570/200000] Loss: 0.0014\n",
      "Iter [ 32580/200000] Loss: 0.0010\n",
      "Iter [ 32590/200000] Loss: 0.0011\n",
      "Iter [ 32600/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2417\n",
      "Iter [ 32610/200000] Loss: 0.0009\n",
      "Iter [ 32620/200000] Loss: 0.0008\n",
      "Iter [ 32630/200000] Loss: 0.0009\n",
      "Iter [ 32640/200000] Loss: 0.0009\n",
      "Iter [ 32650/200000] Loss: 0.0010\n",
      "Iter [ 32660/200000] Loss: 0.0008\n",
      "Iter [ 32670/200000] Loss: 0.0007\n",
      "Iter [ 32680/200000] Loss: 0.0009\n",
      "Iter [ 32690/200000] Loss: 0.0009\n",
      "Iter [ 32700/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2286\n",
      "Iter [ 32710/200000] Loss: 0.0009\n",
      "Iter [ 32720/200000] Loss: 0.0011\n",
      "Iter [ 32730/200000] Loss: 0.0012\n",
      "Iter [ 32740/200000] Loss: 0.0009\n",
      "Iter [ 32750/200000] Loss: 0.0008\n",
      "Iter [ 32760/200000] Loss: 0.0011\n",
      "Iter [ 32770/200000] Loss: 0.0011\n",
      "Iter [ 32780/200000] Loss: 0.0011\n",
      "Iter [ 32790/200000] Loss: 0.0008\n",
      "Iter [ 32800/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2368\n",
      "Iter [ 32810/200000] Loss: 0.0007\n",
      "Iter [ 32820/200000] Loss: 0.0009\n",
      "Iter [ 32830/200000] Loss: 0.0009\n",
      "Iter [ 32840/200000] Loss: 0.0009\n",
      "Iter [ 32850/200000] Loss: 0.0009\n",
      "Iter [ 32860/200000] Loss: 0.0009\n",
      "Iter [ 32870/200000] Loss: 0.0008\n",
      "Iter [ 32880/200000] Loss: 0.0008\n",
      "Iter [ 32890/200000] Loss: 0.0008\n",
      "Iter [ 32900/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2366\n",
      "Iter [ 32910/200000] Loss: 0.0011\n",
      "Iter [ 32920/200000] Loss: 0.0009\n",
      "Iter [ 32930/200000] Loss: 0.0010\n",
      "Iter [ 32940/200000] Loss: 0.0013\n",
      "Iter [ 32950/200000] Loss: 0.0008\n",
      "Iter [ 32960/200000] Loss: 0.0008\n",
      "Iter [ 32970/200000] Loss: 0.0009\n",
      "Iter [ 32980/200000] Loss: 0.0010\n",
      "Iter [ 32990/200000] Loss: 0.0010\n",
      "Iter [ 33000/200000] Loss: 0.0010\n",
      "  ‚îî‚îÄ Test Loss: 0.2369\n",
      "Iter [ 33010/200000] Loss: 0.0011\n",
      "Iter [ 33020/200000] Loss: 0.0007\n",
      "Iter [ 33030/200000] Loss: 0.0008\n",
      "Iter [ 33040/200000] Loss: 0.0007\n",
      "Iter [ 33050/200000] Loss: 0.0009\n",
      "Iter [ 33060/200000] Loss: 0.0008\n",
      "Iter [ 33070/200000] Loss: 0.0010\n",
      "Iter [ 33080/200000] Loss: 0.0007\n",
      "Iter [ 33090/200000] Loss: 0.0008\n",
      "Iter [ 33100/200000] Loss: 0.0008\n",
      "  ‚îî‚îÄ Test Loss: 0.2250\n",
      "Iter [ 33110/200000] Loss: 0.0008\n",
      "Iter [ 33120/200000] Loss: 0.0008\n",
      "Iter [ 33130/200000] Loss: 0.0008\n",
      "Iter [ 33140/200000] Loss: 0.0009\n",
      "Iter [ 33150/200000] Loss: 0.0009\n",
      "Iter [ 33160/200000] Loss: 0.0009\n",
      "Iter [ 33170/200000] Loss: 0.0010\n",
      "Iter [ 33180/200000] Loss: 0.0009\n",
      "Iter [ 33190/200000] Loss: 0.0007\n",
      "Iter [ 33200/200000] Loss: 0.0012\n",
      "  ‚îî‚îÄ Test Loss: 0.2414\n",
      "Iter [ 33210/200000] Loss: 0.0008\n",
      "Iter [ 33220/200000] Loss: 0.0008\n",
      "Iter [ 33230/200000] Loss: 0.0008\n",
      "Iter [ 33240/200000] Loss: 0.0008\n",
      "Iter [ 33250/200000] Loss: 0.0007\n",
      "Iter [ 33260/200000] Loss: 0.0008\n",
      "Iter [ 33270/200000] Loss: 0.0008\n",
      "Iter [ 33280/200000] Loss: 0.0008\n",
      "Iter [ 33290/200000] Loss: 0.0005\n",
      "Iter [ 33300/200000] Loss: 0.0007\n",
      "  ‚îî‚îÄ Test Loss: 0.2234\n",
      "Iter [ 33310/200000] Loss: 0.0007\n",
      "Iter [ 33320/200000] Loss: 0.0007\n",
      "Iter [ 33330/200000] Loss: 0.0007\n",
      "Iter [ 33340/200000] Loss: 0.0008\n",
      "Iter [ 33350/200000] Loss: 0.0008\n",
      "Iter [ 33360/200000] Loss: 0.0009\n",
      "Iter [ 33370/200000] Loss: 0.0008\n",
      "Iter [ 33380/200000] Loss: 0.0007\n",
      "Iter [ 33390/200000] Loss: 0.0008\n",
      "Iter [ 33400/200000] Loss: 0.0008\n",
      "  ‚îî‚îÄ Test Loss: 0.2329\n",
      "Iter [ 33410/200000] Loss: 0.0009\n",
      "Iter [ 33420/200000] Loss: 0.0007\n",
      "Iter [ 33430/200000] Loss: 0.0008\n",
      "Iter [ 33440/200000] Loss: 0.0008\n",
      "Iter [ 33450/200000] Loss: 0.0007\n",
      "Iter [ 33460/200000] Loss: 0.0006\n",
      "Iter [ 33470/200000] Loss: 0.0009\n",
      "Iter [ 33480/200000] Loss: 0.0008\n",
      "Iter [ 33490/200000] Loss: 0.0007\n",
      "Iter [ 33500/200000] Loss: 0.0007\n",
      "  ‚îî‚îÄ Test Loss: 0.2270\n",
      "Iter [ 33510/200000] Loss: 0.0006\n",
      "Iter [ 33520/200000] Loss: 0.0006\n",
      "Iter [ 33530/200000] Loss: 0.0007\n",
      "Iter [ 33540/200000] Loss: 0.0006\n",
      "Iter [ 33550/200000] Loss: 0.0007\n",
      "Iter [ 33560/200000] Loss: 0.0007\n",
      "Iter [ 33570/200000] Loss: 0.0008\n",
      "Iter [ 33580/200000] Loss: 0.0008\n",
      "Iter [ 33590/200000] Loss: 0.0008\n",
      "Iter [ 33600/200000] Loss: 0.0007\n",
      "  ‚îî‚îÄ Test Loss: 0.2285\n",
      "Iter [ 33610/200000] Loss: 0.0010\n",
      "Iter [ 33620/200000] Loss: 0.0011\n",
      "Iter [ 33630/200000] Loss: 0.0009\n",
      "Iter [ 33640/200000] Loss: 0.0007\n",
      "Iter [ 33650/200000] Loss: 0.0008\n",
      "Iter [ 33660/200000] Loss: 0.0006\n",
      "Iter [ 33670/200000] Loss: 0.0005\n",
      "Iter [ 33680/200000] Loss: 0.0007\n",
      "Iter [ 33690/200000] Loss: 0.0008\n",
      "Iter [ 33700/200000] Loss: 0.0008\n",
      "  ‚îî‚îÄ Test Loss: 0.2480\n",
      "Iter [ 33710/200000] Loss: 0.0008\n",
      "Iter [ 33720/200000] Loss: 0.0008\n",
      "Iter [ 33730/200000] Loss: 0.0007\n",
      "Iter [ 33740/200000] Loss: 0.0007\n",
      "Iter [ 33750/200000] Loss: 0.0007\n",
      "Iter [ 33760/200000] Loss: 0.0008\n",
      "Iter [ 33770/200000] Loss: 0.0013\n",
      "Iter [ 33780/200000] Loss: 0.0007\n",
      "Iter [ 33790/200000] Loss: 0.0008\n",
      "Iter [ 33800/200000] Loss: 0.0007\n",
      "  ‚îî‚îÄ Test Loss: 0.2331\n",
      "Iter [ 33810/200000] Loss: 0.0008\n",
      "Iter [ 33820/200000] Loss: 0.0009\n",
      "Iter [ 33830/200000] Loss: 0.0011\n",
      "Iter [ 33840/200000] Loss: 0.0011\n",
      "Iter [ 33850/200000] Loss: 0.0008\n",
      "Iter [ 33860/200000] Loss: 0.0009\n",
      "Iter [ 33870/200000] Loss: 0.0008\n",
      "Iter [ 33880/200000] Loss: 0.0007\n",
      "Iter [ 33890/200000] Loss: 0.0007\n",
      "Iter [ 33900/200000] Loss: 0.0008\n",
      "  ‚îî‚îÄ Test Loss: 0.2530\n",
      "Iter [ 33910/200000] Loss: 0.0009\n",
      "Iter [ 33920/200000] Loss: 0.0007\n",
      "Iter [ 33930/200000] Loss: 0.0007\n",
      "Iter [ 33940/200000] Loss: 0.0007\n",
      "Iter [ 33950/200000] Loss: 0.0008\n",
      "Iter [ 33960/200000] Loss: 0.0007\n",
      "Iter [ 33970/200000] Loss: 0.0007\n",
      "Iter [ 33980/200000] Loss: 0.0010\n",
      "Iter [ 33990/200000] Loss: 0.0009\n",
      "Iter [ 34000/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2435\n",
      "Iter [ 34010/200000] Loss: 0.0007\n",
      "Iter [ 34020/200000] Loss: 0.0008\n",
      "Iter [ 34030/200000] Loss: 0.0009\n",
      "Iter [ 34040/200000] Loss: 0.0010\n",
      "Iter [ 34050/200000] Loss: 0.0010\n",
      "Iter [ 34060/200000] Loss: 0.0008\n",
      "Iter [ 34070/200000] Loss: 0.0010\n",
      "Iter [ 34080/200000] Loss: 0.0009\n",
      "Iter [ 34090/200000] Loss: 0.0009\n",
      "Iter [ 34100/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2413\n",
      "Iter [ 34110/200000] Loss: 0.0010\n",
      "Iter [ 34120/200000] Loss: 0.0009\n",
      "Iter [ 34130/200000] Loss: 0.0007\n",
      "Iter [ 34140/200000] Loss: 0.0009\n",
      "Iter [ 34150/200000] Loss: 0.0008\n",
      "Iter [ 34160/200000] Loss: 0.0009\n",
      "Iter [ 34170/200000] Loss: 0.0008\n",
      "Iter [ 34180/200000] Loss: 0.0009\n",
      "Iter [ 34190/200000] Loss: 0.0011\n",
      "Iter [ 34200/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2448\n",
      "Iter [ 34210/200000] Loss: 0.0008\n",
      "Iter [ 34220/200000] Loss: 0.0010\n",
      "Iter [ 34230/200000] Loss: 0.0010\n",
      "Iter [ 34240/200000] Loss: 0.0012\n",
      "Iter [ 34250/200000] Loss: 0.0009\n",
      "Iter [ 34260/200000] Loss: 0.0010\n",
      "Iter [ 34270/200000] Loss: 0.0008\n",
      "Iter [ 34280/200000] Loss: 0.0009\n",
      "Iter [ 34290/200000] Loss: 0.0006\n",
      "Iter [ 34300/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2295\n",
      "Iter [ 34310/200000] Loss: 0.0009\n",
      "Iter [ 34320/200000] Loss: 0.0011\n",
      "Iter [ 34330/200000] Loss: 0.0007\n",
      "Iter [ 34340/200000] Loss: 0.0008\n",
      "Iter [ 34350/200000] Loss: 0.0008\n",
      "Iter [ 34360/200000] Loss: 0.0009\n",
      "Iter [ 34370/200000] Loss: 0.0011\n",
      "Iter [ 34380/200000] Loss: 0.0009\n",
      "Iter [ 34390/200000] Loss: 0.0012\n",
      "Iter [ 34400/200000] Loss: 0.0022\n",
      "  ‚îî‚îÄ Test Loss: 0.2256\n",
      "Iter [ 34410/200000] Loss: 0.0010\n",
      "Iter [ 34420/200000] Loss: 0.0010\n",
      "Iter [ 34430/200000] Loss: 0.0012\n",
      "Iter [ 34440/200000] Loss: 0.0011\n",
      "Iter [ 34450/200000] Loss: 0.0015\n",
      "Iter [ 34460/200000] Loss: 0.0012\n",
      "Iter [ 34470/200000] Loss: 0.0011\n",
      "Iter [ 34480/200000] Loss: 0.0009\n",
      "Iter [ 34490/200000] Loss: 0.0009\n",
      "Iter [ 34500/200000] Loss: 0.0011\n",
      "  ‚îî‚îÄ Test Loss: 0.2400\n",
      "Iter [ 34510/200000] Loss: 0.0010\n",
      "Iter [ 34520/200000] Loss: 0.0010\n",
      "Iter [ 34530/200000] Loss: 0.0011\n",
      "Iter [ 34540/200000] Loss: 0.0009\n",
      "Iter [ 34550/200000] Loss: 0.0009\n",
      "Iter [ 34560/200000] Loss: 0.0010\n",
      "Iter [ 34570/200000] Loss: 0.0008\n",
      "Iter [ 34580/200000] Loss: 0.0009\n",
      "Iter [ 34590/200000] Loss: 0.0010\n",
      "Iter [ 34600/200000] Loss: 0.0012\n",
      "  ‚îî‚îÄ Test Loss: 0.2382\n",
      "Iter [ 34610/200000] Loss: 0.0012\n",
      "Iter [ 34620/200000] Loss: 0.0010\n",
      "Iter [ 34630/200000] Loss: 0.0009\n",
      "Iter [ 34640/200000] Loss: 0.0011\n",
      "Iter [ 34650/200000] Loss: 0.0009\n",
      "Iter [ 34660/200000] Loss: 0.0012\n",
      "Iter [ 34670/200000] Loss: 0.0011\n",
      "Iter [ 34680/200000] Loss: 0.0013\n",
      "Iter [ 34690/200000] Loss: 0.0008\n",
      "Iter [ 34700/200000] Loss: 0.0009\n",
      "  ‚îî‚îÄ Test Loss: 0.2494\n",
      "Iter [ 34710/200000] Loss: 0.0010\n",
      "Iter [ 34720/200000] Loss: 0.0009\n",
      "Iter [ 34730/200000] Loss: 0.0009\n",
      "Iter [ 34740/200000] Loss: 0.0010\n",
      "Iter [ 34750/200000] Loss: 0.0007\n",
      "Iter [ 34760/200000] Loss: 0.0008\n",
      "Iter [ 34770/200000] Loss: 0.0008\n",
      "Iter [ 34780/200000] Loss: 0.0008\n",
      "Iter [ 34790/200000] Loss: 0.0008\n",
      "Iter [ 34800/200000] Loss: 0.0010\n",
      "  ‚îî‚îÄ Test Loss: 0.2176\n",
      "Iter [ 34810/200000] Loss: 0.0008\n",
      "Iter [ 34820/200000] Loss: 0.0011\n",
      "Iter [ 34830/200000] Loss: 0.0009\n",
      "Iter [ 34840/200000] Loss: 0.0008\n",
      "Iter [ 34850/200000] Loss: 0.0010\n",
      "Iter [ 34860/200000] Loss: 0.0008\n",
      "Iter [ 34870/200000] Loss: 0.0010\n",
      "Iter [ 34880/200000] Loss: 0.0009\n",
      "Iter [ 34890/200000] Loss: 0.0009\n",
      "Iter [ 34900/200000] Loss: 0.0007\n",
      "  ‚îî‚îÄ Test Loss: 0.2357\n",
      "Iter [ 34910/200000] Loss: 0.0006\n",
      "Iter [ 34920/200000] Loss: 0.0007\n",
      "Iter [ 34930/200000] Loss: 0.0007\n",
      "Iter [ 34940/200000] Loss: 0.0008\n",
      "Iter [ 34950/200000] Loss: 0.0008\n",
      "Iter [ 34960/200000] Loss: 0.0007\n",
      "Iter [ 34970/200000] Loss: 0.0007\n",
      "Iter [ 34980/200000] Loss: 0.0006\n",
      "Iter [ 34990/200000] Loss: 0.0006\n",
      "Iter [ 35000/200000] Loss: 0.0007\n",
      "  ‚îî‚îÄ Test Loss: 0.2309\n",
      "üíæ Checkpoint saved: ./checkpoints\\checkpoint_epoch_35000.pth\n",
      "üóëÔ∏è  Removed old checkpoint: ./checkpoints\\checkpoint_epoch_15000.pth\n",
      "  ‚îî‚îÄ Checkpoint saved\n",
      "Iter [ 35010/200000] Loss: 0.0006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 56\u001B[39m\n\u001B[32m     54\u001B[39m optimizer.zero_grad()\n\u001B[32m     55\u001B[39m loss.backward()\n\u001B[32m---> \u001B[39m\u001B[32m56\u001B[39m \u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     58\u001B[39m \u001B[38;5;66;03m# Print progress\u001B[39;00m\n\u001B[32m     59\u001B[39m iteration += \u001B[32m1\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Napredna-Obdelava-Slik\\NOS-N1\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:517\u001B[39m, in \u001B[36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    512\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    513\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    514\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    515\u001B[39m             )\n\u001B[32m--> \u001B[39m\u001B[32m517\u001B[39m out = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    518\u001B[39m \u001B[38;5;28mself\u001B[39m._optimizer_step_code()\n\u001B[32m    520\u001B[39m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Napredna-Obdelava-Slik\\NOS-N1\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:82\u001B[39m, in \u001B[36m_use_grad_for_differentiable.<locals>._use_grad\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     80\u001B[39m     torch.set_grad_enabled(\u001B[38;5;28mself\u001B[39m.defaults[\u001B[33m\"\u001B[39m\u001B[33mdifferentiable\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m     81\u001B[39m     torch._dynamo.graph_break()\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m     ret = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     83\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     84\u001B[39m     torch._dynamo.graph_break()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Napredna-Obdelava-Slik\\NOS-N1\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:247\u001B[39m, in \u001B[36mAdam.step\u001B[39m\u001B[34m(self, closure)\u001B[39m\n\u001B[32m    235\u001B[39m     beta1, beta2 = group[\u001B[33m\"\u001B[39m\u001B[33mbetas\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    237\u001B[39m     has_complex = \u001B[38;5;28mself\u001B[39m._init_group(\n\u001B[32m    238\u001B[39m         group,\n\u001B[32m    239\u001B[39m         params_with_grad,\n\u001B[32m   (...)\u001B[39m\u001B[32m    244\u001B[39m         state_steps,\n\u001B[32m    245\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m247\u001B[39m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    248\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    249\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    250\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mamsgrad\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    256\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    257\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    258\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    259\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mweight_decay\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    260\u001B[39m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43meps\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    261\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmaximize\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    262\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mforeach\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    263\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcapturable\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    264\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdifferentiable\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    265\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfused\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    266\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgrad_scale\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    267\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfound_inf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    268\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdecoupled_weight_decay\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    269\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    271\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Napredna-Obdelava-Slik\\NOS-N1\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:150\u001B[39m, in \u001B[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    148\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(*args, **kwargs)\n\u001B[32m    149\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m150\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Napredna-Obdelava-Slik\\NOS-N1\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:953\u001B[39m, in \u001B[36madam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[39m\n\u001B[32m    950\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    951\u001B[39m     func = _single_tensor_adam\n\u001B[32m--> \u001B[39m\u001B[32m953\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    954\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    955\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    956\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    957\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    958\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    959\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    960\u001B[39m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    961\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    962\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    963\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    964\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    965\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    966\u001B[39m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[43m=\u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    967\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    968\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    969\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    970\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    971\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    972\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    973\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Napredna-Obdelava-Slik\\NOS-N1\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:759\u001B[39m, in \u001B[36m_multi_tensor_adam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001B[39m\n\u001B[32m    756\u001B[39m     torch._foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001B[32m    757\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    758\u001B[39m     bias_correction1 = [\n\u001B[32m--> \u001B[39m\u001B[32m759\u001B[39m         \u001B[32m1\u001B[39m - beta1 ** \u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m device_state_steps\n\u001B[32m    760\u001B[39m     ]\n\u001B[32m    761\u001B[39m     bias_correction2 = [\n\u001B[32m    762\u001B[39m         \u001B[32m1\u001B[39m - beta2 ** _get_value(step) \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m device_state_steps\n\u001B[32m    763\u001B[39m     ]\n\u001B[32m    765\u001B[39m     step_size = _stack_if_compiling([(lr / bc) * -\u001B[32m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m bc \u001B[38;5;129;01min\u001B[39;00m bias_correction1])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Napredna-Obdelava-Slik\\NOS-N1\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:97\u001B[39m, in \u001B[36m_get_value\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m     95\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n\u001B[32m     96\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m97\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, torch.Tensor) \u001B[38;5;28;01melse\u001B[39;00m x\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# PLOT TRAINING CURVES\n",
    "# ============================================================\n",
    "\n",
    "checkpoint = load_checkpoint_generic(checkpoint_dir, device)\n",
    "if checkpoint:\n",
    "    train_losses = checkpoint.get('train_losses', [])\n",
    "    test_losses = checkpoint.get('test_losses', [])\n",
    "    print_every = checkpoint['config']['print_every']\n",
    "    eval_every = checkpoint['config']['eval_every']\n",
    "    iteration = checkpoint['iteration']\n",
    "    print(f\"‚úì Loaded training history from checkpoint (iteration {checkpoint.get('iteration', 0):,})\")\n",
    "else:\n",
    "    ValueError(\"No checkpoint found for plotting training curves.\")\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "if len(train_losses) == 0 and len(test_losses) == 0:\n",
    "    print(\"‚ö†Ô∏è  No training data to plot. Run the training loop first.\")\n",
    "else:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "    # Plot training losses\n",
    "    if len(train_losses) > 0:\n",
    "        iterations_range = [(i + 1) for i in range(len(train_losses))]\n",
    "        ax.plot(iterations_range, train_losses, 'b-', label='Training Loss', linewidth=2, alpha=0.7)\n",
    "        print(f\"‚úì Training losses plotted ({len(train_losses)} points)\")\n",
    "\n",
    "    # Plot test losses\n",
    "    if len(test_losses) > 0:\n",
    "        test_iterations_range = [(i + 1) * eval_every for i in range(len(test_losses))]\n",
    "        ax.plot(test_iterations_range, test_losses, 'r-', label='Test Loss', linewidth=2, alpha=0.7)\n",
    "        print(f\"‚úì Test losses plotted ({len(test_losses)} points)\")\n",
    "\n",
    "    ax.set_xlabel('Iteration', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12)\n",
    "    ax.set_title('Training Progress - Interest Point Detection', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add minor gridlines for better readability\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(which='minor', alpha=0.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print()\n",
    "    if len(train_losses) > 0:\n",
    "        print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "        print(f\"Best Training Loss: {min(train_losses):.4f}\")\n",
    "    if len(test_losses) > 0:\n",
    "        print(f\"Final Test Loss: {test_losses[-1]:.4f}\")\n",
    "        print(f\"Best Test Loss: {min(test_losses):.4f}\")\n",
    "    print()\n",
    "    print(\"=\" * 60)\n"
   ],
   "id": "65bb98e5b93c7f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T19:26:10.679611Z",
     "start_time": "2025-12-07T19:26:10.493464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# MODEL TESTING - Load Checkpoint and Evaluate\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL TESTING\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Test configuration\n",
    "test_checkpoint_dir = './checkpoints'  # Path to checkpoint directory\n",
    "num_test_visualizations = 20  # Number of samples to visualize\n",
    "\n",
    "# Initialize model\n",
    "print(\"Initializing model...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = KeypointNet().to(device)\n",
    "\n",
    "# Load latest checkpoint\n",
    "checkpoint = load_checkpoint_generic(test_checkpoint_dir, device)\n",
    "if checkpoint:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    iteration = checkpoint.get('iteration', 'unknown')\n",
    "    print(f\"‚úì Loaded checkpoint from iteration {iteration:,}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No checkpoint found! Using untrained model.\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load test dataset if not already loaded\n",
    "# if 'test_dataset' not in locals() or test_dataset is None:\n",
    "#     test_samples_path = os.path.join(dataset_cache_dir, f'test_samples_{num_test_samples}.npz')\n",
    "if True:\n",
    "    test_dataset = KeypointDataset(\n",
    "        num_samples=num_test_samples,\n",
    "        image_shape=image_size,\n",
    "\n",
    "        # use_homography_augment=False,\n",
    "        # use_photometric_augment=False,\n",
    "        # use_geometric_augment=False,\n",
    "        use_homography_augment=True,\n",
    "        use_photometric_augment=True,\n",
    "        use_geometric_augment=True,\n",
    "\n",
    "        pregenerate=False,\n",
    "        # load_from_file=test_samples_path\n",
    "    )\n",
    "    print(f\"‚úì Test dataset loaded: {len(test_dataset)} samples\")\n",
    "\n",
    "# Evaluate on full test set\n",
    "print(\"Evaluating on test set...\")\n",
    "total_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits = model(images, return_logits=True)\n",
    "        targets_idx = targets.argmax(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(logits, targets_idx)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_test_loss = total_loss / len(test_loader)\n",
    "print(f\"‚úì Test Loss: {avg_test_loss:.4f}\")\n",
    "print()\n",
    "\n",
    "# Visualize predictions\n",
    "print(f\"Visualizing {num_test_visualizations} predictions...\")\n",
    "test_indices = np.random.choice(len(test_dataset), num_test_visualizations, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(num_test_visualizations, 3, figsize=(15, 5 * num_test_visualizations))\n",
    "if num_test_visualizations == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for plot_idx, dataset_idx in enumerate(test_indices):\n",
    "        image, target = test_dataset[dataset_idx]\n",
    "\n",
    "        # Get prediction\n",
    "        image_batch = image.unsqueeze(0).to(device)\n",
    "        logits = model(image_batch, return_logits=True)\n",
    "        heatmap = model(image_batch, return_logits=False)\n",
    "\n",
    "        # Extract keypoints from target and prediction\n",
    "        from Models import extract_keypoints_from_target, process_output_torch\n",
    "\n",
    "        target_kpts = extract_keypoints_from_target(target)\n",
    "        _, pred_kpts_list = process_output_torch(logits, threshold=0.015)\n",
    "        pred_kpts = pred_kpts_list[0]\n",
    "\n",
    "        # Convert tensors to numpy for visualization\n",
    "        img_np = image.squeeze().cpu().numpy()\n",
    "        heatmap_np = heatmap.squeeze().cpu().numpy()\n",
    "\n",
    "        # Plot input image with ground truth\n",
    "        axes[plot_idx, 0].imshow(img_np, cmap='gray')\n",
    "        if len(target_kpts) > 0:\n",
    "            axes[plot_idx, 0].scatter(target_kpts[:, 0], target_kpts[:, 1],\n",
    "                                     c='lime', s=50, marker='x', linewidths=2)\n",
    "        axes[plot_idx, 0].set_title(f'Input + Ground Truth ({len(target_kpts)} points)')\n",
    "        axes[plot_idx, 0].axis('off')\n",
    "\n",
    "        # Plot heatmap\n",
    "        axes[plot_idx, 1].imshow(img_np, cmap='gray')\n",
    "        axes[plot_idx, 1].imshow(heatmap_np, cmap='jet', alpha=0.5)\n",
    "        axes[plot_idx, 1].set_title('Prediction Heatmap')\n",
    "        axes[plot_idx, 1].axis('off')\n",
    "\n",
    "        # Plot detected keypoints\n",
    "        axes[plot_idx, 2].imshow(img_np, cmap='gray')\n",
    "        if len(pred_kpts) > 0:\n",
    "            axes[plot_idx, 2].scatter(pred_kpts[:, 0], pred_kpts[:, 1],\n",
    "                                     c='red', s=50, marker='+', linewidths=2)\n",
    "        axes[plot_idx, 2].set_title(f'Detected Keypoints ({len(pred_kpts)} points)')\n",
    "        axes[plot_idx, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úì Testing complete!\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "403f75d0b1638e8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL TESTING\n",
      "============================================================\n",
      "\n",
      "Initializing model...\n",
      "‚úÖ Loaded checkpoint: ./checkpoints\\checkpoint_epoch_35000.pth (epoch 35000)\n",
      "‚úì Loaded checkpoint from iteration 35,000\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Either 'load_from_file' or 'generate_fn' must be provided",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 37\u001B[39m\n\u001B[32m     33\u001B[39m \u001B[38;5;66;03m# Load test dataset if not already loaded\u001B[39;00m\n\u001B[32m     34\u001B[39m \u001B[38;5;66;03m# if 'test_dataset' not in locals() or test_dataset is None:\u001B[39;00m\n\u001B[32m     35\u001B[39m \u001B[38;5;66;03m#     test_samples_path = os.path.join(dataset_cache_dir, f'test_samples_{num_test_samples}.npz')\u001B[39;00m\n\u001B[32m     36\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m     test_dataset = \u001B[43mKeypointDataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     38\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_test_samples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     39\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimage_shape\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimage_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     40\u001B[39m \n\u001B[32m     41\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# use_homography_augment=False,\u001B[39;49;00m\n\u001B[32m     42\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# use_photometric_augment=False,\u001B[39;49;00m\n\u001B[32m     43\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# use_geometric_augment=False,\u001B[39;49;00m\n\u001B[32m     44\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_homography_augment\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     45\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_photometric_augment\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     46\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_geometric_augment\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     47\u001B[39m \n\u001B[32m     48\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpregenerate\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     49\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# load_from_file=test_samples_path\u001B[39;49;00m\n\u001B[32m     50\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     51\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m‚úì Test dataset loaded: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(test_dataset)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m samples\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     53\u001B[39m \u001B[38;5;66;03m# Evaluate on full test set\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Racunalniski-Vid\\RV-N2\\Models.py:437\u001B[39m, in \u001B[36mKeypointDataset.__init__\u001B[39m\u001B[34m(self, num_samples, image_shape, generate_fn, generate_kwargs, use_homography_augment, use_photometric_augment, use_geometric_augment, pregenerate, load_from_file)\u001B[39m\n\u001B[32m    435\u001B[39m \u001B[38;5;66;03m# Validate that either load_from_file or generate_fn is provided\u001B[39;00m\n\u001B[32m    436\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m load_from_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m generate_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m437\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mEither \u001B[39m\u001B[33m'\u001B[39m\u001B[33mload_from_file\u001B[39m\u001B[33m'\u001B[39m\u001B[33m or \u001B[39m\u001B[33m'\u001B[39m\u001B[33mgenerate_fn\u001B[39m\u001B[33m'\u001B[39m\u001B[33m must be provided\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    439\u001B[39m \u001B[38;5;66;03m# Remove use_homography from generate_kwargs if using augmentation\u001B[39;00m\n\u001B[32m    440\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.use_homography_augment \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33muse_homography\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.generate_kwargs:\n",
      "\u001B[31mValueError\u001B[39m: Either 'load_from_file' or 'generate_fn' must be provided"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
