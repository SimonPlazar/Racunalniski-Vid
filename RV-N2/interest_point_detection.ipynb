{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T14:53:55.742228Z",
     "start_time": "2025-12-07T14:53:46.990211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "from Models import KeypointNet, KeypointDataset\n",
    "from Generator import generate_synthetic_image\n",
    "from Helper import save_checkpoint_generic, load_checkpoint_generic\n"
   ],
   "id": "cfa9f4796db9bbc2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T14:54:18.495652Z",
     "start_time": "2025-12-07T14:54:18.396002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# HYPERPARAMETERS\n",
    "# ============================================================\n",
    "\n",
    "# Model parameters (SuperPoint: Adam, lr=0.001, beta=(0.9, 0.999))\n",
    "learning_rate = 0.001\n",
    "adam_betas = (0.9, 0.999)\n",
    "weight_decay = 0.0\n",
    "\n",
    "# Training parameters (iteration-based)\n",
    "num_iterations = 200_000  # SuperPoint uses 200k iterations\n",
    "batch_size = 32  # SuperPoint uses 32\n",
    "\n",
    "# Image parameters\n",
    "image_size = (240, 320)  # (Height, Width)\n",
    "\n",
    "# Dataset parameters\n",
    "num_train_samples = 5000  # Number of pregenerated training samples\n",
    "num_test_samples = 500   # Number of pregenerated test samples\n",
    "\n",
    "# Augmentation settings (applied during training, not during generation)\n",
    "use_homography_augment = True    # Apply random homography to training data\n",
    "use_photometric_augment = True   # Apply brightness/contrast to training data\n",
    "use_geometric_augment = True     # Apply flips to training data\n",
    "\n",
    "# Dataset file paths (.npz format - contains pregenerated images)\n",
    "dataset_cache_dir = './dataset_cache'\n",
    "load_datasets_if_exist = True    # Load from .npz files if available\n",
    "\n",
    "# Checkpoint parameters\n",
    "checkpoint_dir = './checkpoints'\n",
    "save_checkpoint_every = 5000  # Save every N iterations\n",
    "max_checkpoints = 4\n",
    "\n",
    "# Logging parameters\n",
    "print_every = 20   # Print loss every N iterations\n",
    "eval_every = 100   # Evaluate on test set every N iterations\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(dataset_cache_dir, exist_ok=True)\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"  Training samples: {num_train_samples}\")\n",
    "print(f\"  Test samples: {num_test_samples}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Target iterations: {num_iterations:,}\")\n",
    "print(f\"  Training augmentation: {'ENABLED' if use_homography_augment else 'DISABLED'}\")\n",
    "print()\n"
   ],
   "id": "ed395d636eee7e2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration loaded\n",
      "  Device: CUDA\n",
      "  Training samples: 5000\n",
      "  Test samples: 500\n",
      "  Batch size: 32\n",
      "  Target iterations: 200,000\n",
      "  Training augmentation: ENABLED\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T14:50:23.484846Z",
     "start_time": "2025-12-07T14:46:30.050603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# DATASET GENERATION AND SAVING (Run once to create datasets)\n",
    "# ============================================================\n",
    "\n",
    "train_samples_path = os.path.join(dataset_cache_dir, f'train_samples_{num_train_samples}.npz')\n",
    "test_samples_path = os.path.join(dataset_cache_dir, f'test_samples_{num_test_samples}.npz')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Generate and save training samples (raw, no augmentation)\n",
    "print(f\"Generating {num_train_samples} training samples...\")\n",
    "train_generator = KeypointDataset(\n",
    "    num_samples=num_train_samples,\n",
    "    image_shape=image_size,\n",
    "    generate_fn=generate_synthetic_image,\n",
    "    generate_kwargs={\n",
    "        'width': image_size[1],\n",
    "        'height': image_size[0],\n",
    "        'shape_type': 'random',\n",
    "    },\n",
    "    use_homography_augment=False,  # No augmentation during generation\n",
    "    use_photometric_augment=False,\n",
    "    use_geometric_augment=False,\n",
    "    pregenerate=True\n",
    ")\n",
    "print(f\"‚úì Training samples generated: {len(train_generator)} samples\")\n",
    "\n",
    "# Save training samples\n",
    "print(f\"Saving to {train_samples_path}...\")\n",
    "train_generator.save_to_file(train_samples_path)\n",
    "print(f\"‚úì Training samples saved!\")\n",
    "print()\n",
    "\n",
    "# Generate and save test samples (raw, no augmentation)\n",
    "print(f\"Generating {num_test_samples} test samples...\")\n",
    "test_generator = KeypointDataset(\n",
    "    num_samples=num_test_samples,\n",
    "    image_shape=image_size,\n",
    "    generate_fn=generate_synthetic_image,\n",
    "    generate_kwargs={\n",
    "        'width': image_size[1],\n",
    "        'height': image_size[0],\n",
    "        'shape_type': 'random',\n",
    "    },\n",
    "    use_homography_augment=False,  # No augmentation during generation\n",
    "    use_photometric_augment=False,\n",
    "    use_geometric_augment=False,\n",
    "    pregenerate=True\n",
    ")\n",
    "print(f\"‚úì Test samples generated: {len(test_generator)} samples\")\n",
    "\n",
    "# Save test samples\n",
    "print(f\"Saving to {test_samples_path}...\")\n",
    "test_generator.save_to_file(test_samples_path)\n",
    "print(f\"‚úì Test samples saved!\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úì Dataset generation complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training samples: {train_samples_path}\")\n",
    "print(f\"Test samples: {test_samples_path}\")\n",
    "print()\n"
   ],
   "id": "db4c52bcd31f3516",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET GENERATION\n",
      "============================================================\n",
      "\n",
      "Generating 5000 training samples...\n",
      "Pre-generating 5000 base samples...\n",
      "  500/5000 samples\n",
      "  1000/5000 samples\n",
      "  1500/5000 samples\n",
      "  2000/5000 samples\n",
      "  2500/5000 samples\n",
      "  3000/5000 samples\n",
      "  3500/5000 samples\n",
      "  4000/5000 samples\n",
      "  4500/5000 samples\n",
      "  5000/5000 samples\n",
      "‚úì Pre-generation complete!\n",
      "‚úì Training samples generated: 1000000000 samples\n",
      "Saving to ./dataset_cache\\train_samples_5000.npz...\n",
      "‚úì Saved 5000 samples to ./dataset_cache\\train_samples_5000.npz\n",
      "‚úì Training samples saved!\n",
      "\n",
      "Generating 500 test samples...\n",
      "Pre-generating 500 base samples...\n",
      "  500/500 samples\n",
      "‚úì Pre-generation complete!\n",
      "‚úì Test samples generated: 1000000000 samples\n",
      "Saving to ./dataset_cache\\test_samples_500.npz...\n",
      "‚úì Saved 500 samples to ./dataset_cache\\test_samples_500.npz\n",
      "‚úì Test samples saved!\n",
      "\n",
      "============================================================\n",
      "‚úì Dataset generation complete!\n",
      "============================================================\n",
      "Training samples: ./dataset_cache\\train_samples_5000.npz\n",
      "Test samples: ./dataset_cache\\test_samples_500.npz\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T14:55:23.089118Z",
     "start_time": "2025-12-07T14:55:07.628525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# LOAD DATASETS AND INIT MODEL\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING SETUP\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATASETS FROM .NPZ FILES\n",
    "# ============================================================\n",
    "\n",
    "train_samples_path = os.path.join(dataset_cache_dir, f'train_samples_{num_train_samples}.npz')\n",
    "test_samples_path = os.path.join(dataset_cache_dir, f'test_samples_{num_test_samples}.npz')\n",
    "\n",
    "train_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "# Load training dataset WITH augmentation\n",
    "if load_datasets_if_exist and os.path.exists(train_samples_path):\n",
    "    print(f\"Loading training samples from {train_samples_path}...\")\n",
    "    print(f\"  Augmentation: {'ENABLED' if use_homography_augment else 'DISABLED'}\")\n",
    "    try:\n",
    "        train_dataset = KeypointDataset(\n",
    "            num_samples=num_train_samples,\n",
    "            image_shape=image_size,\n",
    "            use_homography_augment=use_homography_augment,\n",
    "            use_photometric_augment=use_photometric_augment,\n",
    "            use_geometric_augment=use_geometric_augment,\n",
    "            pregenerate=False,  # Don't regenerate, just load\n",
    "            load_from_file=train_samples_path\n",
    "        )\n",
    "        print(f\"‚úì Training dataset loaded: {len(train_dataset)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Failed to load training dataset: {e}\")\n",
    "        print(\"Please run the dataset generation cell first!\")\n",
    "        train_dataset = None\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Training samples not found at {train_samples_path}\")\n",
    "    print(\"Please run the dataset generation cell first!\")\n",
    "\n",
    "# Load test dataset WITHOUT augmentation\n",
    "if load_datasets_if_exist and os.path.exists(test_samples_path):\n",
    "    print(f\"Loading test samples from {test_samples_path}...\")\n",
    "    print(f\"  Augmentation: DISABLED (test set)\")\n",
    "    try:\n",
    "        test_dataset = KeypointDataset(\n",
    "            num_samples=num_test_samples,\n",
    "            image_shape=image_size,\n",
    "            use_homography_augment=False,  # No augmentation for test\n",
    "            use_photometric_augment=False,\n",
    "            use_geometric_augment=False,\n",
    "            pregenerate=False,  # Don't regenerate, just load\n",
    "            load_from_file=test_samples_path\n",
    "        )\n",
    "        print(f\"‚úì Test dataset loaded: {len(test_dataset)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Failed to load test dataset: {e}\")\n",
    "        print(\"Please run the dataset generation cell first!\")\n",
    "        test_dataset = None\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Test samples not found at {test_samples_path}\")\n",
    "    print(\"Please run the dataset generation cell first!\")\n",
    "\n",
    "# Check if datasets were loaded successfully\n",
    "if train_dataset is None or test_dataset is None:\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚ö†Ô∏è  ERROR: Datasets not loaded!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Please run Cell 3 (Dataset Generation) first to create the .npz files.\")\n",
    "    print()\n",
    "    raise RuntimeError(\"Datasets not found. Run dataset generation cell first.\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# CREATE DATALOADERS\n",
    "# ============================================================\n",
    "\n",
    "print(\"Creating DataLoaders...\")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"‚úì DataLoaders created\")\n",
    "print(f\"  Training batches per epoch: {len(train_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# MODEL INITIALIZATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"Initializing model...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = KeypointNet().to(device)\n",
    "\n",
    "# Optimizer (SuperPoint paper: Adam with lr=0.001, betas=(0.9, 0.999))\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=adam_betas,\n",
    "    weight_decay=weight_decay\n",
    ")\n"
   ],
   "id": "5ff1a7b425389884",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING SETUP\n",
      "============================================================\n",
      "\n",
      "Loading training samples from ./dataset_cache\\train_samples_5000.npz...\n",
      "  Augmentation: ENABLED\n",
      "Loading 5000 samples from ./dataset_cache\\train_samples_5000.npz...\n",
      "‚úì Loaded 5000 samples!\n",
      "‚úì Training dataset loaded: 1000000000 samples\n",
      "Loading test samples from ./dataset_cache\\test_samples_500.npz...\n",
      "  Augmentation: DISABLED (test set)\n",
      "Loading 500 samples from ./dataset_cache\\test_samples_500.npz...\n",
      "‚úì Loaded 500 samples!\n",
      "‚úì Test dataset loaded: 1000000000 samples\n",
      "\n",
      "Creating DataLoaders...\n",
      "‚úì DataLoaders created\n",
      "  Training batches per epoch: 31250000\n",
      "  Test batches: 31250000\n",
      "\n",
      "Initializing model...\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-07T14:55:49.339695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "# Loss tracking\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "start_iteration = 0\n",
    "\n",
    "# Load checkpoint if exists\n",
    "checkpoint = load_checkpoint_generic(checkpoint_dir, device)\n",
    "if checkpoint:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_iteration = checkpoint.get('iteration', 0)\n",
    "    train_losses = checkpoint.get('train_losses', [])\n",
    "    test_losses = checkpoint.get('test_losses', [])\n",
    "    print(f\"‚úì Resuming from iteration {start_iteration:,}\")\n",
    "else:\n",
    "    print(\"‚úì Starting from scratch\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# ITERATION-BASED TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "model.train()\n",
    "running_loss = 0.0\n",
    "iteration = start_iteration\n",
    "\n",
    "# Create iterator for infinite cycling through dataset\n",
    "train_iterator = iter(train_loader)\n",
    "\n",
    "while iteration < num_iterations:\n",
    "    # Get next batch (infinite cycling)\n",
    "    try:\n",
    "        images, targets = next(train_iterator)\n",
    "    except StopIteration:\n",
    "        # Restart iterator when dataset is exhausted\n",
    "        train_iterator = iter(train_loader)\n",
    "        images, targets = next(train_iterator)\n",
    "\n",
    "    images = images.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(images, return_logits=True)  # (B, 65, H/8, W/8)\n",
    "    targets_idx = targets.argmax(dim=1)  # (B, H/8, W/8)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = F.cross_entropy(logits, targets_idx)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track loss\n",
    "    running_loss += loss.item()\n",
    "    iteration += 1\n",
    "\n",
    "    # Print progress\n",
    "    if iteration % print_every == 0:\n",
    "        avg_loss = running_loss / print_every\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Iter [{iteration:>6}/{num_iterations}] Loss: {avg_loss:.4f}\")\n",
    "        running_loss = 0.0\n",
    "\n",
    "    # Evaluate on test set\n",
    "    if iteration % eval_every == 0:\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        num_test_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for test_images, test_targets in test_loader:\n",
    "                test_images = test_images.to(device)\n",
    "                test_targets = test_targets.to(device)\n",
    "\n",
    "                logits = model(test_images, return_logits=True)\n",
    "                targets_idx = test_targets.argmax(dim=1)\n",
    "                loss = F.cross_entropy(logits, targets_idx)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                num_test_batches += 1\n",
    "\n",
    "        avg_test_loss = test_loss / num_test_batches\n",
    "        test_losses.append(avg_test_loss)\n",
    "        print(f\"  ‚îî‚îÄ Test Loss: {avg_test_loss:.4f}\")\n",
    "        model.train()\n",
    "\n",
    "    # Save checkpoint\n",
    "    if iteration % save_checkpoint_every == 0 or iteration == num_iterations:\n",
    "        save_checkpoint_generic(\n",
    "            checkpoint_dir,\n",
    "            iteration,\n",
    "            {\n",
    "                'iteration': iteration,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'test_losses': test_losses,\n",
    "                'config': {\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'batch_size': batch_size,\n",
    "                    'num_iterations': num_iterations,\n",
    "                }\n",
    "            },\n",
    "            max_checkpoints=max_checkpoints\n",
    "        )\n",
    "        print(f\"  ‚îî‚îÄ Checkpoint saved\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úì TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "if len(train_losses) > 0:\n",
    "    print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "if len(test_losses) > 0:\n",
    "    print(f\"Final Test Loss: {test_losses[-1]:.4f}\")\n",
    "print()\n"
   ],
   "id": "5c963dabbb8a6d57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ No checkpoint found, starting from scratch\n",
      "‚úì Starting from scratch\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# PLOT TRAINING CURVES\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "if len(train_losses) == 0 and len(test_losses) == 0:\n",
    "    print(\"‚ö†Ô∏è  No training data to plot. Run the training loop first.\")\n",
    "else:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "    # Plot training losses\n",
    "    if len(train_losses) > 0:\n",
    "        iterations_range = [(i + 1) * print_every for i in range(len(train_losses))]\n",
    "        ax.plot(iterations_range, train_losses, 'b-', label='Training Loss', linewidth=2, alpha=0.7)\n",
    "        print(f\"‚úì Training losses plotted ({len(train_losses)} points)\")\n",
    "\n",
    "    # Plot test losses\n",
    "    if len(test_losses) > 0:\n",
    "        test_iterations_range = [(i + 1) * eval_every for i in range(len(test_losses))]\n",
    "        ax.plot(test_iterations_range, test_losses, 'r-', label='Test Loss', linewidth=2, alpha=0.7)\n",
    "        print(f\"‚úì Test losses plotted ({len(test_losses)} points)\")\n",
    "\n",
    "    ax.set_xlabel('Iteration', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12)\n",
    "    ax.set_title('Training Progress - Interest Point Detection', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add minor gridlines for better readability\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(which='minor', alpha=0.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print()\n",
    "    if len(train_losses) > 0:\n",
    "        print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "        print(f\"Best Training Loss: {min(train_losses):.4f}\")\n",
    "    if len(test_losses) > 0:\n",
    "        print(f\"Final Test Loss: {test_losses[-1]:.4f}\")\n",
    "        print(f\"Best Test Loss: {min(test_losses):.4f}\")\n",
    "    print()\n",
    "    print(\"=\" * 60)\n"
   ],
   "id": "65bb98e5b93c7f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# MODEL TESTING AND VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL TESTING\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get a few test samples\n",
    "num_vis_samples = 4\n",
    "test_samples = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, targets) in enumerate(test_loader):\n",
    "        if i >= num_vis_samples:\n",
    "            break\n",
    "        test_samples.append((images[0], targets[0]))\n",
    "\n",
    "print(f\"Loaded {len(test_samples)} test samples for visualization\")\n",
    "print()\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(len(test_samples), 3, figsize=(15, 5 * len(test_samples)))\n",
    "if len(test_samples) == 1:\n",
    "    axes = axes[np.newaxis, :]\n",
    "\n",
    "for idx, (image, target) in enumerate(test_samples):\n",
    "    # Prepare input\n",
    "    image_input = image.unsqueeze(0).to(device)\n",
    "\n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        pred_heatmap = model(image_input)  # (1, 65, H/8, W/8)\n",
    "\n",
    "    # Convert to numpy for visualization\n",
    "    image_np = image.squeeze().cpu().numpy()\n",
    "    target_np = target.cpu().numpy()\n",
    "    pred_np = pred_heatmap.squeeze().cpu().numpy()\n",
    "\n",
    "    # Get max prob across channels for visualization\n",
    "    target_max = target_np.max(axis=0)\n",
    "    pred_max = pred_np.max(axis=0)\n",
    "\n",
    "    # Plot original image\n",
    "    axes[idx, 0].imshow(image_np, cmap='gray')\n",
    "    axes[idx, 0].set_title(f'Sample {idx + 1}: Input Image', fontsize=12, fontweight='bold')\n",
    "    axes[idx, 0].axis('off')\n",
    "\n",
    "    # Plot ground truth heatmap\n",
    "    axes[idx, 1].imshow(target_max, cmap='hot')\n",
    "    axes[idx, 1].set_title('Ground Truth Heatmap', fontsize=12, fontweight='bold')\n",
    "    axes[idx, 1].axis('off')\n",
    "\n",
    "    # Plot predicted heatmap\n",
    "    axes[idx, 2].imshow(pred_max, cmap='hot')\n",
    "    axes[idx, 2].set_title('Predicted Heatmap', fontsize=12, fontweight='bold')\n",
    "    axes[idx, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úì Visualization complete!\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "403f75d0b1638e8d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
