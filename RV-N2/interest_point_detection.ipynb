{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T20:08:27.275588Z",
     "start_time": "2025-12-04T20:07:59.613003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from Models import KeypointNet\n",
    "from Models import KeypointDataset\n",
    "from Generator import generate_synthetic_image\n"
   ],
   "id": "28711158f41d2aba",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# TRAINING SETUP AND LOOP\n",
    "# ============================================================\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from Helper import save_checkpoint_generic, load_checkpoint_generic\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING HYPERPARAMETERS\n",
    "# ============================================================\n",
    "\n",
    "# Model parameters (SuperPoint paper: Adam with lr=0.001, beta=(0.9, 0.999))\n",
    "learning_rate = 0.001\n",
    "adam_betas = (0.9, 0.999)\n",
    "weight_decay = 0.0\n",
    "\n",
    "# Training parameters (SuperPoint paper: 200k iterations, batch_size=32)\n",
    "# Adjust based on your hardware - paper used larger batches and more iterations\n",
    "num_epochs = 100\n",
    "batch_size = 16  # Increase if you have enough GPU memory (paper used 32)\n",
    "\n",
    "image_size = (240, 320)  # Height, Width\n",
    "\n",
    "# Dataset parameters\n",
    "num_train_samples = 5000  # Will result in ~312 iterations per epoch with batch_size=16\n",
    "num_test_samples = 500\n",
    "use_augmentation = True\n",
    "pregenerate_data = True  # Set to False for on-the-fly generation\n",
    "\n",
    "# Dataset saving/loading\n",
    "dataset_cache_dir = './dataset_cache'\n",
    "save_datasets = True  # Save datasets to disk for reuse\n",
    "load_datasets_if_exist = True  # Load from disk if available\n",
    "\n",
    "# Checkpoint parameters\n",
    "checkpoint_dir = './checkpoints'\n",
    "save_checkpoint_every = 5  # Save every N epochs\n",
    "max_checkpoints = 4  # Keep only last N checkpoints\n",
    "\n",
    "# Visualization parameters\n",
    "print_every = 20  # Print loss every N batches\n",
    "test_threshold = 0.015  # Threshold for keypoint detection during testing\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(dataset_cache_dir, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# DATASET CREATION WITH SAVE/LOAD\n",
    "# ============================================================\n",
    "\n",
    "import pickle\n",
    "\n",
    "train_dataset_path = os.path.join(dataset_cache_dir, f'train_dataset_{num_train_samples}.pkl')\n",
    "test_dataset_path = os.path.join(dataset_cache_dir, f'test_dataset_{num_test_samples}.pkl')\n",
    "\n",
    "# Try to load existing datasets\n",
    "train_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if load_datasets_if_exist and os.path.exists(train_dataset_path):\n",
    "    print(f\"Loading cached training dataset from {train_dataset_path}...\")\n",
    "    try:\n",
    "        with open(train_dataset_path, 'rb') as f:\n",
    "            train_dataset = pickle.load(f)\n",
    "        print(f\"✓ Training dataset loaded: {len(train_dataset)} samples\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Failed to load training dataset: {e}\")\n",
    "        print(\"Will create new dataset...\\n\")\n",
    "        train_dataset = None\n",
    "\n",
    "if load_datasets_if_exist and os.path.exists(test_dataset_path):\n",
    "    print(f\"Loading cached test dataset from {test_dataset_path}...\")\n",
    "    try:\n",
    "        with open(test_dataset_path, 'rb') as f:\n",
    "            test_dataset = pickle.load(f)\n",
    "        print(f\"✓ Test dataset loaded: {len(test_dataset)} samples\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Failed to load test dataset: {e}\")\n",
    "        print(\"Will create new dataset...\\n\")\n",
    "        test_dataset = None\n",
    "\n",
    "# Create datasets if not loaded\n",
    "if train_dataset is None:\n",
    "    print(\"Creating training dataset...\")\n",
    "    train_dataset = KeypointDataset(\n",
    "        num_samples=num_train_samples,\n",
    "        image_shape=image_size,\n",
    "        generate_fn=generate_synthetic_image,\n",
    "        generate_kwargs={\n",
    "            'width': image_size[1],\n",
    "            'height': image_size[0],\n",
    "            'shape_type': 'random',\n",
    "            'use_homography': True\n",
    "        },\n",
    "        augment=use_augmentation,\n",
    "        pregenerate=pregenerate_data\n",
    "    )\n",
    "    print(f\"✓ Training dataset created: {len(train_dataset)} samples\\n\")\n",
    "\n",
    "    # Save dataset\n",
    "    if save_datasets and pregenerate_data:\n",
    "        print(f\"Saving training dataset to {train_dataset_path}...\")\n",
    "        try:\n",
    "            with open(train_dataset_path, 'wb') as f:\n",
    "                pickle.dump(train_dataset, f)\n",
    "            print(f\"✓ Training dataset saved!\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Failed to save training dataset: {e}\\n\")\n",
    "\n",
    "if test_dataset is None:\n",
    "    print(\"Creating test dataset...\")\n",
    "    test_dataset = KeypointDataset(\n",
    "        num_samples=num_test_samples,\n",
    "        image_shape=image_size,\n",
    "        generate_fn=generate_synthetic_image,\n",
    "        generate_kwargs={\n",
    "            'width': image_size[1],\n",
    "            'height': image_size[0],\n",
    "            'shape_type': 'random',\n",
    "            'use_homography': True\n",
    "        },\n",
    "        augment=False,  # No augmentation for test set\n",
    "        pregenerate=True  # Always pregenerate test set\n",
    "    )\n",
    "    print(f\"✓ Test dataset created: {len(test_dataset)} samples\\n\")\n",
    "\n",
    "    # Save dataset\n",
    "    if save_datasets:\n",
    "        print(f\"Saving test dataset to {test_dataset_path}...\")\n",
    "        try:\n",
    "            with open(test_dataset_path, 'wb') as f:\n",
    "                pickle.dump(test_dataset, f)\n",
    "            print(f\"✓ Test dataset saved!\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Failed to save test dataset: {e}\\n\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"✓ DataLoaders created\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# MODEL INITIALIZATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"Initializing model...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "model = KeypointNet().to(device)\n",
    "\n",
    "# Optimizer (SuperPoint paper: Adam with lr=0.001, betas=(0.9, 0.999))\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=adam_betas,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# Loss tracking\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "start_epoch = 0\n",
    "\n",
    "# Load checkpoint if exists\n",
    "checkpoint = load_checkpoint_generic(checkpoint_dir, device)\n",
    "if checkpoint:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    train_losses = checkpoint.get('train_losses', [])\n",
    "    test_losses = checkpoint.get('test_losses', [])\n",
    "    print(f\"Resuming from epoch {start_epoch}\\n\")\n",
    "\n",
    "print(\"Model ready for training!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n"
   ],
   "id": "e2897ebe283fcd0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print()\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    # ========== TRAINING ==========\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    num_train_batches = 0\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(images, return_logits=True)  # (B, 65, H/8, W/8)\n",
    "\n",
    "        # Convert targets to class indices\n",
    "        targets_idx = targets.argmax(dim=1)  # (B, H/8, W/8)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(logits, targets_idx)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss\n",
    "        epoch_train_loss += loss.item()\n",
    "        num_train_batches += 1\n",
    "\n",
    "        # Print progress\n",
    "        if (batch_idx + 1) % print_every == 0:\n",
    "            avg_loss = epoch_train_loss / num_train_batches\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}] Batch [{batch_idx + 1}/{len(train_loader)}] \"\n",
    "                  f\"Loss: {loss.item():.4f} (Avg: {avg_loss:.4f})\")\n",
    "\n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = epoch_train_loss / num_train_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # ========== TESTING ==========\n",
    "    model.eval()\n",
    "    epoch_test_loss = 0.0\n",
    "    num_test_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(images, return_logits=True)\n",
    "\n",
    "            # Convert targets to class indices\n",
    "            targets_idx = targets.argmax(dim=1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.cross_entropy(logits, targets_idx)\n",
    "\n",
    "            epoch_test_loss += loss.item()\n",
    "            num_test_batches += 1\n",
    "\n",
    "    avg_test_loss = epoch_test_loss / num_test_batches\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Summary:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Test Loss:  {avg_test_loss:.4f}\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % save_checkpoint_every == 0 or (epoch + 1) == num_epochs:\n",
    "        save_checkpoint_generic(\n",
    "            checkpoint_dir,\n",
    "            epoch,\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'test_losses': test_losses,\n",
    "                'config': {\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'weight_decay': weight_decay,\n",
    "                    'batch_size': batch_size,\n",
    "                    'num_train_samples': num_train_samples,\n",
    "                    'num_test_samples': num_test_samples,\n",
    "                }\n",
    "            },\n",
    "            max_checkpoints=max_checkpoints\n",
    "        )\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "\n",
    "# ============================================================\n",
    "# PLOT TRAINING CURVES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nPlotting training curves...\")\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "ax.plot(epochs_range, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "ax.plot(epochs_range, test_losses, 'r-', label='Test Loss', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Training and Test Loss', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training curves plotted!\")\n",
    "print(f\"\\nFinal Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Test Loss: {test_losses[-1]:.4f}\")\n"
   ],
   "id": "c0b60abfbe5b8a90"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
