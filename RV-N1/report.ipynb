{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from Generator import generate_pair\n",
    "\n",
    "\n",
    "def nn_train_single(model, num_epochs, model_file_name, img, optimizer, criterion,\n",
    "                    checkpoint_dir=\"checkpoints\", num_classes=21, disp_range=(-16, 16)):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    device = next(model.parameters()).device\n",
    "    start_epoch = load_latest_checkpoint(checkpoint_dir, model, optimizer, device)\n",
    "    writer = SummaryWriter(log_dir=os.path.join(checkpoint_dir, \"runs\"))\n",
    "\n",
    "    progress_bar = tqdm(range(start_epoch, num_epochs), desc=\"Training\", ncols=120)\n",
    "\n",
    "    for epoch in progress_bar:\n",
    "        model.train()\n",
    "\n",
    "        # === Generate random sample ===\n",
    "        pair, offsets, *_ = generate_pair(\n",
    "            img=random.choice(img) if isinstance(img, list) else img,\n",
    "            window_size=64, margin=16, disp_range=disp_range\n",
    "        )\n",
    "        pair = torch.from_numpy(pair).permute(2, 0, 1).unsqueeze(0).to(device).float()\n",
    "        offsets = torch.from_numpy(offsets.flatten()).unsqueeze(0).to(device).float()  # (1,8)\n",
    "\n",
    "        # === Forward ===\n",
    "        preds = model(pair)\n",
    "\n",
    "        # === Compute loss depending on model type ===\n",
    "        target = offsets_to_class_indices(offsets, num_classes, disp_range)\n",
    "        preds_reshaped = preds.permute(0, 2, 1).reshape(-1, num_classes)\n",
    "        target = target.view(-1)\n",
    "        loss = criterion(preds_reshaped, target)\n",
    "\n",
    "        # === Backprop ===\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # === Compute metrics ===\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # ✅ hard decode → realistic RMSE\n",
    "            pred_offsets = classes_to_offsets(preds, disp_range, soft=False)\n",
    "\n",
    "            # ------ classification accuracy ------\n",
    "            # predicted classes\n",
    "            pred_class = torch.argmax(preds, dim=1)  # (B, 8)\n",
    "\n",
    "            # target classes\n",
    "            tgt_class = offsets_to_class_indices(offsets, num_classes, disp_range)\n",
    "\n",
    "            class_acc = (pred_class == tgt_class).float().mean().item()\n",
    "\n",
    "            # ------ geometric error metrics ------\n",
    "            mae = torch.mean(torch.abs(pred_offsets - offsets)).item()\n",
    "            rmse = torch.sqrt(torch.mean((pred_offsets - offsets) ** 2) + 1e-8).item()\n",
    "\n",
    "        # === Logging ===\n",
    "        writer.add_scalar(\"Loss/CE\", loss.item(), epoch)\n",
    "        writer.add_scalar(\"Metric/ClassAcc\", class_acc, epoch)\n",
    "\n",
    "        # geometric metrics for both models\n",
    "        writer.add_scalar(\"Metric/MAE_px\", mae, epoch)\n",
    "        writer.add_scalar(\"Metric/RMSE_px\", rmse, epoch)\n",
    "\n",
    "        progress_bar.set_description(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.5f}\", mae=f\"{mae:.3f}\", rmse=f\"{rmse:.3f}\")\n",
    "\n",
    "        # === Save checkpoint ===\n",
    "        if (epoch + 1) % 1000 == 0 or (epoch + 1) == num_epochs:\n",
    "            save_checkpoint(checkpoint_dir, epoch + 1, model, optimizer)\n",
    "\n",
    "    writer.close()\n",
    "    torch.save(model.state_dict(), model_file_name)\n",
    "    print(f\"✅ Final model saved: {model_file_name}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ],
   "id": "b64c9830ec93e55e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# Prepair Dataset\n",
    "# ============================================================\n",
    "\n",
    "from Generator import prepair_dataset\n",
    "\n",
    "INPUT_DIR = \"datasets/val2017\"\n",
    "PREPROCESSED_DIR = \"datasets/val2017_preprocessed\"\n",
    "\n",
    "TARGET_SIZE = (320, 240)\n",
    "# prepair_dataset(INPUT_DIR, PREPROCESSED_DIR, TARGET_SIZE)\n"
   ],
   "id": "e6ef2d4e70e6a340"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from Generator import visualize_generate_pair\n",
    "\n",
    "# Run visualization\n",
    "visualize_generate_pair(PREPROCESSED_DIR)"
   ],
   "id": "f65fd2f320b7679d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from Models import visualize_offset_sign\n",
    "\n",
    "visualize_offset_sign(PREPROCESSED_DIR)"
   ],
   "id": "29b1edef1e66a525"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# visualization of regression results\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HomographyRegressor().to(device)\n",
    "# state = torch.load(\"checkpoints_homography_regressor_oneImage/checkpoint_epoch_50000.pth\")[\"model_state_dict\"]\n",
    "state = torch.load(\"checkpoints_homography_regressor_oneImage/h_regressor_ep50000_I1.pth\")\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "img = cv2.imread(\"datasets/val2017_preprocessed/000000002299.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "visualize_regression_result(model, img)"
   ],
   "id": "92184a3b862b86af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from Generator import generate_pair\n",
    "\n",
    "def nn_train_single(model, num_epochs, model_file_name, img, optimizer, criterion,\n",
    "                    checkpoint_dir=\"checkpoints\", num_classes=21, disp_range=(-16, 16)):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    device = next(model.parameters()).device\n",
    "    start_epoch = load_latest_checkpoint(checkpoint_dir, model, optimizer, device)\n",
    "    writer = SummaryWriter(log_dir=os.path.join(checkpoint_dir, \"runs\"))\n",
    "\n",
    "    progress_bar = tqdm(range(start_epoch, num_epochs), desc=\"Training\", ncols=120)\n",
    "\n",
    "    for epoch in progress_bar:\n",
    "        model.train()\n",
    "\n",
    "        # === Generate random sample ===\n",
    "        pair, offsets, *_ = generate_pair(\n",
    "            img=random.choice(img) if isinstance(img, list) else img,\n",
    "            window_size=64, margin=16, disp_range=disp_range\n",
    "        )\n",
    "        pair = torch.from_numpy(pair).permute(2, 0, 1).unsqueeze(0).to(device).float()\n",
    "        offsets = torch.from_numpy(offsets.flatten()).unsqueeze(0).to(device).float()  # (1,8)\n",
    "\n",
    "        # === Forward ===\n",
    "        preds = model(pair)\n",
    "\n",
    "        # === Compute loss depending on model type ===\n",
    "        if isinstance(model, HomographyClassifier):\n",
    "            # classification head → CrossEntropy loss\n",
    "            target = offsets_to_class_indices(offsets, num_classes, disp_range)\n",
    "            preds_reshaped = preds.permute(0, 2, 1).reshape(-1, num_classes)\n",
    "            target = target.view(-1)\n",
    "            loss = criterion(preds_reshaped, target)\n",
    "        else:\n",
    "            # regression head → MSE loss\n",
    "            loss = criterion(preds, -offsets)\n",
    "\n",
    "        # === Backprop ===\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # === Compute metrics ===\n",
    "        with torch.no_grad():\n",
    "\n",
    "            if isinstance(model, HomographyClassifier):\n",
    "                # ✅ hard decode → realistic RMSE\n",
    "                pred_offsets = classes_to_offsets(preds, disp_range, soft=False)\n",
    "\n",
    "                # ------ classification accuracy ------\n",
    "                # predicted classes\n",
    "                pred_class = torch.argmax(preds, dim=1)  # (B, 8)\n",
    "\n",
    "                # target classes\n",
    "                tgt_class = offsets_to_class_indices(offsets, num_classes, disp_range)\n",
    "\n",
    "                class_acc = (pred_class == tgt_class).float().mean().item()\n",
    "\n",
    "            else:\n",
    "                pred_offsets = preds\n",
    "                class_acc = None\n",
    "\n",
    "            # ------ geometric error metrics ------\n",
    "            mae = torch.mean(torch.abs(pred_offsets - offsets)).item()\n",
    "            rmse = torch.sqrt(torch.mean((pred_offsets - offsets) ** 2) + 1e-8).item()\n",
    "\n",
    "        # === Logging ===\n",
    "        if isinstance(model, HomographyClassifier):\n",
    "            writer.add_scalar(\"Loss/CE\", loss.item(), epoch)\n",
    "            writer.add_scalar(\"Metric/ClassAcc\", class_acc, epoch)\n",
    "\n",
    "        else:\n",
    "            writer.add_scalar(\"Loss/MSE\", loss.item(), epoch)\n",
    "            writer.add_scalar(\"Loss/RMSE\", torch.sqrt(loss + 1e-8).item(), epoch)\n",
    "\n",
    "        # geometric metrics for both models\n",
    "        writer.add_scalar(\"Metric/MAE_px\", mae, epoch)\n",
    "        writer.add_scalar(\"Metric/RMSE_px\", rmse, epoch)\n",
    "\n",
    "        progress_bar.set_description(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.5f}\", mae=f\"{mae:.3f}\", rmse=f\"{rmse:.3f}\")\n",
    "\n",
    "        # === Save checkpoint ===\n",
    "        if (epoch + 1) % 1000 == 0 or (epoch + 1) == num_epochs:\n",
    "            save_checkpoint(checkpoint_dir, epoch + 1, model, optimizer)\n",
    "\n",
    "    writer.close()\n",
    "    torch.save(model.state_dict(), model_file_name)\n",
    "    print(f\"✅ Final model saved: {model_file_name}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ],
   "id": "7018003df382f739"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def nn_train_multi(model, num_epochs, batch_size, samples_per_epoch, model_file_name,\n",
    "                   images, optimizer, criterion,\n",
    "                   checkpoint_dir=\"checkpoints\", num_classes=21, disp_range=(-16, 16)):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    device = next(model.parameters()).device\n",
    "    start_epoch = load_latest_checkpoint(checkpoint_dir, model, optimizer, device)\n",
    "    writer = SummaryWriter(log_dir=os.path.join(checkpoint_dir, \"runs\"))\n",
    "\n",
    "    dataset = HomographyPairDataset(images, samples_per_epoch)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                            num_workers=0, pin_memory=True)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss, epoch_mae, epoch_rmse = 0.0, 0.0, 0.0\n",
    "\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", ncols=120, leave=False)\n",
    "\n",
    "        for pairs, offsets in progress_bar:\n",
    "            pairs = pairs.to(device)\n",
    "            offsets = offsets.to(device)\n",
    "\n",
    "            preds = model(pairs)\n",
    "\n",
    "            # === Compute loss ===\n",
    "            if isinstance(model, HomographyClassifier):\n",
    "                target = offsets_to_class_indices(offsets, num_classes, disp_range)\n",
    "                preds_reshaped = preds.permute(0, 2, 1).reshape(-1, num_classes)\n",
    "                target = target.view(-1)\n",
    "                loss = criterion(preds_reshaped, target)\n",
    "            else:\n",
    "                loss = criterion(preds, offsets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # === Metrics ===\n",
    "            with torch.no_grad():\n",
    "                if isinstance(model, HomographyClassifier):\n",
    "                    pred_offsets = classes_to_offsets(preds, disp_range, soft=True)\n",
    "                else:\n",
    "                    pred_offsets = preds\n",
    "\n",
    "                mae = torch.mean(torch.abs(pred_offsets - offsets)).item()\n",
    "                rmse = torch.sqrt(torch.mean((pred_offsets - offsets) ** 2) + 1e-8).item()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_mae += mae\n",
    "            epoch_rmse += rmse\n",
    "\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.5f}\", mae=f\"{mae:.3f}\", rmse=f\"{rmse:.3f}\")\n",
    "\n",
    "        # === Epoch summary ===\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        avg_mae = epoch_mae / len(dataloader)\n",
    "        avg_rmse = epoch_rmse / len(dataloader)\n",
    "\n",
    "        if isinstance(model, HomographyClassifier):\n",
    "            writer.add_scalar(\"Loss/CrossEntropy\", avg_loss, epoch)\n",
    "        else:\n",
    "            writer.add_scalar(\"Loss/MSE\", avg_loss, epoch)\n",
    "            writer.add_scalar(\"Loss/RMSE\", np.sqrt(avg_loss), epoch)\n",
    "\n",
    "        writer.add_scalar(\"Error/MAE\", avg_mae, epoch)\n",
    "        writer.add_scalar(\"Error/RMSE\", avg_rmse, epoch)\n",
    "\n",
    "        # === Checkpoint saving ===\n",
    "        if (epoch + 1) % 100 == 0 or (epoch + 1) == num_epochs:\n",
    "            save_checkpoint(checkpoint_dir, epoch + 1, model, optimizer)\n",
    "\n",
    "    writer.close()\n",
    "    torch.save(model.state_dict(), model_file_name)\n",
    "    print(f\"✅ Final model saved: {model_file_name}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ],
   "id": "ccb6ce1a012baeba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# EVALUACIJSKI MODUL\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "# --- 1) Generiranje testnega nabora (~100 slik × 10 primerov) ---\n",
    "def generate_test_set(images, n_images=100, samples_per_image=10,\n",
    "                      window_size=64, margin=16, disp_range=(-16, 16), seed=42):\n",
    "    \"\"\"\n",
    "    images: lista numpy sivinskih slik\n",
    "    n_images: koliko različnih izvornih slik izbrati (max len(images))\n",
    "    samples_per_image: število primerov na sliko\n",
    "    returns: lista sample dictov:\n",
    "      {\n",
    "        'image': full_image,\n",
    "        'pair': pair (2chan 64x64 float32),\n",
    "        'offsets': gt_offsets (4x2 float32),\n",
    "        'src_corners': src_corners (4x2),\n",
    "        'dst_corners': dst_corners (4x2),\n",
    "        'x': x, 'y': y,\n",
    "        'orig_patch': orig_patch,\n",
    "        'warped_patch': warped_patch\n",
    "      }\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    selected = images[:min(n_images, len(images))]\n",
    "    samples = []\n",
    "    for img in selected:\n",
    "        for _ in range(samples_per_image):\n",
    "            pair, offsets, src_corners, warped_full = generate_pair(img, window_size=window_size, margin=margin,\n",
    "                                                                    disp_range=disp_range)\n",
    "            dst_corners = src_corners + offsets\n",
    "            # Find x,y from src_corners (top-left)\n",
    "            x = int(src_corners[0, 0]);\n",
    "            y = int(src_corners[0, 1])\n",
    "            orig_patch = (img[y:y + window_size, x:x + window_size]).astype(np.uint8)\n",
    "            warped_patch = (warped_full[y:y + window_size, x:x + window_size]).astype(np.uint8)\n",
    "            samples.append({\n",
    "                'image': img,\n",
    "                'pair': pair,  # float32 [H,W,2] / normalized [0,1]\n",
    "                'offsets': offsets.reshape(4, 2),\n",
    "                'src_corners': src_corners,\n",
    "                'dst_corners': dst_corners,\n",
    "                'x': x, 'y': y,\n",
    "                'orig_patch': orig_patch,\n",
    "                'warped_patch': warped_patch\n",
    "            })\n",
    "    print(f\"➡️ Generiranih {len(samples)} testnih primerov iz {len(selected)} slik.\")\n",
    "    return samples\n",
    "\n",
    "\n",
    "# --- 2) RMSE utility ---\n",
    "def corner_rmse(pred_offsets, gt_offsets):\n",
    "    \"\"\"\n",
    "    pred_offsets, gt_offsets: (4,2) arrays\n",
    "    return scalar RMSE over 8 values\n",
    "    \"\"\"\n",
    "    diff = (pred_offsets - gt_offsets).astype(np.float32).reshape(-1)\n",
    "    return float(np.sqrt(np.mean(diff ** 2) + 1e-12))\n",
    "\n",
    "\n",
    "# --- 3) Eval: Nevronski model (regresor ali klasifikator) ---\n",
    "def eval_model_on_testset(model, test_samples, device,\n",
    "                          model_type='regressor',  # 'regressor' or 'classifier'\n",
    "                          disp_range=(-16, 16), negate_pred=False,\n",
    "                          soft_decode=False, batch_size=32):\n",
    "    \"\"\"\n",
    "    model_type: 'regressor' -> model returns (B,8) offsets; 'classifier' -> logits (B,num_classes,8)\n",
    "    negate_pred: if your regressor predicts negative offsets during training, set True\n",
    "    soft_decode: if classifier, whether to soft-decode expected value\n",
    "    returns: dict with 'rmses' list, 'per_sample' list of dicts with preds etc.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = {'rmses': [], 'per_sample': []}\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_samples), batch_size):\n",
    "            batch = test_samples[i:i + batch_size]\n",
    "            pairs = np.stack([s['pair'] for s in batch], axis=0)  # (B,H,W,2)\n",
    "            pairs_t = torch.from_numpy(pairs).permute(0, 3, 1, 2).float().to(device)  # (B,2,H,W)\n",
    "            preds = model(pairs_t)\n",
    "            if model_type == 'classifier':\n",
    "                # preds shape (B, num_classes, 8)\n",
    "                pred_offsets = classes_to_offsets(preds, disp_range, soft=soft_decode).cpu().numpy()  # (B,8)\n",
    "            else:\n",
    "                pred_offsets = preds.cpu().numpy()  # (B,8)\n",
    "                if negate_pred:\n",
    "                    pred_offsets = -pred_offsets\n",
    "            # reshape Bx8 -> Bx4x2\n",
    "            pred_offsets = pred_offsets.reshape(pred_offsets.shape[0], 4, 2)\n",
    "            for j, s in enumerate(batch):\n",
    "                rmse = corner_rmse(pred_offsets[j], s['offsets'])\n",
    "                results['rmses'].append(rmse)\n",
    "                results['per_sample'].append({\n",
    "                    'pred_offsets': pred_offsets[j],\n",
    "                    'gt_offsets': s['offsets'],\n",
    "                    'src_corners': s['src_corners'],\n",
    "                    'dst_corners_gt': s['dst_corners'],\n",
    "                    'orig_patch': s['orig_patch'],\n",
    "                    'warped_patch': s['warped_patch'],\n",
    "                    'image': s['image'],\n",
    "                    'x': s['x'], 'y': s['y'],\n",
    "                    'rmse': rmse\n",
    "                })\n",
    "    return results\n",
    "\n",
    "\n",
    "# --- 4) Klasični OpenCV pristop (SIFT/SURF/ORB + findHomography) ---\n",
    "def estimate_homography_classical(orig_patch, warped_patch, min_matches=4):\n",
    "    \"\"\"\n",
    "    SIFT-only homography estimator.\n",
    "    Returns: (success:bool, H:ndarray|None, num_matches:int, message:str)\n",
    "    \"\"\"\n",
    "    assert orig_patch.ndim == 2 and warped_patch.ndim == 2\n",
    "\n",
    "    # create SIFT detector\n",
    "    try:\n",
    "        sift = cv2.SIFT_create()\n",
    "    except Exception as e:\n",
    "        return False, None, 0, f\"SIFT not available: {e}\"\n",
    "\n",
    "    # detect and compute\n",
    "    kp1, des1 = sift.detectAndCompute(orig_patch, None)\n",
    "    kp2, des2 = sift.detectAndCompute(warped_patch, None)\n",
    "\n",
    "    if des1 is None or des2 is None or len(kp1) < 2 or len(kp2) < 2:\n",
    "        return False, None, 0, f\"Not enough keypoints ({len(kp1) if kp1 else 0}, {len(kp2) if kp2 else 0})\"\n",
    "\n",
    "    # FLANN matcher for SIFT\n",
    "    index_params = dict(algorithm=1, trees=5)  # KDTree\n",
    "    search_params = dict(checks=50)\n",
    "    matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    try:\n",
    "        knn_matches = matcher.knnMatch(des1, des2, k=2)\n",
    "    except Exception as e:\n",
    "        return False, None, 0, f\"FLANN matching failed: {e}\"\n",
    "\n",
    "    # Lowe's ratio test\n",
    "    good = []\n",
    "    for m in knn_matches:\n",
    "        if len(m) == 2:\n",
    "            a, b = m\n",
    "            if a.distance < 0.75 * b.distance:\n",
    "                good.append(a)\n",
    "\n",
    "    if len(good) < min_matches:\n",
    "        return False, None, len(good), f\"Too few good matches ({len(good)})\"\n",
    "\n",
    "    # build point arrays and estimate homography\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "\n",
    "    try:\n",
    "        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "    except Exception as e:\n",
    "        return False, None, len(good), f\"findHomography failed: {e}\"\n",
    "\n",
    "    if H is None:\n",
    "        return False, None, len(good), \"findHomography returned None\"\n",
    "\n",
    "    inliers = int(mask.sum()) if mask is not None else 0\n",
    "    return True, H, len(good), f\"SIFT: success, inliers={inliers}\"\n",
    "\n",
    "\n",
    "def eval_classical_on_testset(test_samples, fallback_identity=True,\n",
    "                              use_256_for_classical=False):\n",
    "    \"\"\"\n",
    "    Evaluate classical approach on list of test_samples (as generated).\n",
    "    If use_256_for_classical: will upsample patches to 256x256 before detection (to help classical),\n",
    "      then the computed corner error is rescaled (divided by 4) as requested.\n",
    "    Returns results dict similar to eval_model_on_testset plus num_failures count.\n",
    "    \"\"\"\n",
    "    results = {'rmses': [], 'per_sample': [], 'num_failures': 0, 'num_total': len(test_samples)}\n",
    "    for s in test_samples:\n",
    "        orig = s['orig_patch']\n",
    "        warped = s['warped_patch']\n",
    "        # optionally upsample to 256x256:\n",
    "        scale_factor = 1\n",
    "        if use_256_for_classical:\n",
    "            target = 256\n",
    "            scale_factor = target / orig.shape[0]\n",
    "            orig_up = cv2.resize(orig, (target, target), interpolation=cv2.INTER_LINEAR)\n",
    "            warped_up = cv2.resize(warped, (target, target), interpolation=cv2.INTER_LINEAR)\n",
    "            ok, H, nm, msg = estimate_homography_classical(orig_up, warped_up)\n",
    "            if ok:\n",
    "                # we computed H that maps src->dst in upsampled coordinates.\n",
    "                # To apply on original coordinates, adjust H for scaling:\n",
    "                S = np.array([[1 / scale_factor, 0, 0], [0, 1 / scale_factor, 0], [0, 0, 1]])\n",
    "                H_adj = S @ H @ np.linalg.inv(S)\n",
    "                H_used = H_adj\n",
    "            else:\n",
    "                H_used = None\n",
    "        else:\n",
    "            ok, H, nm, msg = estimate_homography_classical(orig, warped)\n",
    "            H_used = H if ok else None\n",
    "\n",
    "        if H_used is None and fallback_identity:\n",
    "            # identity homography => predicted dst corners = src_corners\n",
    "            pred_dst = s['src_corners']\n",
    "            results['num_failures'] += 1\n",
    "            comment = 'fallback_identity'\n",
    "        elif H_used is None:\n",
    "            pred_dst = s['src_corners']\n",
    "            comment = 'failed_no_fallback'\n",
    "            results['num_failures'] += 1\n",
    "        else:\n",
    "            # apply H to src_corners (each point as homogenous)\n",
    "            pts = s['src_corners'].reshape(-1, 2)\n",
    "            ones = np.ones((pts.shape[0], 1))\n",
    "            hom_pts = np.concatenate([pts, ones], axis=1).T  # 3x4\n",
    "            mapped = (H_used @ hom_pts).T  # 4x3\n",
    "            mapped = mapped[:, :2] / mapped[:, 2:3]\n",
    "            pred_dst = mapped.astype(np.float32)\n",
    "            comment = f\"ok_matches={nm}\"\n",
    "\n",
    "        pred_offsets = pred_dst - s['src_corners']\n",
    "        rmse = corner_rmse(pred_offsets, s['offsets'])\n",
    "        # adjust RMSE if we used 256-upsample and user requested dividing by 4\n",
    "        if use_256_for_classical:\n",
    "            rmse = float(rmse / 4.0)\n",
    "\n",
    "        results['rmses'].append(rmse)\n",
    "        results['per_sample'].append({\n",
    "            'pred_offsets': pred_offsets,\n",
    "            'gt_offsets': s['offsets'],\n",
    "            'src_corners': s['src_corners'],\n",
    "            'dst_corners_gt': s['dst_corners'],\n",
    "            'rmse': rmse,\n",
    "            'comment': comment\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "# --- 5) Plotting and statistics ---\n",
    "def summarize_and_plot(\n",
    "        results_dict,\n",
    "        labels,\n",
    "        outdir='eval_results',\n",
    "        bins=40,\n",
    "        save_plots=True,\n",
    "        ymax=200\n",
    "):\n",
    "    \"\"\"\n",
    "    results_dict: list of results (each has 'rmses')\n",
    "    labels: list of labels\n",
    "    ymax: y-axis limit for boxplot + histogram (None = auto)\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    rmse_arrays = [np.array(r['rmses'], dtype=np.float32) for r in results_dict]\n",
    "\n",
    "    # ---- SUMMARY PRINT ----\n",
    "    summary = {}\n",
    "    for lab, arr, r in zip(labels, rmse_arrays, results_dict):\n",
    "        mean = float(np.mean(arr))\n",
    "        med = float(np.median(arr))\n",
    "        std = float(np.std(arr))\n",
    "        num = len(arr)\n",
    "        num_fail = r.get('num_failures', 0)\n",
    "        summary[lab] = {'mean': mean, 'median': med, 'std': std, 'n': num, 'failures': num_fail}\n",
    "        print(f\"--- {lab} --- n={num}, failures={num_fail}\\n  mean={mean:.3f}, median={med:.3f}, std={std:.3f}\")\n",
    "\n",
    "    # ---- BOXPLOT ----\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    # plt.boxplot(rmse_arrays, labels=labels, showfliers=True)\n",
    "    plt.boxplot(rmse_arrays, tick_labels=labels, showfliers=True)\n",
    "\n",
    "    if ymax is not None:\n",
    "        plt.ylim(0, ymax)\n",
    "\n",
    "    plt.ylabel(\"RMSE (px)\")\n",
    "    plt.title(\"RMSE boxplot\")\n",
    "    if save_plots:\n",
    "        p = os.path.join(outdir, \"rmse_boxplot.png\")\n",
    "        plt.savefig(p, dpi=150)\n",
    "        print(\"➡️ Shrani:\", p)\n",
    "    plt.show()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# EXAMPLE uporabe (po treningu)\n",
    "# -------------------------\n",
    "# Predpostavke:\n",
    "# - imaš naložen model (regressor ali classifier) kot 'model' v device\n",
    "# - imaš seznam sivinskih slik 'images' (npr. get_random_images(...))\n",
    "# - če uporabljaš classifier, nastavi model_type='classifier' pri eval_model_on_testset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HomographyRegressor().to(device)\n",
    "# state = torch.load(\"checkpoints_homography_regressor_oneImage/checkpoint_epoch_50000.pth\")[\"model_state_dict\"]\n",
    "state = torch.load(\"checkpoints_homography_regressor_oneImage/h_regressor_ep50000_I1.pth\")\n",
    "model.load_state_dict(state)\n",
    "\n",
    "# Primer: generiraj testset\n",
    "test_samples = generate_test_set(\n",
    "    images=get_random_images(\n",
    "        num_images=100,\n",
    "        image_dir=PREPROCESSED_DIR\n",
    "    ),\n",
    "    n_images=100,\n",
    "    samples_per_image=10,\n",
    "    window_size=64,\n",
    "    margin=16,\n",
    "    disp_range=(-16, 16)\n",
    ")\n",
    "\n",
    "# Primer: ocena tvojega nevronskega modela (classifier ali regressor)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "model_type = 'regressor'  # ali 'regressor'\n",
    "neg = False  # če tvoj regressor v treningu uči negativne pomike, nastavi True\n",
    "model_results = eval_model_on_testset(\n",
    "    model,\n",
    "    test_samples,\n",
    "    device,\n",
    "    model_type=model_type,\n",
    "    negate_pred=neg,\n",
    "    soft_decode=True\n",
    ")\n",
    "\n",
    "# Primer: ocena klasičnega pristopa\n",
    "classical_results = eval_classical_on_testset(\n",
    "    test_samples,\n",
    "    fallback_identity=True,\n",
    "    use_256_for_classical=True\n",
    ")\n",
    "\n",
    "# Primer: primerjava in ploti\n",
    "summary = summarize_and_plot(\n",
    "    [model_results, classical_results],\n",
    "    labels=[model.__class__.__name__, 'Classical_OpenCV'],\n",
    "    outdir='eval_results',\n",
    "    save_plots=False,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
