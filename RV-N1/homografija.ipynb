{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ],
   "id": "7324b9abfa64a763",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-27T23:14:52.903439900Z",
     "start_time": "2025-10-27T21:24:04.719099Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# Prepair Dataset\n",
    "# ============================================================\n",
    "\n",
    "def Preprocess_and_save_images(INPUT_DIR, OUTPUT_DIR, TARGET_SIZE):\n",
    "    # Ustvari izhodni direktorij, ƒçe ≈°e ne obstaja\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # Pridobi vse .jpg slike\n",
    "    image_paths = [os.path.join(INPUT_DIR, f) for f in os.listdir(INPUT_DIR)\n",
    "                   if f.lower().endswith(('.jpg', '.jpeg'))]\n",
    "\n",
    "    print(f\"Najdenih {len(image_paths)} slik za obdelavo...\")\n",
    "\n",
    "    for path in tqdm(image_paths, desc=\"Obdelujem slike\"):\n",
    "        # Preberi sliko\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            print(f\"‚ùå Napaka pri branju slike: {path}\")\n",
    "            continue\n",
    "\n",
    "        # Pretvori v sivinsko\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Spremeni velikost (320x240)\n",
    "        resized = cv2.resize(gray, TARGET_SIZE)\n",
    "\n",
    "        # Ustvari enako ime datoteke v izhodni mapi\n",
    "        filename = os.path.basename(path)\n",
    "        output_path = os.path.join(OUTPUT_DIR, filename)\n",
    "\n",
    "        # Shrani obdelano sliko\n",
    "        cv2.imwrite(output_path, resized)\n",
    "\n",
    "    print(\"‚úÖ Vse slike so uspe≈°no predobdelane in shranjene v:\")\n",
    "    print(f\"   {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "INPUT_DIR = \"datasets/val2017\"\n",
    "PREPROCESSED_DIR = \"datasets/val2017_preprocessed\"\n",
    "\n",
    "TARGET_SIZE = (320, 240)\n",
    "# Preprocess_and_save_images(INPUT_DIR, OUTPUT_DIR, TARGET_SIZE)\n"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T23:15:13.616768100Z",
     "start_time": "2025-10-27T21:24:04.860633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# HELPER FUNKCIJE ZA GENERIRANJE PAROV\n",
    "# ============================================================\n",
    "\n",
    "def sample_window(img_shape, window_size=64, margin=16):\n",
    "    h, w = img_shape[:2]\n",
    "    x = random.randint(margin, w - margin - window_size)\n",
    "    y = random.randint(margin, h - margin - window_size)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_corners(x, y, window_size=64):\n",
    "    return np.array([\n",
    "        [x, y],\n",
    "        [x + window_size, y],\n",
    "        [x + window_size, y + window_size],\n",
    "        [x, y + window_size]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "\n",
    "def perturb_corners(corners, disp_range=(-16, 16)):\n",
    "    min_disp, max_disp = disp_range\n",
    "    disp = np.random.randint(min_disp, max_disp + 1, size=corners.shape).astype(np.float32)\n",
    "    return corners + disp\n",
    "\n",
    "\n",
    "def generate_pair(img, window_size=64, margin=16, disp_range=(-16, 16)):\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    x, y = sample_window((h, w), window_size, margin)\n",
    "\n",
    "    src_corners = get_corners(x, y, window_size)\n",
    "    dst_corners = perturb_corners(src_corners, disp_range)\n",
    "\n",
    "    # Homografija H (src -> dst) in njen inverz\n",
    "    H = cv2.getPerspectiveTransform(src_corners, dst_corners)\n",
    "    H_inv = np.linalg.inv(H)\n",
    "\n",
    "    # Warp celotne slike z H^-1\n",
    "    warped = cv2.warpPerspective(img, H_inv, (w, h), flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Izre≈æi patche\n",
    "    orig_patch = img[y:y + window_size, x:x + window_size]\n",
    "    warped_patch = warped[y:y + window_size, x:x + window_size]\n",
    "\n",
    "    # Stack v 2 kanala in normaliziraj\n",
    "    pair = np.stack([orig_patch, warped_patch], axis=-1).astype(np.float32) / 255.0\n",
    "\n",
    "    # Ground truth: pomiki kotiƒçkov\n",
    "    offsets = (dst_corners - src_corners).astype(np.float32)\n",
    "\n",
    "    return pair, offsets, src_corners, warped\n"
   ],
   "id": "f65adb294cefbdbc",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T23:15:14.467035900Z",
     "start_time": "2025-10-27T21:24:04.998129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# TEST GENERIRANJA PARA\n",
    "# ============================================================\n",
    "\n",
    "def visualize_generate_pair(image_dir):\n",
    "    # Nalo≈æ nakljuƒçno sliko\n",
    "    image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir)\n",
    "                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    img_path = random.choice(image_paths)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "    window_size = 64\n",
    "    margin = 16\n",
    "    disp_range = (-16, 16)\n",
    "\n",
    "    # Sample window\n",
    "    x, y = sample_window((h, w), window_size, margin)\n",
    "\n",
    "    # Get corners\n",
    "    src_corners = get_corners(x, y, window_size)\n",
    "    dst_corners = perturb_corners(src_corners, disp_range)\n",
    "\n",
    "    # Compute homography\n",
    "    H = cv2.getPerspectiveTransform(src_corners, dst_corners)\n",
    "    H_inv = np.linalg.inv(H)\n",
    "\n",
    "    # Warp image\n",
    "    warped = cv2.warpPerspective(img, H_inv, (w, h), flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Extract patches\n",
    "    orig_patch = img[y:y + window_size, x:x + window_size]\n",
    "    warped_patch = warped[y:y + window_size, x:x + window_size]\n",
    "\n",
    "    # Calculate offsets\n",
    "    offsets = dst_corners - src_corners\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    # 1. Original image with source corners\n",
    "    ax = axes[0, 0]\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    for i, (cx, cy) in enumerate(src_corners):\n",
    "        ax.plot(cx, cy, 'go', markersize=10)\n",
    "        ax.text(cx, cy - 5, f'{i}', color='green', fontsize=12, ha='center')\n",
    "    rect = plt.Rectangle((x, y), window_size, window_size, fill=False, edgecolor='green', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.set_title('Original Image + Source Corners (green)')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # 2. Original image with destination corners\n",
    "    ax = axes[0, 1]\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    for i, (cx, cy) in enumerate(dst_corners):\n",
    "        ax.plot(cx, cy, 'ro', markersize=10)\n",
    "        ax.text(cx, cy - 5, f'{i}', color='red', fontsize=12, ha='center')\n",
    "    # Draw lines showing displacement\n",
    "    for i in range(4):\n",
    "        ax.arrow(src_corners[i, 0], src_corners[i, 1],\n",
    "                 offsets[i, 0], offsets[i, 1],\n",
    "                 head_width=3, head_length=3, fc='yellow', ec='yellow', alpha=0.7)\n",
    "    rect = plt.Rectangle((x, y), window_size, window_size, fill=False, edgecolor='green', linewidth=2, linestyle='--')\n",
    "    ax.add_patch(rect)\n",
    "    ax.set_title(f'Perturbed Corners (red)\\nAvg offset: {np.abs(offsets).mean():.1f}px')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # 3. Warped image with H^-1\n",
    "    ax = axes[0, 2]\n",
    "    ax.imshow(warped, cmap='gray')\n",
    "    rect = plt.Rectangle((x, y), window_size, window_size, fill=False, edgecolor='blue', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.set_title('Warped Image (H‚Åª¬π applied)')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # 4. Original patch\n",
    "    ax = axes[1, 0]\n",
    "    ax.imshow(orig_patch, cmap='gray')\n",
    "    ax.set_title('Original Patch (64√ó64)')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # 5. Warped patch\n",
    "    ax = axes[1, 1]\n",
    "    ax.imshow(warped_patch, cmap='gray')\n",
    "    ax.set_title('Warped Patch (64√ó64)')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Hide the 6th subplot\n",
    "    axes[1, 2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print offsets\n",
    "    print(\"\\nCorner offsets (Œîx, Œîy):\")\n",
    "    for i, (dx, dy) in enumerate(offsets):\n",
    "        print(f\"  Corner {i}: ({dx:+.1f}, {dy:+.1f}) px\")\n",
    "\n",
    "\n",
    "PREPROCESSED_DIR = \"datasets/val2017_preprocessed\"\n",
    "# Run visualization\n",
    "# if os.path.exists(PREPROCESSED_DIR):\n",
    "#     visualize_generate_pair(PREPROCESSED_DIR)"
   ],
   "id": "af2ec042a650314e",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T23:15:15.996973500Z",
     "start_time": "2025-10-27T21:24:05.171162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_offset_sign(image_dir, window_size=64, margin=16, disp_range=(-16, 16)):\n",
    "    # Pick random image\n",
    "    image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir)\n",
    "                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    if not image_paths:\n",
    "        print(\"‚ùå No images found in\", image_dir)\n",
    "        return\n",
    "\n",
    "    img_path = random.choice(image_paths)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        print(\"‚ùå Could not read image:\", img_path)\n",
    "        return\n",
    "\n",
    "    # Generate pair using your pipeline\n",
    "    pair, offsets, src_corners, warped_true = generate_pair(\n",
    "        img, window_size=window_size, margin=margin, disp_range=disp_range\n",
    "    )\n",
    "    dst_corners = src_corners + offsets\n",
    "\n",
    "    # Reconstruct warps using both offset signs\n",
    "    H_plus = cv2.getPerspectiveTransform(src_corners, dst_corners)\n",
    "    H_minus = cv2.getPerspectiveTransform(src_corners, src_corners - offsets)\n",
    "\n",
    "    warped_plus = cv2.warpPerspective(img, H_plus, (img.shape[1], img.shape[0]))\n",
    "    warped_minus = cv2.warpPerspective(img, H_minus, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    # --- Visualization ---\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n",
    "\n",
    "    # 1Ô∏è‚É£ True warped image\n",
    "    axes[0].imshow(warped_true, cmap='gray')\n",
    "    axes[0].add_patch(plt.Polygon(src_corners, fill=False, edgecolor='green', lw=2, label='src'))\n",
    "    axes[0].add_patch(plt.Polygon(dst_corners, fill=False, edgecolor='blue', lw=2, label='dst (GT)'))\n",
    "    axes[0].set_title(\"True warped image (from generate_pair)\")\n",
    "    axes[0].legend()\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # 2Ô∏è‚É£ Reconstructed with +offsets\n",
    "    axes[1].imshow(warped_plus, cmap='gray')\n",
    "    axes[1].add_patch(plt.Polygon(src_corners, fill=False, edgecolor='green', lw=2))\n",
    "    axes[1].add_patch(plt.Polygon(dst_corners, fill=False, edgecolor='red', lw=2))\n",
    "    axes[1].set_title(\"Reconstructed warp (+offsets)\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # 3Ô∏è‚É£ Reconstructed with -offsets\n",
    "    axes[2].imshow(warped_minus, cmap='gray')\n",
    "    axes[2].add_patch(plt.Polygon(src_corners, fill=False, edgecolor='green', lw=2))\n",
    "    axes[2].add_patch(plt.Polygon(dst_corners, fill=False, edgecolor='red', lw=2))\n",
    "    axes[2].set_title(\"Reconstructed warp (-offsets)\")\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Numeric comparison for confirmation\n",
    "    diff_plus = np.mean(np.abs(warped_true.astype(np.float32) - warped_plus.astype(np.float32)))\n",
    "    diff_minus = np.mean(np.abs(warped_true.astype(np.float32) - warped_minus.astype(np.float32)))\n",
    "\n",
    "    print(f\"Mean pixel difference (true vs +offsets): {diff_plus:.2f}\")\n",
    "    print(f\"Mean pixel difference (true vs -offsets): {diff_minus:.2f}\")\n",
    "\n",
    "    if diff_minus < diff_plus:\n",
    "        print(\"‚ö†Ô∏è Offsets likely need to be NEGATED during training.\")\n",
    "    else:\n",
    "        print(\"‚úÖ Offsets appear to have the correct sign.\")\n",
    "\n",
    "# visualize_offset_sign(PREPROCESSED_DIR)\n"
   ],
   "id": "f285bcd7ccf2846c",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T23:15:16.546230800Z",
     "start_time": "2025-10-27T21:24:05.280908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=None, stride=1,\n",
    "                 dropout_rate=0.1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        out_channels = out_channels or in_channels  # ƒçe ni doloƒçeno, ohrani enako ≈°t. kanalov\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dropout = nn.Dropout2d(p=dropout_rate)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        out = self.relu(out)\n",
    "        return out\n"
   ],
   "id": "407bb52091e55766",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T23:15:16.824217400Z",
     "start_time": "2025-10-27T21:24:05.377597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResNetBody(nn.Module):\n",
    "    def __init__(self, in_channels=2, dropout_rate=0.1):\n",
    "        super(ResNetBody, self).__init__()\n",
    "\n",
    "        # ----- 1. stopnja -----\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ResNetBlock(in_channels, 64, dropout_rate=dropout_rate),\n",
    "            ResNetBlock(64, 64, dropout_rate=dropout_rate),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 64x64 -> 32x32\n",
    "        )\n",
    "\n",
    "        # ----- 2. stopnja -----\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResNetBlock(64, 64, dropout_rate=dropout_rate),\n",
    "            ResNetBlock(64, 64, dropout_rate=dropout_rate),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 32x32 -> 16x16\n",
    "        )\n",
    "\n",
    "        # ----- 3. stopnja -----\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ResNetBlock(64, 128, dropout_rate=dropout_rate),\n",
    "            ResNetBlock(128, 128, dropout_rate=dropout_rate),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 16x16 -> 8x8\n",
    "        )\n",
    "\n",
    "        # ----- 4. stopnja -----\n",
    "        self.layer4 = nn.Sequential(\n",
    "            ResNetBlock(128, 128, dropout_rate=dropout_rate),\n",
    "            ResNetBlock(128, 128, dropout_rate=dropout_rate),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "            # zadnji max pool ni potreben, ohranimo 8x8\n",
    "        )\n",
    "\n",
    "        # ----- Polno povezan sloj -----\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(128 * 8 * 8, 512)\n",
    "\n",
    "    def forward(self, x):  # Nx2x64x64\n",
    "        x = self.layer1(x)  # Nx64x32x32\n",
    "        x = self.layer2(x)  # Nx64x16x16\n",
    "        x = self.layer3(x)  # Nx128x8x8\n",
    "        x = self.layer4(x)  # Nx128x8x8\n",
    "        x = self.flatten(x)  # Nx8192\n",
    "        x = self.fc(x)  # Nx512\n",
    "        return x\n"
   ],
   "id": "a684a1af5368087a",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T23:15:17.077225Z",
     "start_time": "2025-10-27T21:24:05.528184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, in_features=512, out_features=8):\n",
    "        super(RegressionHead, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x):  # Nx512\n",
    "        return self.fc(x)  # Nx8"
   ],
   "id": "5b273193639e43d7",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T23:15:17.293273300Z",
     "start_time": "2025-10-27T21:24:05.664033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, in_features=512, num_classes=21, class_dim=8):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.class_dim = class_dim\n",
    "        self.fc = nn.Linear(in_features, num_classes * class_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):  # Nx512\n",
    "        x = self.fc(x)  # Nx168\n",
    "        x = x.view(-1, self.num_classes, self.class_dim)  # Nx21x8\n",
    "        x = self.softmax(x)  # Nx21x8\n",
    "        return x"
   ],
   "id": "485a6f0a708d001c",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T23:15:17.493803900Z",
     "start_time": "2025-10-27T21:24:05.815260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HomographyRegressor(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.1):\n",
    "        super(HomographyRegressor, self).__init__()\n",
    "        self.body = ResNetBody(in_channels=2, dropout_rate=dropout_rate)\n",
    "        self.head = RegressionHead(in_features=512, out_features=8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ],
   "id": "9198046e75e7a91d",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T23:15:17.673791400Z",
     "start_time": "2025-10-27T21:24:05.927074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HomographyClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=21, class_dim=8, dropout_rate=0.1):\n",
    "        super(HomographyClassifier, self).__init__()\n",
    "        self.body = ResNetBody(in_channels=2, dropout_rate=dropout_rate)\n",
    "        self.head = ClassificationHead(in_features=512,\n",
    "                                       num_classes=num_classes,\n",
    "                                       class_dim=class_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ],
   "id": "e6180cb4ddbcba92",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T23:15:18.457149500Z",
     "start_time": "2025-10-27T21:24:06.053189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_epoch(filename):\n",
    "    match = re.search(r\"epoch_(\\d+)\", filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "\n",
    "def nn_train(model, num_epochs, model_file_name, img, optimizer, criterion, checkpoint_dir=\"checkpoints\"):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    start_epoch = 0\n",
    "\n",
    "    # üîÑ Resume if checkpoint exists\n",
    "    checkpoints = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth\")])\n",
    "    checkpoints = sorted(checkpoints, key=extract_epoch)\n",
    "    if checkpoints:\n",
    "        latest_ckpt = os.path.join(checkpoint_dir, checkpoints[-1])\n",
    "        checkpoint = torch.load(latest_ckpt, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "        print(f\"‚úÖ Resuming from checkpoint: {latest_ckpt} (epoch {start_epoch})\")\n",
    "    else:\n",
    "        print(\"üöÄ Starting training from scratch.\")\n",
    "\n",
    "    # üîÅ Training loop with progress bar for epochs\n",
    "    progress_bar = tqdm(range(start_epoch, num_epochs), desc=\"Training\", ncols=100)\n",
    "\n",
    "    for epoch in progress_bar:\n",
    "        model.train()\n",
    "\n",
    "        pair, offsets, src_corners, warped = generate_pair(\n",
    "            img=random.choice(img) if isinstance(img, list) else img,\n",
    "            window_size=64,\n",
    "            margin=16,\n",
    "            disp_range=(-16, 16)\n",
    "        )\n",
    "\n",
    "        pair = torch.from_numpy(pair).permute(2, 0, 1).unsqueeze(0).to(device).float() # 1x2x64x64\n",
    "        offsets = torch.from_numpy(offsets.flatten()).unsqueeze(0).to(device).float()  # 1x8\n",
    "\n",
    "        # Forward\n",
    "        preds = model(pair)\n",
    "        loss = criterion(preds, -offsets)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar with current epoch and loss\n",
    "        progress_bar.set_description(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.6f}\")\n",
    "\n",
    "        # üíæ Save checkpoint every 1000 epochs\n",
    "        if (epoch + 1) % 1000 == 0 or (epoch + 1) == num_epochs:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch + 1}.pth\")\n",
    "            torch.save({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "            print(f\"\\nüíæ Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "            # üßπ Keep only last 4 checkpoints\n",
    "            checkpoints = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth\")])\n",
    "            checkpoints = sorted(checkpoints, key=extract_epoch)\n",
    "            while len(checkpoints) > 4:\n",
    "                old_ckpt = os.path.join(checkpoint_dir, checkpoints[0])\n",
    "                os.remove(old_ckpt)\n",
    "                print(f\"üóëÔ∏è Removed old checkpoint: {old_ckpt}\")\n",
    "                checkpoints.pop(0)\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    # ‚úÖ Save final model\n",
    "    torch.save(model.state_dict(), model_file_name)\n",
    "    print(f\"‚úÖ Final model saved: {model_file_name}\")\n",
    "\n",
    "    # üßπ Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"üßπ GPU memory cleared. Current allocated: {torch.cuda.memory_allocated(device) / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(device) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(device) / 1e9\n",
    "        total = torch.cuda.get_device_properties(device).total_memory / 1e9\n",
    "        print(f\"GPU Memory Status:\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved:  {reserved:.2f} GB\")\n",
    "        print(f\"  Total:     {total:.2f} GB\")\n",
    "        print(f\"  Free:      {total - reserved:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA not available\")\n",
    "\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"‚úÖ GPU cache cleared\")\n",
    "        check_gpu_memory()\n",
    "    else:\n",
    "        print(\"CUDA not available\")"
   ],
   "id": "3d87bd0e7187ee65",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T23:15:18.540263600Z",
     "start_time": "2025-10-27T21:25:01.379420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 50000\n",
    "learning_rate = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = HomographyRegressor(dropout_rate=0.1).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "image_names = [\n",
    "    \"000000002299.jpg\",\n",
    "    # \"000000000285.jpg\",\n",
    "    # \"000000000632.jpg\",\n",
    "]\n",
    "\n",
    "images = []\n",
    "for filename in image_names:\n",
    "    img_path = os.path.join(PREPROCESSED_DIR, filename)\n",
    "    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is not None:\n",
    "        images.append(image)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: Could not load {filename}\")\n",
    "\n",
    "print(f\"üì∑ Loaded {len(images)} image(s) for training\")\n",
    "#\n",
    "# nn_train(\n",
    "#     model=model,\n",
    "#     num_epochs=num_epochs,\n",
    "#     model_file_name=f\"h_regressor_ep{num_epochs}_I{len(images)}.pth\",\n",
    "#     img=images[0] if len(images) == 1 else images,\n",
    "#     optimizer=optimizer,\n",
    "#     criterion=criterion,\n",
    "#     checkpoint_dir=\"checkpoints_homography_-O\"\n",
    "# )"
   ],
   "id": "54bad3d8e444d679",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "üì∑ Loaded 1 image(s) for training\n",
      "üöÄ Starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1002/50000:   2%|‚ñç                     | 1001/50000 [01:22<1:37:24,  8.38it/s, loss=85.074081]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved checkpoint: checkpoints_homography_-O\\checkpoint_epoch_1000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2004/50000:   4%|‚ñâ                       | 2002/50000 [02:44<51:45, 15.46it/s, loss=99.486053]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved checkpoint: checkpoints_homography_-O\\checkpoint_epoch_2000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3002/50000:   6%|‚ñà‚ñé                    | 3001/50000 [04:07<1:19:23,  9.87it/s, loss=14.871353]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved checkpoint: checkpoints_homography_-O\\checkpoint_epoch_3000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4002/50000:   8%|‚ñà‚ñä                    | 4001/50000 [05:22<1:29:18,  8.58it/s, loss=30.407738]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved checkpoint: checkpoints_homography_-O\\checkpoint_epoch_4000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5002/50000:  10%|‚ñà‚ñà‚ñè                   | 5001/50000 [06:48<1:13:19, 10.23it/s, loss=11.859369]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved checkpoint: checkpoints_homography_-O\\checkpoint_epoch_5000.pth\n",
      "üóëÔ∏è Removed old checkpoint: checkpoints_homography_-O\\checkpoint_epoch_1000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5012/50000:  10%|‚ñà‚ñà‚ñè                   | 5012/50000 [06:49<1:01:18, 12.23it/s, loss=37.289501]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[42]\u001B[39m\u001B[32m, line 29\u001B[39m\n\u001B[32m     25\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m‚ö†Ô∏è Warning: Could not load \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilename\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     27\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33müì∑ Loaded \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(images)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m image(s) for training\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m29\u001B[39m \u001B[43mnn_train\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_file_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mh_regressor_ep\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m_I\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m.pth\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[43m    \u001B[49m\u001B[43mimg\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     34\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     35\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     36\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcheckpoint_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcheckpoints_homography_-O\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     37\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[39]\u001B[39m\u001B[32m, line 49\u001B[39m, in \u001B[36mnn_train\u001B[39m\u001B[34m(model, num_epochs, model_file_name, img, optimizer, criterion, checkpoint_dir)\u001B[39m\n\u001B[32m     47\u001B[39m \u001B[38;5;66;03m# Backprop\u001B[39;00m\n\u001B[32m     48\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m49\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     50\u001B[39m optimizer.step()\n\u001B[32m     52\u001B[39m \u001B[38;5;66;03m# Update progress bar with current epoch and loss\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\RV\\Racunalniski Vid\\RV-N1\\.venv\\Lib\\site-packages\\torch\\_tensor.py:625\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    616\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    617\u001B[39m         Tensor.backward,\n\u001B[32m    618\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    623\u001B[39m         inputs=inputs,\n\u001B[32m    624\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m625\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\RV\\Racunalniski Vid\\RV-N1\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\RV\\Racunalniski Vid\\RV-N1\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    839\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    840\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    843\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    844\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    845\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def test_model(model_path, test_image_names, num_samples=10, visualize=True):\n",
    "    # Load model (accept both plain state_dict and checkpoint dict)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = HomographyRegressor(dropout_rate=0.1).to(device)\n",
    "\n",
    "    loaded = torch.load(model_path, map_location=device)\n",
    "    if isinstance(loaded, dict) and \"model_state_dict\" in loaded:\n",
    "        model.load_state_dict(loaded[\"model_state_dict\"])\n",
    "        epoch_info = loaded.get(\"epoch\", None)\n",
    "        print(f\"‚úÖ Checkpoint loaded from: {model_path}\" + (f\" (epoch {epoch_info})\" if epoch_info is not None else \"\"))\n",
    "    else:\n",
    "        model.load_state_dict(loaded)\n",
    "        print(f\"‚úÖ Model state_dict loaded from: {model_path}\")\n",
    "\n",
    "    model.eval()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load test images\n",
    "    test_images = []\n",
    "    for filename in test_image_names:\n",
    "        img_path = os.path.join(PREPROCESSED_DIR, filename)\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is not None:\n",
    "            test_images.append(image)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Warning: Could not load {filename}\")\n",
    "\n",
    "    print(f\"üì∑ Loaded {len(test_images)} test image(s)\")\n",
    "\n",
    "    if len(test_images) == 0:\n",
    "        print(\"‚ùå No test images loaded!\")\n",
    "        return None\n",
    "\n",
    "    # Test the model\n",
    "    errors = []\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(num_samples), desc=\"Testing\"):\n",
    "            # Generate test pair\n",
    "            test_img = random.choice(test_images)\n",
    "            pair, offsets, src_corners, warped = generate_pair(\n",
    "                img=test_img,\n",
    "                window_size=64,\n",
    "                margin=16,\n",
    "                disp_range=(-16, 16)\n",
    "            )\n",
    "\n",
    "            # Prepare input\n",
    "            pair_tensor = torch.from_numpy(pair).permute(2, 0, 1).unsqueeze(0).to(device).float()\n",
    "            offsets_gt = offsets.flatten()\n",
    "\n",
    "            # Predict (use detach before converting to numpy)\n",
    "            pred = model(pair_tensor)\n",
    "            pred_np = pred.detach().cpu().numpy().flatten()\n",
    "\n",
    "            # Calculate error\n",
    "            error = np.abs(pred_np - offsets_gt)\n",
    "            errors.append(error)\n",
    "            predictions.append(pred_np)\n",
    "            ground_truths.append(offsets_gt)\n",
    "\n",
    "    errors = np.array(errors)\n",
    "    predictions = np.array(predictions)\n",
    "    ground_truths = np.array(ground_truths)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mean_error = errors.mean()\n",
    "    std_error = errors.std()\n",
    "    max_error = errors.max()\n",
    "    min_error = errors.min()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"TEST RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Number of samples: {num_samples}\")\n",
    "    print(f\"Mean absolute error: {mean_error:.4f} pixels\")\n",
    "    print(f\"Std deviation: {std_error:.4f} pixels\")\n",
    "    print(f\"Min error: {min_error:.4f} pixels\")\n",
    "    print(f\"Max error: {max_error:.4f} pixels\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Visualize if requested\n",
    "    if visualize:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "        # 1. Error distribution\n",
    "        ax = axes[0, 0]\n",
    "        ax.hist(errors.flatten(), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "        ax.axvline(mean_error, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_error:.4f}')\n",
    "        ax.set_xlabel('Absolute Error (pixels)')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title('Error Distribution')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # 2. Per-coordinate error\n",
    "        ax = axes[0, 1]\n",
    "        coord_errors = errors.mean(axis=0)\n",
    "        coord_names = ['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']\n",
    "        bars = ax.bar(coord_names, coord_errors, color=['red' if i % 2 == 0 else 'blue' for i in range(8)])\n",
    "        ax.set_ylabel('Mean Absolute Error (pixels)')\n",
    "        ax.set_title('Error per Corner Coordinate')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        # 3. Prediction vs Ground Truth scatter\n",
    "        ax = axes[1, 0]\n",
    "        ax.scatter(ground_truths.flatten(), predictions.flatten(), alpha=0.5, s=10)\n",
    "        min_val = min(ground_truths.min(), predictions.min())\n",
    "        max_val = max(ground_truths.max(), predictions.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect prediction')\n",
    "        ax.set_xlabel('Ground Truth Offset (pixels)')\n",
    "        ax.set_ylabel('Predicted Offset (pixels)')\n",
    "        ax.set_title('Predictions vs Ground Truth')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axis('equal')\n",
    "\n",
    "        # 4. Sample visualization\n",
    "        ax = axes[1, 1]\n",
    "        test_img = random.choice(test_images)\n",
    "        pair, offsets, src_corners, warped = generate_pair(\n",
    "            img=test_img,\n",
    "            window_size=64,\n",
    "            margin=16,\n",
    "            disp_range=(-16, 16)\n",
    "        )\n",
    "        pair_tensor = torch.from_numpy(pair).permute(2, 0, 1).unsqueeze(0).to(device).float()\n",
    "        pred = model(pair_tensor).cpu().detach().numpy().flatten().reshape(4, 2)\n",
    "\n",
    "        # Show the pair\n",
    "        combined = np.hstack([pair[:, :, 0], pair[:, :, 1]])\n",
    "        ax.imshow(combined, cmap='gray')\n",
    "        ax.set_title('Sample: Original (left) | Warped (right)')\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('test_model_results.png', dpi=150, bbox_inches='tight')\n",
    "        print(f\"üìä Visualization saved to: test_model_results.png\")\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        'mean_error': mean_error,\n",
    "        'std_error': std_error,\n",
    "        'max_error': max_error,\n",
    "        'min_error': min_error,\n",
    "        'errors': errors,\n",
    "        'predictions': predictions,\n",
    "        'ground_truths': ground_truths\n",
    "    }\n",
    "\n",
    "# test on latest checkpoint\n",
    "latest_model_path = \"checkpoints_homography_-O/checkpoint_epoch_5000.pth\"\n",
    "test_image_names = [\n",
    "    \"000000002299.jpg\",\n",
    "]\n",
    "test_model(\n",
    "    model_path=latest_model_path,\n",
    "    test_image_names=test_image_names,\n",
    "    num_samples=1000,\n",
    "    visualize=True\n",
    ")"
   ],
   "id": "79622f3cab0cd0bd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
