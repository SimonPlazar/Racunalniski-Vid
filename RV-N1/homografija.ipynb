{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T20:07:56.049181Z",
     "start_time": "2025-11-09T20:07:56.046352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ],
   "id": "7324b9abfa64a763",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T20:07:56.057469Z",
     "start_time": "2025-11-09T20:07:56.055253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Models import HomographyRegressor, HomographyClassifier\n",
    "from Models import HomographyPairDataset, FixedSrcRandomDispDataset\n",
    "\n",
    "from Models import save_checkpoint, load_latest_checkpoint\n",
    "from Models import offsets_to_class_indices, classes_to_offsets\n",
    "from Models import classification_loss\n",
    "\n",
    "from Generator import get_images_from_names, get_random_images, get_all_images"
   ],
   "id": "4c51f835af5056ff",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T20:07:56.068072Z",
     "start_time": "2025-11-09T20:07:56.061773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def nn_train_classify(model, dataloader, num_epochs, model_file_name, optimizer, criterion,\n",
    "                      checkpoint_dir=\"checkpoints\", num_classes=21, disp_range=(-16, 16)):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    device = next(model.parameters()).device\n",
    "    start_epoch = load_latest_checkpoint(checkpoint_dir, model, optimizer, device)\n",
    "    writer = SummaryWriter(log_dir=os.path.join(checkpoint_dir, \"runs\"))\n",
    "\n",
    "    epoch_pbar = tqdm(\n",
    "        range(start_epoch, num_epochs),\n",
    "        desc=\"Training\",\n",
    "        ncols=120,\n",
    "        miniters=1,\n",
    "        smoothing=0,\n",
    "        dynamic_ncols=True,\n",
    "        initial=start_epoch,\n",
    "        total=num_epochs\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        for epoch in epoch_pbar:\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            running_rmse_hard = 0.0\n",
    "            running_rmse_soft = 0.0\n",
    "            count = 0\n",
    "\n",
    "            for pairs, offsets in dataloader:\n",
    "                pairs = pairs.to(device)\n",
    "                offsets = offsets.to(device)\n",
    "                B = pairs.shape[0]\n",
    "\n",
    "                # --- Forward ---\n",
    "                logits = model(pairs)  # (B, 8, 21)\n",
    "\n",
    "                # --- Compute loss ---\n",
    "                loss = classification_loss(criterion, logits, offsets,\n",
    "                                           disp_range=disp_range, num_classes=num_classes)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # --- Metrics ---\n",
    "                with torch.no_grad():\n",
    "                    pred_hard = classes_to_offsets(logits, disp_range, soft=False)\n",
    "                    pred_soft = classes_to_offsets(logits, disp_range, soft=True)\n",
    "\n",
    "                    # rmse_hard_val = rmse(pred_hard, offsets).mean().item()\n",
    "                    # rmse_soft_val = rmse(pred_soft, offsets).mean().item()\n",
    "                    rmse_hard_val = torch.sqrt(((pred_hard - offsets) ** 2).mean(dim=-1)).item()\n",
    "                    rmse_soft_val = torch.sqrt(((pred_soft - offsets) ** 2).mean(dim=-1)).item()\n",
    "\n",
    "                # --- Logging ---\n",
    "                running_loss += loss.item() * B\n",
    "                running_rmse_hard += rmse_hard_val * B\n",
    "                running_rmse_soft += rmse_soft_val * B\n",
    "                count += B\n",
    "\n",
    "            # Average metrics per epoch\n",
    "            avg_loss = running_loss / count\n",
    "            avg_rmse_hard = running_rmse_hard / count\n",
    "            avg_rmse_soft = running_rmse_soft / count\n",
    "\n",
    "            epoch_pbar.set_postfix({\n",
    "                \"loss\": f\"{avg_loss:.4f}\",\n",
    "                \"rmse_hard\": f\"{avg_rmse_hard:.3f}px\",\n",
    "                \"rmse_soft\": f\"{avg_rmse_soft:.3f}px\"\n",
    "            })\n",
    "\n",
    "            # TensorBoard\n",
    "            writer.add_scalar(\"Loss/train\", avg_loss, epoch + 1)\n",
    "            writer.add_scalar(\"RMSE/hard\", avg_rmse_hard, epoch + 1)\n",
    "            writer.add_scalar(\"RMSE/soft\", avg_rmse_soft, epoch + 1)\n",
    "\n",
    "            # --- Checkpoint every N epochs ---\n",
    "            if (epoch + 1) % 1000 == 0 or (epoch + 1) == num_epochs:\n",
    "                save_checkpoint(checkpoint_dir, epoch + 1, model, optimizer)\n",
    "\n",
    "        # Save final model\n",
    "        torch.save(model.state_dict(), model_file_name)\n",
    "        print(f\"‚úÖ Final model saved: {model_file_name}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        epoch_pbar.close()\n",
    "        print(f\"\\n‚ö†Ô∏è Interrupted at epoch {epoch + 1}\")\n",
    "        save_checkpoint(checkpoint_dir, epoch + 1, model, optimizer)\n",
    "        print(\"‚úÖ Checkpoint saved\")\n",
    "\n",
    "    finally:\n",
    "        epoch_pbar.close()\n",
    "        writer.close()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n"
   ],
   "id": "a84fb34901631e1a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T20:07:56.076787Z",
     "start_time": "2025-11-09T20:07:56.071031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def nn_train_regressor(model, num_epochs, batch_size, samples_per_epoch, model_file_name,\n",
    "                       images, optimizer, criterion,\n",
    "                       checkpoint_dir=\"checkpoints\"):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    device = next(model.parameters()).device\n",
    "    start_epoch = load_latest_checkpoint(checkpoint_dir, model, optimizer, device)\n",
    "    writer = SummaryWriter(log_dir=os.path.join(checkpoint_dir, \"runs\"))\n",
    "\n",
    "    dataset = HomographyPairDataset(images, samples_per_epoch)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                            num_workers=0, pin_memory=True)\n",
    "\n",
    "    epoch_pbar = tqdm(\n",
    "        range(start_epoch, num_epochs),\n",
    "        desc=\"Training\",\n",
    "        ncols=120,\n",
    "        miniters=1,\n",
    "        smoothing=0,\n",
    "        dynamic_ncols=True,\n",
    "        initial=start_epoch,\n",
    "        total=num_epochs\n",
    "    )\n",
    "    try:\n",
    "        for epoch in epoch_pbar:\n",
    "            model.train()\n",
    "            epoch_loss, epoch_mae, epoch_rmse = 0.0, 0.0, 0.0\n",
    "\n",
    "            for pairs, offsets in dataloader:\n",
    "                pairs = pairs.to(device)\n",
    "                offsets = offsets.to(device)\n",
    "\n",
    "                preds = model(pairs)\n",
    "\n",
    "                # === Compute loss ===\n",
    "                loss = criterion(preds, offsets)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # === Metrics ===\n",
    "                with torch.no_grad():\n",
    "                    mae = torch.mean(torch.abs(preds - offsets)).item()\n",
    "                    rmse = torch.sqrt(torch.mean((preds - offsets) ** 2) + 1e-8).item()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_mae += mae\n",
    "                epoch_rmse += rmse\n",
    "\n",
    "            # === Epoch summary ===\n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            avg_mae = epoch_mae / len(dataloader)\n",
    "            avg_rmse = epoch_rmse / len(dataloader)\n",
    "\n",
    "            epoch_pbar.set_postfix({\n",
    "                \"loss\": f\"{avg_loss:.4f}\",\n",
    "                \"mae\": f\"{avg_mae:.3f}px\",\n",
    "                \"rmse\": f\"{avg_rmse:.3f}px\"\n",
    "            })\n",
    "\n",
    "            writer.add_scalar(\"Loss/MSE\", avg_loss, epoch)\n",
    "            writer.add_scalar(\"Error/MAE\", avg_mae, epoch)\n",
    "            writer.add_scalar(\"Error/RMSE\", avg_rmse, epoch)\n",
    "\n",
    "            # === Checkpoint every N epochs ===\n",
    "            if (epoch + 1) % 1000 == 0 or (epoch + 1) == num_epochs:\n",
    "                save_checkpoint(checkpoint_dir, epoch + 1, model, optimizer)\n",
    "\n",
    "        # Save final model\n",
    "        torch.save(model.state_dict(), model_file_name)\n",
    "        print(f\"‚úÖ Final model saved: {model_file_name}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        epoch_pbar.close()\n",
    "        print(f\"\\n‚ö†Ô∏è Interrupted at epoch {epoch + 1}\")\n",
    "        save_checkpoint(checkpoint_dir, epoch + 1, model, optimizer)\n",
    "        print(\"‚úÖ Checkpoint saved\")\n",
    "\n",
    "    finally:\n",
    "        epoch_pbar.close()\n",
    "        writer.close()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()"
   ],
   "id": "c9eb9032a42626ea",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T20:21:17.342995Z",
     "start_time": "2025-11-09T20:14:07.570909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # TRAIN Regressor\n",
    "#\n",
    "# PREPROCESSED_DIR = \"datasets/val2017_preprocessed\"\n",
    "# num_epochs = 30000\n",
    "# batch_size = 32\n",
    "# learning_rate = 1e-4\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "#\n",
    "# model = HomographyRegressor(dropout_rate=0.1).to(device)\n",
    "# criterion = nn.MSELoss()\n",
    "#\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#\n",
    "# image_names = [\n",
    "#     \"000000002299.jpg\",\n",
    "#     #     # \"000000000285.jpg\",\n",
    "#     #     # \"000000000632.jpg\",\n",
    "# ]\n",
    "# # images = get_images_from_names(image_names, PREPROCESSED_DIR)\n",
    "# # images = get_random_images(1, image_dir=PREPROCESSED_DIR)\n",
    "# images = get_all_images(PREPROCESSED_DIR)\n",
    "#\n",
    "# print(f\"üì∑ Loaded {len(images)} image(s) for training\")\n",
    "#\n",
    "# nn_train_regressor(\n",
    "#     model=model,\n",
    "#     num_epochs=num_epochs,\n",
    "#     batch_size=batch_size,\n",
    "#     samples_per_epoch=64,\n",
    "#     model_file_name=f\"h_regressor_all.pth\",\n",
    "#     images=images,\n",
    "#     optimizer=optimizer,\n",
    "#     criterion=criterion,\n",
    "#     checkpoint_dir=\"checkpoints_homography_regressor_all\"\n",
    "# )"
   ],
   "id": "3e65e0d1d7f93b81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 39555/50000 [07:09<01:53, 92.05it/s, loss=81.4260, rmse=9.024px]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è Interrupted at epoch 39556\n",
      "‚úÖ Checkpoint saved\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T20:07:58.387757Z",
     "start_time": "2025-11-09T20:07:56.080855Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "üì∑ Loaded 5000 image(s) for training\n"
     ]
    }
   ],
   "execution_count": 13,
   "source": [
    "# # TRAIN Classifier\n",
    "#\n",
    "# PREPROCESSED_DIR = \"datasets/val2017_preprocessed\"\n",
    "# num_epochs = 30000\n",
    "# samples_per_epoch = 128\n",
    "# batch_size = 384\n",
    "# learning_rate = 1e-3\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "#\n",
    "# model = HomographyClassifier(num_classes=21, class_dim=8, dropout_rate=0.1).to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "#\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#\n",
    "# # image_names = [\n",
    "# #     \"000000002299.jpg\",\n",
    "# #     #     # \"000000000285.jpg\",\n",
    "# #     #     # \"000000000632.jpg\",\n",
    "# # ]\n",
    "# # images = get_images_from_names(image_names, PREPROCESSED_DIR)\n",
    "# # images = get_random_images(image_dir=PREPROCESSED_DIR, num_images=16)\n",
    "# images = get_all_images(image_dir=PREPROCESSED_DIR)\n",
    "#\n",
    "# print(f\"üì∑ Loaded {len(images)} image(s) for training\")\n",
    "#\n",
    "# dataset = HomographyPairDataset(images, samples_per_epoch=batch_size)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=0, pin_memory=True, shuffle=True)\n",
    "#\n",
    "# nn_train_classify(\n",
    "#     model=model,\n",
    "#     dataloader=dataloader,\n",
    "#     num_epochs=num_epochs,\n",
    "#     model_file_name=f\"h_classify.pth\",\n",
    "#     optimizer=optimizer,\n",
    "#     criterion=criterion,\n",
    "#     checkpoint_dir=\"checkpoints_homography_classify_all\"\n",
    "# )"
   ],
   "id": "45385941a63088af"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
